---
title: "RAG Optimization Strategies"
original_file: "./RAG_Optimization_Strategies.pdf"
document_type: "research"
conversion_date: "2025-11-29"
topics: ["prompt-engineering", "llm", "rag", "chain-of-thought", "react"]
keywords: ["answer", "community", "entities", "page", "graph", "graphrag", "data", "question", "rag", "llm"]
summary: "<!-- Page 1 -->

From Local to Global: A GraphRAG Approach to

### Query-Focused Summarization

DarrenEdge1† HaTrinh1† NewmanCheng2 JoshuaBradley2 AlexChao3
ApurvaMody3 StevenTruitt2 DashaMetropolitansky1 RobertOsazuwaNess1

### JonathanLarson1

1MicrosoftResearch
2MicrosoftStrategicMissionsandTechnologies
3MicrosoftOfficeoftheCTO
{daedge,trinhha,newmancheng,joshbradley,achao,moapurva,
steventruitt,dasham,robertness,jolarso}@microsoft.com
†Theseauthorscontributedequallytothiswork

### Abstract

"
related_documents: []
---

# RAG Optimization Strategies

<!-- Page 1 -->

From Local to Global: A GraphRAG Approach to

### Query-Focused Summarization

DarrenEdge1† HaTrinh1† NewmanCheng2 JoshuaBradley2 AlexChao3
ApurvaMody3 StevenTruitt2 DashaMetropolitansky1 RobertOsazuwaNess1

### JonathanLarson1

1MicrosoftResearch
2MicrosoftStrategicMissionsandTechnologies
3MicrosoftOfficeoftheCTO
{daedge,trinhha,newmancheng,joshbradley,achao,moapurva,
steventruitt,dasham,robertness,jolarso}@microsoft.com
†Theseauthorscontributedequallytothiswork

### Abstract

The use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs)
toanswerquestionsoverprivateand/orpreviouslyunseendocumentcollections.
However, RAG fails on global questions directed at an entire text corpus, such
as “What are the main themes in the dataset?”, since this is inherently a queryfocused summarization (QFS) task, rather than an explicit retrieval task. Prior
QFS methods, meanwhile, do not scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, we
propose GraphRAG, a graph-based approach to question answering over private
textcorporathatscaleswithboththegeneralityofuserquestionsandthequantity
ofsourcetext. OurapproachusesanLLMtobuildagraphindexintwostages:
first,toderiveanentityknowledgegraphfromthesourcedocuments,thentopregeneratecommunitysummariesforallgroupsofcloselyrelatedentities. Givena
question,eachcommunitysummaryisusedtogenerateapartialresponse,before
all partial responses are again summarized in a final response to the user. For a
classofglobalsensemakingquestionsoverdatasetsinthe1milliontokenrange,
we show that GraphRAG leads to substantial improvements over a conventional
RAGbaselineforboththecomprehensivenessanddiversityofgeneratedanswers.
1 Introduction
Retrieval augmented generation (RAG) (Lewis et al., 2020) is an established approach to using
LLMs to answer queries based on data that is too large to contain in a language model’s context
window,meaningthemaximumnumberoftokens(unitsoftext)thatcanbeprocessedbytheLLM
atonce (Kuratovetal.,2024;Liuetal.,2023).InthecanonicalRAGsetup,thesystemhasaccessto
alargeexternalcorpusoftextrecordsandretrievesasubsetofrecordsthatareindividuallyrelevant
tothequeryandcollectivelysmallenoughtofitintothecontextwindowoftheLLM.TheLLMthen
Preprint.Underreview.
5202
beF
91
]LC.sc[
2v03161.4042:viXra

<!-- Page 2 -->

generatesaresponsebasedonboththequeryandtheretrievedrecords(Baumeletal.,2018;Dang,
2006;Laskaretal.,2020;Yaoetal.,2017). Thisconventionalapproach,whichwecollectivelycall
vectorRAG,workswellforqueriesthatcanbeansweredwithinformationlocalizedwithinasmall
set of records. However, vector RAG approaches do not support sensemaking queries, meaning
queriesthatrequireglobalunderstandingoftheentiredataset, suchas”Whatarethekeytrendsin
howscientificdiscoveriesareinfluencedbyinterdisciplinaryresearchoverthepastdecade?”
Sensemaking tasks require reasoning over “connections (which can be among people, places, and
events)inordertoanticipatetheirtrajectoriesandacteffectively”(Kleinetal.,2006). LLMssuch
asGPT(Achiametal.,2023;Brownetal.,2020),Llama(Touvronetal.,2023),andGemini(Anil
et al., 2023) excel at sensemaking in complex domains like scientific discovery (Microsoft, 2023)
andintelligenceanalysis(RanadeandJoshi,2023). Givenasensemakingqueryandatextwithan
implicitandinterconnectedsetofconcepts,anLLMcangenerateasummarythatanswersthequery.
Thechallenge,however,ariseswhenthevolumeofdatarequiresaRAGapproach,sincevectorRAG
approachesareunabletosupportsensemakingoveranentirecorpus.
Inthispaper,wepresentGraphRAG–agraph-basedRAGapproachthatenablessensemakingover
the entirety of a large text corpus. GraphRAG first uses an LLM to construct a knowledge graph,
wherenodescorrespondtokeyentitiesinthecorpusandedgesrepresentrelationshipsbetweenthose
entities. Next, it partitions the graph into a hierarchy of communities of closely related entities,
before using an LLM to generate community-level summaries. These summaries are generated in
abottom-upmannerfollowingthehierarchicalstructureofextractedcommunities,withsummaries
at higher levels of the hierarchy recursively incorporating lower-level summaries. Together, these
communitysummariesprovideglobaldescriptionsandinsightsoverthecorpus.Finally,GraphRAG
answers queries through map-reduce processing of community summaries; in the map step, the
summariesareusedtoprovidepartialanswerstothequeryindependentlyandinparallel,theninthe
reducestep,thepartialanswersarecombinedandusedtogenerateafinalglobalanswer.
The GraphRAG method and its ability to perform global sensemaking over an entire corpus form
the main contribution of this work. To demonstrate this ability, we developed a novel application
oftheLLM-as-a-judgetechnique (Zhengetal.,2024)suitableforquestionstargetingbroadissues
and themes where there is no ground-truth answer. This approach first uses one LLM to generate
a diverse set of global sensemaking questions based on corpus-specific use cases, before using a
secondLLMtojudgetheanswersoftwodifferentRAGsystemsusingpredefinedcriteria(defined
inSection3.3). WeusethisapproachtocompareGraphRAGtovectorRAGontworepresentative
real-world text datasets. Results show GraphRAG strongly outperforms vector RAG when using
GPT-4astheLLM.
GraphRAG is available as open-source software at https://github.com/microsoft/graphrag. In addition, versions of the GraphRAG approach are also available as extensions to multiple opensourcelibraries,includingLangChain(LangChain,2024),LlamaIndex(LlamaIndex,2024),NebulaGraph(NebulaGraph,2024),andNeo4J(Neo4J,2024).
2 Background
2.1 RAGApproachesandSystems
RAGgenerallyreferstoanysystemwhereauserqueryisusedtoretrieverelevantinformationfrom
externaldatasources,whereuponthisinformationisincorporatedintothegenerationofaresponse
tothequerybyanLLM(orothergenerativeAImodel,suchasamulti-mediamodel).Thequeryand
retrievedrecordspopulateaprompttemplate,whichisthenpassedtotheLLM(Rametal.,2023).
RAG is ideal when the total number of records in a data source is too large to include in a single
prompttotheLLM,i.e. theamountoftextinthedatasourceexceedstheLLM’scontextwindow.
IncanonicalRAGapproaches,theretrievalprocessreturnsasetnumberofrecordsthataresemantically similar to the query and the generated answer uses only the information in those retrieved
records. A common approach to conventional RAG is to use text embeddings, retrieving records
closesttothequeryinvectorspacewhereclosenesscorrespondstosemanticsimilarity(Gaoetal.,
2023).WhilesomeRAGapproachesmayusealternativeretrievalmechanisms,wecollectivelyrefer
tothefamilyofconventionalapproachesasvectorRAG.GraphRAGcontrastswithvectorRAGin
itsabilitytoanswerqueriesthatrequireglobalsensemakingovertheentiredatacorpus.
2

<!-- Page 3 -->

GraphRAGbuildsuponpriorworkonadvancedRAGstrategies. GraphRAGleveragessummaries
overlargesectionsofthedatasourceasaformof”self-memory”(describedinChengetal.2024),
which are later used to answer queries as in Mao et al. 2020). These summaries are generated in
parallel and iteratively aggregated into global summaries, similar to prior techniques (Feng et al.,
2023;Gaoetal.,2023;Khattabetal.,2022;Shaoetal.,2023;Suetal.,2020;Trivedietal.,2022;
Wang et al., 2024). In particular, GraphRAG is similar to other approaches that use hierarchical
indexingtocreatesummaries(similartoKimetal.2023;Sarthietal.2024). GraphRAGcontrasts
withtheseapproachesbygeneratingagraphindexfromthesourcedata,thenapplyinggraph-based
communitydetectiontocreateathematicpartitioningofthedata.
2.2 UsingKnowledgeGraphswithLLMsandRAG
Approaches to knowledge graph extraction from natural language text corpora include rulematching,statisticalpatternrecognition,clustering,andembeddings(Etzionietal.,2004;Kimetal.,
2016;MooneyandBunescu,2005;Yatesetal.,2007). GraphRAGfallsintoamorerecentbodyof
research that use of LLMs for knowledge graph extraction (Ban et al., 2023; Melnyk et al., 2022;
OpenAI,2023;Tanetal.,2017;Trajanoskaetal.,2023;Yaoetal.,2023;Yatesetal.,2007;Zhang
et al., 2024a). It also adds to a growing body of RAG approaches that use a knowledge graph as
an index (Gao et al., 2023). Some techniques use subgraphs, elements of the graph, or properties
of the graph structure directly in the prompt (Baek et al., 2023; He et al., 2024; Zhang, 2023)
or as factual grounding for generated outputs (Kang et al., 2023; Ranade and Joshi, 2023). Other
techniques(Wangetal.,2023b)usetheknowledgegraphtoenhanceretrieval,whereatquerytime
an LLM-based agent dynamically traverses a graph with nodes representing document elements
(e.g., passages, tables) and edges encoding lexical and semantical similarity or structural relationships.GraphRAGcontrastswiththeseapproachesbyfocusingonapreviouslyunexploredqualityof
graphsinthiscontext: theirinherentmodularity(Newman,2006)andtheabilitytopartitiongraphs
intonestedmodularcommunitiesofcloselyrelatednodes(e.g.,Louvain,Blondeletal.2008; Leiden,Traagetal.2019). Specifically,GraphRAGrecursivelycreatesincreasinglyglobalsummaries
byusingtheLLMtocreatesummariesspanningthiscommunityhierarchy.
2.3 AdaptivebenchmarkingforRAGEvaluation
Many benchmark datasets for open-domain question answering exist, including HotPotQA (Yang
etal.,2018),MultiHop-RAG(TangandYang,2024),andMT-Bench(Zhengetal.,2024). However,
these benchmarks are oriented towards vector RAG performance, i.e., they evaluate performance
on explicit fact retrieval. In this work, we propose an approach for generating a set of questions
forevaluatingglobalsensemakingovertheentiretyofthecorpus. OurapproachisrelatedtoLLM
methodsthatuseacorpustogeneratequestionswhoseanswerswouldbesummariesofthecorpus,
suchasinXuandLapata(2021). However,inordertoproduceafairevaluation,ourmethodavoids
generating the questions directly from the corpus itself (as an alternative implementation, one can
useasubsetofthecorpusheldoutfromsubsequentgraphextractionandanswerevaluationsteps).
Adaptivebenchmarkingreferstotheprocessofdynamicallygeneratingevaluationbenchmarkstailored to specific domains or use cases. Recent work has used LLMs for adaptive benchmarking
toensurerelevance, diversity, andalignmentwiththetargetapplicationortask(Yuanetal.,2024;
Zhang et al., 2024b). In this work, we propose an adaptive benchmarking approach to generating
globalsensemakingqueriesfortheLLM.OurapproachbuildsonpriorworkinLLM-basedpersona
generation, where the LLM is used to generate diverse and authentic sets of personas (Kosinski,
2024;Salminenetal.,2024;Shinetal.,2024). Ouradaptivebenchmarkingprocedureusespersona
generation to create queries that are representative of real-world RAG system usage. Specifically,
our approach uses the LLM to infer the potential users would use the RAG system and their use
cases,whichguidethegenerationofcorpus-specificsensemakingqueries.
2.4 RAGevaluationcriteria
OurevaluationreliesontheLLMtoevaluatehowwelltheRAGsystemanswersthegeneratedquestions. Prior work has shown LLMs to be good evaluators of natural language generation, including work where LLMs evaluations were competitive with human evaluations (Wang et al., 2023a;
Zheng et al., 2024). Some prior work proposes criteria for having LLMs quantify the quality of
3

<!-- Page 4 -->

Source Documents Global Answer
text extraction query-focused
and chunking summarization
Text Chunks Community Answers
domain-tailored query-focused
summarization summarization
Entities & Relationships Community Summaries
domain-tailored domain-tailored
summarization community summarization
detection

### Knowledge Graph Graph Communities


### Indexing Time PipelineStage Query Time

Figure1: GraphRAGpipelineusinganLLM-derivedgraphindexofsourcedocumenttext. This
graph index spans nodes (e.g., entities), edges (e.g., relationships), and covariates (e.g., claims)
thathavebeendetected,extracted,andsummarizedbyLLMpromptstailoredtothedomainofthe
dataset. Communitydetection(e.g.,Leiden,Traagetal.,2019)isusedtopartitionthegraphindex
intogroupsofelements(nodes, edges, covariates)thattheLLMcansummarizeinparallelatboth
indexingtimeandquerytime. The“globalanswer”toagivenqueryisproducedusingafinalround
ofquery-focusedsummarizationoverallcommunitysummariesreportingrelevancetothatquery.
generatedtextssuchas“fluency” (Wangetal.,2023a)Someofthesecriteriaaregenerictovector
RAGsystemsandnotrelevanttoglobalsensemaking, suchas“contextrelevance”, “faithfulness”,
and“answerrelevance”(RAGAS,Esetal.2023). Lackingagoldstandardforevaluation,onecan
quantifyrelativeperformanceforagivencriterionbypromptingtheLLMtocomparegenerations
fromtwodifferentcompetingmodels(LLM-as-a-judge,(Zhengetal.,2024)). Inthiswork,wedesign criteria for evaluating RAG-generated answers to global sensemaking questions and evaluate
our results using the comparative approach. We also validate results using statistics derived from
LLM-extractedstatementsofverifiablefacts,or“claims.”
3 Methods
3.1 GraphRAGWorkflow
Figure1illustratesthehigh-leveldataflowoftheGraphRAGapproachandpipeline. Inthissection,
wedescribethekeydesignparameters,techniques,andimplementationdetailsforeachstep.
3.1.1 SourceDocuments→TextChunks
Tostart,thedocumentsinthecorpusaresplitintotextchunks. TheLLMextractsinformationfrom
each chunk for downstream processing. Selecting the size of the chunk is a fundamental design
decision; longertextchunksrequirefewerLLMcallsforsuchextraction(whichreducescost)but
sufferfromdegradedrecallofinformationthatappearsearlyinthechunk(Kuratovetal.,2024;Liu
etal.,2023). SeeSectionA.1forpromptsandexamplesoftherecall-precisiontrade-offs.
3.1.2 TextChunks→Entities&Relationships
In this step, the LLM is prompted to extract instances of important entities and the relationships
betweentheentitiesfromagivenchunk. Additionally,theLLMgeneratesshortdescriptionsforthe
entitiesandrelationships. Toillustrate,supposeachunkcontainedthefollowingtext:
4

<!-- Page 5 -->

NeoChip’s(NC)sharessurgedintheirfirstweekoftradingontheNewTechExchange. However,marketanalystscautionthatthechipmaker’spublicdebutmay
notreflecttrendsforothertechnologyIPOs. NeoChip,previouslyaprivateentity,
was acquired by Quantum Systems in 2016. The innovative semiconductor firm
specializesinlow-powerprocessorsforwearablesandIoTdevices.

### TheLLMispromptedsuchthatitextractsthefollowing:

• TheentityNeoChip,withdescription“NeoChipisapubliclytradedcompanyspecializing
inlow-powerprocessorsforwearablesandIoTdevices.”
• The entity Quantum Systems, with description “Quantum Systems is a firm that previouslyownedNeoChip.”
• ArelationshipbetweenNeoChipandQuantum Systems,withdescription“QuantumSystemsownedNeoChipfrom2016untilNeoChipbecamepubliclytraded.”
Thesepromptscanbetailoredtothedomainofthedocumentcorpusbychoosingdomainappropriate
few-shot exemplars for in-context learning (Brown et al., 2020). For example, while our default
prompt extracts the broad class of “named entities” like people, places, and organizations and is
generallyapplicable,domainswithspecializedknowledge(e.g.,science,medicine,law)willbenefit
fromfew-shotexemplarsspecializedtothosedomains.
The LLM can also be prompted to extract claims about detected entities. Claims are important
factual statements about entities, such as dates, events, and interactions with other entities. As
withentitiesandrelationships,in-contextlearningexemplarscanprovidedomain-specificguidance.
Claimdescriptionsextractedfromtheexampletetxchunkareasfollows:
• NeoChip’ssharessurgedduringtheirfirstweekoftradingontheNewTechExchange.
• NeoChipdebutedasapubliclylistedcompanyontheNewTechExchange.
• QuantumSystemsacquiredNeoChipin2016andheldownershipuntilNeoChipwentpublic.
SeeAppendixAforpromptsanddetailsonourimplementationofentityandclaimextraction.
3.1.3 Entities&Relationships→KnowledgeGraph
TheuseofanLLMtoextractentities,relationships,andclaimsisaformofabstractivesummarization–thesearemeaningfulsummariesofconceptsthat,inthecaseofrelationshipsandclaims,may
not be explicitly stated in the text. The entity/relationship/claim extraction processes creates multiple instances of a single element because an element is typically detected and extracted multiple
timesacrossdocuments.
In the final step of the knowledge graph extraction process, these instances of entities and relationships become individual nodes and edges in the graph. Entity descriptions are aggregated and
summarizedforeachnodeandedge.Relationshipsareaggregatedintographedges,wherethenumberofduplicatesforagivenrelationshipbecomesedgeweights. Claimsareaggregatedsimilarly.
Inthismanuscript,ouranalysisusesexactstringmatchingforentitymatching–thetaskofreconcilingdifferentextractednamesforthesameentity(BarlaugandGulla,2021;ChristenandChristen,
2012;Elmagarmidetal.,2006). However,softermatchingapproachescanbeusedwithminoradjustments to prompts or code. Furthermore, GraphRAG is generally resilient to duplicate entities
sinceduplicatesaretypicallyclusteredtogetherforsummarizationinsubsequentsteps.
3.1.4 KnowledgeGraph→GraphCommunities
Given the graph index created in the previous step, a variety of community detection algorithms
may be used to partition the graph into communities of strongly connected nodes (e.g., see the
surveys by Fortunato (2010) and Jin et al. (2021)). In our pipeline, we use Leiden community
detection(Traagetal.,2019)inahierarchicalmanner,recursivelydetectingsub-communitieswithin
eachdetectedcommunityuntilreachingleafcommunitiesthatcannolongerbepartitioned.
5

<!-- Page 6 -->

Eachlevelofthishierarchyprovidesacommunitypartitionthatcoversthenodesofthegraphina
mutuallyexclusive,collectivelyexhaustiveway,enablingdivide-and-conquerglobalsummarization.
AnillustrationofsuchhierarchicalpartitioningonanexampledatasetcanbefoundinAppendixB.
3.1.5 GraphCommunities→CommunitySummaries
Thenextstepcreatesreport-likesummariesofeachcommunityinthecommunityhierarchy,using
a method designed to scale to very large datasets. These summaries are independently useful as a
waytounderstandtheglobalstructureandsemanticsofthedataset,andmaythemselvesbeusedto
make sense of a corpus in the absence of a specific query. For example, a user may scan through
communitysummariesatonelevellookingforgeneralthemesofinterest,thenreadlinkedreports
atalowerlevelthatprovideadditionaldetailsforeachsubtopic. Here,however,wefocusontheir
utilityaspartofagraph-basedindexusedforansweringglobalqueries.
GraphRAG generates community summaries by adding various element summaries (for nodes,
edges,andrelatedclaims)toacommunitysummarytemplate. Communitysummariesfromlowerlevelcommunitiesareusedtogeneratesummariesforhigher-levelcommunitiesasfollows:
• Leaf-levelcommunities. Theelementsummariesofaleaf-levelcommunityareprioritized
and then iteratively added to the LLM context window until the token limit is reached.
Theprioritizationisasfollows: foreachcommunityedgeindecreasingorderofcombined
source and target node degree (i.e., overall prominence), add descriptions of the source
node,targetnode,theedgeitself,andrelatedclaims.
• Higher-level communities. If all element summaries fit within the token limit of the contextwindow,proceedasforleaf-levelcommunitiesandsummarizeallelementsummaries
within the community. Otherwise, rank sub-communities in decreasing order of element
summary tokens and iteratively substitute sub-community summaries (shorter) for their
associatedelementsummaries(longer)untiltheyfitwithinthecontextwindow.
3.1.6 CommunitySummaries→CommunityAnswers→GlobalAnswer
Givenauserquery,thecommunitysummariesgeneratedinthepreviousstepcanbeusedtogenerate
a final answer in a multi-stage process. The hierarchical nature of the community structure also
meansthatquestionscanbeansweredusingthecommunitysummariesfromdifferentlevels,raising
the question of whether a particular level in the hierarchical community structure offers the best
balanceofsummarydetailandscopeforgeneralsensemakingquestions(evaluatedinsection4).
Foragivencommunitylevel,theglobalanswertoanyuserqueryisgeneratedasfollows:
• Preparecommunitysummaries. Communitysummariesarerandomlyshuffledanddivided
into chunks of pre-specified token size. This ensures relevant information is distributed
acrosschunks,ratherthanconcentrated(andpotentiallylost)inasinglecontextwindow.
• Mapcommunityanswers. Intermediateanswersaregeneratedinparallel. TheLLMisalso
askedtogenerateascorebetween0-100indicatinghowhelpfulthegeneratedanswerisin
answeringthetargetquestion. Answerswithscore0arefilteredout.
• Reducetoglobalanswer. Intermediatecommunityanswersaresortedindescendingorder
ofhelpfulnessscoreanditerativelyaddedintoanewcontextwindowuntilthetokenlimit
isreached. Thisfinalcontextisusedtogeneratetheglobalanswerreturnedtotheuser.
3.2 GlobalSensemakingQuestionGeneration
To evaluate the effectiveness of RAG systems for global sensemaking tasks, we use an LLM to
generate a set of corpus-specific questions designed to asses high-level understanding of a given
corpus,withoutrequiringretrievalofspecificlow-levelfacts. Instead,givenahigh-leveldescription
of a corpus and its purposes, the LLM is prompted to generate personas of hypothetical users of
the RAG system. For each hypothetical user, the LLM is then prompted to specify tasks that this
user would use the RAG system to complete. Finally, for each combination of user and task, the
LLMispromptedtogeneratequestionsthatrequireunderstandingoftheentirecorpus. Algorithm
1describestheapproach.
6

<!-- Page 7 -->


### Algorithm1: PromptingProcedureforQuestionGeneration

1: Input: Description of a corpus, number of users K, number of tasks per user N, number of
questionsper(user,task)combinationM.
2: Output:AsetofK∗N∗M high-levelquestionsrequiringglobalunderstandingofthecorpus.
3: procedureGENERATEQUESTIONS
4: Basedonthecorpusdescription,prompttheLLMto:

## DescribepersonasofK potentialusersofthedataset.


## Foreachuser,identifyN tasksrelevanttotheuser.


## Specifictoeachuser&taskpair,generateM high-levelquestionsthat:

• Requireunderstandingoftheentirecorpus.
• Donotrequireretrievalofspecificlow-levelfacts.
5: CollectthegeneratedquestionstoproduceK∗N ∗M testquestionsforthedataset.
6: endprocedure
Forourevaluation,wesetK = M = N = 5foratotalof125testquestionsperdataset. Table1
showsexamplequestionsforeachofthetwoevaluationdatasets.
3.3 CriteriaforEvaluatingGlobalSensemaking
Given the lack of gold standard answers to our activity-based sensemaking questions, we adopt
the head-to-head comparison approach using an LLM evaluator that judges relative performance
accordingtospecificcriteria. Wedesignedthreetargetcriteriacapturingqualitiesthataredesirable
forglobalsensemakingactivities.
AppendixFshowsthepromptsforourhead-to-headmeasurescomputedusinganLLMevaluator,
summarizedas:
• Comprehensiveness. How much detail does the answer provide to cover all aspects and
detailsofthequestion?
• Diversity.Howvariedandrichistheanswerinprovidingdifferentperspectivesandinsights
onthequestion?
• Empowerment. Howwelldoestheanswerhelpthereaderunderstandandmakeinformed
judgmentsaboutthetopic?
Table 1: Examples of potential users, tasks, and questions generated by the LLM based on short
descriptionsofthetargetdatasets. Questionstargetglobalunderstandingratherthanspecificdetails.
Dataset Exampleactivityframingandgenerationofglobalsensemakingquestions
Podcast User:Atechjournalistlookingforinsightsandtrendsinthetechindustry
transcripts Task:Understandinghowtechleadersviewtheroleofpolicyandregulation

### Questions:


## Whichepisodesdealprimarilywithtechpolicyandgovernmentregulation?


## Howdoguestsperceivetheimpactofprivacylawsontechnologydevelopment?


## Doanyguestsdiscussthebalancebetweeninnovationandethicalconsiderations?


## Whatarethesuggestedchangestocurrentpoliciesmentionedbytheguests?


## Arecollaborationsbetweentechcompaniesandgovernmentsdiscussedandhow?

News User:Educatorincorporatingcurrentaffairsintocurricula
articles Task:Teachingabouthealthandwellness

### Questions:


## Whatcurrenttopicsinhealthcanbeintegratedintohealtheducationcurricula?


## Howdonewsarticlesaddresstheconceptsofpreventivemedicineandwellness?


## Arethereexamplesofhealtharticlesthatcontradicteachother,andifso,why?


## Whatinsightscanbegleanedaboutpublichealthprioritiesbasedonnewscoverage?


## Howcaneducatorsusethedatasettohighlighttheimportanceofhealthliteracy?

7

<!-- Page 8 -->

Furthermore, we use a “control criterion” called Directness that answers “How specifically and
clearlydoestheansweraddressthequestion?”. Inplainterms, directnessevaluatestheconcision
ofananswerinagenericsensethatappliestoanygeneratedLLMsummarization. Weincludeitto
behaveasareferenceagainstwhichwecanjudgethesoundnessofresultsfortheothercriteria.Since
directnessiseffectivelyinoppositiontocomprehensivenessanddiversity,wewouldnotexpectany
methodtowinacrossallfourcriteria.
Inourevaluations,theLLMisprovidedwiththequestion,thegeneratedanswersfromtwocompeting systems, and prompted to compare the two answers according to the criterion before giving a
finaljudgmentofwhichanswerispreferred. TheLLMeitherindicatesawinner;or,itreturnsatie
iftheyarefundamentallysimilar. ToaccountfortheinherentstochasticityofLLMgeneration,we
runeachcomparisonwithmultiplereplicatesandaveragetheresultsacrossreplicatesandquestions.
AnillustrationofLLMassessmentforanswerstoasamplequestioncanbefoundinAppendixD.
4 Analysis
4.1 Experiment1
4.1.1 Datasets
We selected two datasets in the one million token range, each representative of corpora that users
mayencounterintheirreal-worldactivities:
Podcast transcripts. Public transcripts of Behind the Tech with Kevin Scott, a podcast featuring
conversationsbetweenMicrosoftCTOKevinScottandvariousthoughtleadersinscienceandtechnology (Scott,2024). Thiscorpuswasdividedinto1669×600-tokentextchunks,with100-token
overlapsbetweenchunks(∼1milliontokens).
News articles. A benchmark dataset comprised of news articles published from September 2013
to December 2023 in a range of categories, including entertainment, business, sports, technology,
health, and science (Tang and Yang, 2024). The corpus is divided into 3197 × 600-token text
chunks,with100-tokenoverlapsbetweenchunks(∼1.7milliontokens).
4.1.2 Conditions
We compared six conditions including GraphRAG at four different graph community levels (C0,
C1,C2,C3),atextsummarizationmethodthatappliesourmap-reduceapproachdirectlytosource
texts(TS),andavectorRAG“semanticsearch”approach(SS):
• CO.Usesroot-levelcommunitysummaries(fewestinnumber)toansweruserqueries.
• C1. Useshigh-levelcommunitysummariestoanswerqueries. Thesearesub-communities
ofC0,ifpresent,otherwiseC0communitiesprojecteddownwards.
• C2. Uses intermediate-level community summaries to answer queries. These are subcommunitiesofC1,ifpresent,otherwiseC1communitiesprojecteddownwards.
• C3. Useslow-levelcommunitysummaries(greatestinnumber)toanswerqueries. These
aresub-communitiesofC2,ifpresent,otherwiseC2communitiesprojecteddownwards.
• TS. The same method as in Section 3.1.6, except source texts (rather than community
summaries)areshuffledandchunkedforthemap-reducesummarizationstages.
• SS.AnimplementationofvectorRAGinwhichtextchunksareretrievedandaddedtothe
availablecontextwindowuntilthespecifiedtokenlimitisreached.
The size of the context window and the prompts used for answer generation are the same across
allsixconditions(exceptforminormodificationstoreferencestylestomatchthetypesofcontext
informationused). Conditionsonlydifferinhowthecontentsofthecontextwindowarecreated.
ThegraphindexsupportingconditionsC0-C3wascreatedusingourgenericpromptsforentityand
relationshipextraction,withentitytypesandfew-shotexamplestailoredtothedomainofthedata.
8

<!-- Page 9 -->

4.1.3 Configuration
Weusedafixedcontextwindowsizeof8ktokensforgeneratingcommunitysummaries,community
answers,andglobalanswers(explainedinAppendixC). Graphindexingwitha600tokenwindow
(explainedinSectionA.2)took281minutesforthePodcastdataset, runningonavirtualmachine
(16GB RAM, Intel(R) Xeon(R) Platinum 8171M CPU @ 2.60GHz) and using a public OpenAI
endpointforgpt-4-turbo(2MTPM,10kRPM).
We implemented Leiden community detection using the graspologic library (Chung et al., 2019).
The prompts used to generate the graph index and global answers can be found in Appendix E,
whilethepromptsusedtoevaluateLLMresponsesagainstourcriteriacanbefoundinAppendixF.
AfullstatisticalanalysisoftheresultspresentedinthenextsectioncanbefoundinAppendixG.
4.2 Experiment2
TovalidatethecomprehensivenessanddiversityresultsfromExperiment1,weimplementedclaimbased measures of these qualities. We use the definition of a factual claim from Ni et al. (2024),
which is “a statement that explicitly presents some verifiable facts.” For example, the sentence
“CaliforniaandNewYorkimplementedincentivesforrenewableenergyadoption,highlightingthe
broaderimportanceofsustainabilityinpolicydecisions”containstwofactualclaims: (1)California
implementedincentivesforrenewableenergyadoption,and(2)NewYorkimplementedincentives
forrenewableenergyadoption.
To extract factual claims, we used Claimify (Metropolitansky and Larson, 2025), an LLM-based
methodthatidentifiessentencesinananswercontainingatleastonefactualclaim,thendecomposes
these sentences into simple, self-contained factual claims. We applied Claimify to the answers
generated under the conditions from Experiment 1. After removing duplicate claims from each
answer,weextracted47,075uniqueclaims,withanaverageof31claimsperanswer.
Wedefinedtwometrics,withhighervaluesindicatingbetterperformance:

## Comprehensiveness: Measured as the average number of claims extracted from the answersgeneratedundereachcondition.


## Diversity: Measuredbyclusteringtheclaimsforeachanswerandcalculatingtheaverage

numberofclusters.
Forclustering,wefollowedtheapproachdescribedbyPadmakumarandHe(2024),whichinvolved
using Scikit-learn’s implementation of agglomerative clustering (Pedregosa et al., 2011). Clusters
weremergedthrough“complete”linkage,meaningtheywerecombinedonlyifthemaximumdistance between their farthest points was less than or equal to a predefined distance threshold. The
distance metric used was 1−ROUGE-L. Since the distance threshold influences the number of
clusters,wereportresultsacrossarangeofthresholds.
5 Results
5.1 Experiment1
Theindexingprocessresultedinagraphconsistingof8,564nodesand20,691edgesforthePodcast
dataset,andalargergraphof15,754nodesand19,520edgesfortheNewsdataset. Table2shows
thenumberofcommunitysummariesatdifferentlevelsofeachgraphcommunityhierarchy.
Globalapproachesvs. vectorRAG.AsshowninFigure2andTable6,globalapproachessignificantlyoutperformedconventionalvectorRAG(SS)inbothcomprehensivenessanddiversitycriteria
acrossdatasets. Specifically,globalapproachesachievedcomprehensivenesswinratesbetween72-
83%(p<.001)forPodcasttranscriptsand72-80%(p<.001)forNewsarticles,whilediversitywin
rates ranged from 75-82% (p<.001) and 62-71% (p<.01) respectively. Our use of directness as a
validitytestconfirmedthatvectorRAGproducesthemostdirectresponsesacrossallcomparisons.
Empowerment. EmpowermentcomparisonsshowedmixedresultsforbothglobalapproachesversusvectorRAG(SS)andGraphRAGapproachesversussourcetextsummarization(TS).Usingan
LLMtoanalyzeLLMreasoningforthismeasureindicatedthattheabilitytoprovidespecificexam-
9

<!-- Page 10 -->


### Podcasttranscripts

SS TS C0 C1 C2 C3 SS TS C0 C1 C2 C3 SS TS C0 C1 C2 C3 SS TS C0 C1 C2 C3
SS 50 17 28 25 22 21 SS 50 18 23 25 19 19 SS 50 42 57 52 49 51 SS 50 56 65 60 60 60
TS 83 50 50 48 43 44 TS 82 50 50 50 43 46 TS 58 50 59 55 52 51 TS 44 50 55 52 51 52
C0 72 50 50 53 50 49 C0 77 50 50 50 46 44 C0 43 41 50 49 47 48 C0 35 45 50 47 48 48
C1 75 52 47 50 52 50 C1 75 50 50 50 44 45 C1 48 45 51 50 49 50 C1 40 48 53 50 50 50
C2 78 57 50 48 50 52 C2 81 57 54 56 50 48 C2 51 48 53 51 50 51 C2 40 49 52 50 50 50
C3 79 56 51 50 48 50 C3 81 54 56 55 52 50 C3 49 49 52 50 49 50 C3 40 48 52 50 50 50
Comprehensiveness Diversity Empowerment Directness

### Newsarticles

SS TS C0 C1 C2 C3 SS TS C0 C1 C2 C3 SS TS C0 C1 C2 C3 SS TS C0 C1 C2 C3
SS 50 20 28 25 21 21 SS 50 33 38 35 29 31 SS 50 47 57 49 50 50 SS 50 54 59 55 55 54
TS 80 50 44 41 38 36 TS 67 50 53 45 44 40 TS 53 50 58 50 50 48 TS 46 50 55 53 52 52
C0 72 56 50 52 54 52 C0 62 47 50 40 41 41 C0 43 42 50 42 45 44 C0 41 45 50 48 48 47
C1 75 59 48 50 58 55 C1 65 55 60 50 50 50 C1 51 50 58 50 52 51 C1 45 47 52 50 49 49
C2 79 62 46 42 50 59 C2 71 56 59 50 50 51 C2 50 50 55 48 50 50 C2 45 48 52 51 50 49
C3 79 64 48 45 41 50 C3 69 60 59 50 49 50 C3 50 52 56 49 50 50 C3 46 48 53 51 51 50

### Comprehensiveness Diversity Empowerment Directness

Figure2: Head-to-headwinratepercentagesof(rowcondition)over(columncondition)acrosstwo
datasets, four metrics, and 125 questions per comparison (each repeated five times and averaged).
Theoverallwinnerperdatasetandmetricisshowninbold. Self-winrateswerenotcomputedbut
areshownastheexpected50%forreference. AllGraphRAGconditionsoutperformedna¨ıveRAG
oncomprehensivenessanddiversity. ConditionsC1-C3alsoshowedslightimprovementsinanswer
comprehensivenessanddiversityoverTS(globaltextsummarizationwithoutagraphindex).
Table2:Numberofcontextunits(communitysummariesforC0-C3andtextchunksforTS),correspondingtokencounts,andpercentageofthemaximumtokencount. Map-reducesummarizationof
sourcetextsisthemostresource-intensiveapproachrequiringthehighestnumberofcontexttokens.
Root-levelcommunitysummaries(C0)requiredramaticallyfewertokensperquery(9x-43x).
PodcastTranscripts NewsArticles

### C0 C1 C2 C3 Ts C0 C1 C2 C3 Ts


### Units 34 367 969 1310 1669 55 555 1797 2142 3197

Tokens 26657 225756 565720 746100 1014611 39770 352641 980898 1140266 1707694
%Max 2.6 22.2 55.8 73.5 100 2.3 20.7 57.4 66.8 100
ples,quotes,andcitationswasjudgedtobekeytohelpingusersreachaninformedunderstanding.
TuningelementextractionpromptsmayhelptoretainmoreofthesedetailsintheGraphRAGindex.
Communitysummariesvs. sourcetexts. Whencomparingcommunitysummariestosourcetexts
using GraphRAG, community summaries generally provided a small but consistent improvement
in answer comprehensiveness and diversity, except for root-level summaries. Intermediate-level
summariesinthePodcastdatasetandlow-levelcommunitysummariesintheNewsdatasetachieved
comprehensivenesswinratesof57%(p<.001)and64%(p<.001),respectively. Diversitywinrates
were57%(p=.036)forPodcastintermediate-levelsummariesand60%(p<.001)forNewslow-level
communitysummaries. Table2alsoillustratesthescalabilityadvantagesofGraphRAGcompared
to source text summarization: for low-level community summaries (C3), GraphRAG required 26-
33% fewer context tokens, while for root-level community summaries (C0), it required over 97%
fewer tokens. For a modest drop in performance compared with other global methods, root-level
GraphRAG offers a highly efficient method for the iterative question answering that characterizes
sensemakingactivity,whileretainingadvantagesincomprehensiveness(72%winrate)anddiversity
(62%winrate)overvectorRAG.
10

<!-- Page 11 -->

Table3:Averagenumberofextractedclaims,reportedbyconditionanddatasettype. Boldedvalues
representthehighestscoreineachcolumn.
AverageNumberofClaims

### Condition

NewsArticles PodcastTranscripts

## C0 34.18 32.21


## C1 32.50 32.20


## C2 31.62 32.46


## C3 33.14 32.28


## Ts 32.89 31.39


## Ss 25.23 26.50

5.2 Experiment2
Table3showstheresultsfortheaveragenumberofextractedclaims(i.e.,theclaim-basedmeasure
of comprehensiveness) per condition. For both the News and Podcast datasets, all global search
conditions(C0-C3)andsourcetextsummarization(TS)hadgreatercomprehensivenessthanvector
RAG(SS).Thedifferenceswerestatisticallysignificant(p<.05)inallcases. Thesefindingsalign
withtheLLM-basedwinratesfromExperiment1.
Table 4 contains the results for the average number of clusters, the claim-based measure of diversity. ForthePodcastdataset,allglobalsearchconditionshadsignificantlygreaterdiversitythanSS
acrossalldistancethresholds(p<.05),consistentwiththewinratesobservedinExperiment1. For
the News dataset, however, only C0 significantly outperformed SS across all distance thresholds
(p<.05). While C1-C3 also achieved higher average cluster counts than SS, the differences were
statisticallysignificantonlyatcertaindistancethresholds. InExperiment1, allglobalsearchconditionssignificantlyoutperformedSSintheNewsdataset–notjustC0. However, thedifferences
in mean diversity scores between SS and the global search conditions were smaller for the News
datasetthanforthePodcastdataset,aligningdirectionallywiththeclaim-basedresults.
Forbothcomprehensivenessanddiversity,acrossbothdatasets,therewerenostatisticallysignificant
differencesobservedamongtheglobalsearchconditionsorbetweenglobalsearchandTS.
Finally,foreachpairwisecomparisoninExperiment1,wetestedwhethertheanswerpreferredby
theLLMalignedwiththewinnerbasedontheclaim-basedmetrics.Sinceeachpairwisecomparison
inExperiment1wasperformedfivetimes,whiletheclaim-basedmetricsprovidedonlyoneoutcome
per comparison, we aggregated the Experiment 1 results into a single label using majority voting.
For example, if C0 won over SS in three out of five judgments for comprehensiveness on a given
question,C0waslabeledthewinnerandSStheloser. However,ifC0wontwice,SSwononce,and
theytiedtwice,thentherewasnomajorityoutcome,sothefinallabelwasatie.
Wefoundthatexacttieswererarefortheclaim-basedmetrics. Onepossiblesolutionistodefinea
tiebasedonathreshold(e.g.,theabsolutedifferencebetweentheclaim-basedresultsforcondition
A and condition B must be less than or equal to x). However, we observed that the results were
sensitive to the choice of threshold. As a result, we focused on cases where the aggregated LLM
labelwasnotatie,representing33%and39%ofpairwisecomparisonsforcomprehensivenessand
diversity, respectively. Inthesecases, theaggregatedLLMlabelmatchedtheclaim-basedlabelin
78%ofpairwisecomparisonsforcomprehensivenessand69-70%fordiversity(acrossalldistance
thresholds),indicatingmoderatelystrongalignment.
6 Discussion
6.1 Limitationsofevaluationapproach
Ourevaluationtodatehasfocusedonsensemakingquestionsspecifictotwocorporaeachcontaining
approximately1milliontokens.Moreworkisneededtounderstandhowperformancegeneralizesto
datasetsfromvariousdomainswithdifferentusecases. Comparisonoffabricationrates,e.g.,using
approacheslikeSelfCheckGPT(Manakuletal.,2023),wouldalsostrengthenthecurrentanalysis.
11

<!-- Page 12 -->

Table4: Averagenumberofclustersacrossdifferentdistancethresholds,reportedbyconditionand
datasettype. Boldedvaluesrepresentthehighestscoreineachrow.

### AverageNumberofClusters

Dataset DistanceThreshold

## C0 C1 C2 C3 Ts Ss

0.5 23.42 21.85 21.90 22.13 21.80 17.92
0.6 21.65 20.38 20.30 20.52 20.13 16.78

### NewsArticles

0.7 20.19 19.06 19.03 19.13 18.62 15.80
0.8 18.86 17.78 17.82 17.79 17.30 14.80
0.5 23.16 22.62 22.52 21.93 21.14 18.55
0.6 21.65 21.33 21.21 20.62 19.70 17.39

### PodcastTranscripts

0.7 20.41 20.04 19.79 19.22 18.08 16.28
0.8 19.26 18.77 18.46 17.89 16.66 15.07
6.2 Futurework
Thegraphindex,richtextannotations,andhierarchicalcommunitystructuresupportingthecurrent
GraphRAG approach offer many possibilities for refinement and adaptation. This includes RAG
approachesthatoperateinamorelocalmanner,viaembedding-basedmatchingofuserqueriesand
graphannotations. Inparticular,weseepotentialinhybridRAGschemesthatcombineembeddingbased matching with just-in-time community report generation before employing our map-reduce
summarizationmechanisms. This“roll-up”approachcouldalsobeextendedacrossmultiplelevels
ofthecommunityhierarchy,aswellasimplementedasamoreexploratory“drilldown”mechanism
thatfollowstheinformationscentcontainedinhigher-levelcommunitysummaries.
Broader impacts. As a mechanism for question answering over large document collections, there
are risks to downstream sensemaking and decision-making tasks if the generated answers do not
accuratelyrepresentthesourcedata. SystemuseshouldbeaccompaniedbycleardisclosuresofAI
useand thepotential forerrors inoutputs. Comparedto vectorRAG, however, GraphRAG shows
promiseasawaytomitigatethesedownstreamrisksforquestionsofaglobalnature,whichmight
otherwisebeansweredbysamplesofretrievedfactsfalselypresentedasglobalsummaries.
7 Conclusion
We have presented GraphRAG, a RAG approach that combines knowledge graph generation and
query-focusedsummarization(QFS)tosupporthumansensemakingoverentiretextcorpora. Initial
evaluationsshowsubstantialimprovementsoveravectorRAGbaselineforboththecomprehensivenessanddiversityofanswers,aswellasfavorablecomparisonstoaglobalbutgraph-freeapproach
usingmap-reducesourcetextsummarization. Forsituationsrequiringmanyglobalqueriesoverthe
same dataset, summaries of root-level communities in the entity-based graph index provide a data
index that is both superior to vector RAG and achieves competitive performance to other global
methodsatafractionofthetokencost.

### Acknowledgements

We would also like to thank the following people who contributed to the work: Alonso Guevara
Ferna´ndez, Amber Hoak, Andre´s Morales Esquivel, Ben Cutler, Billie Rinaldi, Chris Sanchez,
Chris Trevino, Christine Caggiano, David Tittsworth, Dayenne de Souza, Douglas Orbaker, Ed
Clark,GabrielNieves-Ponce,GaudyBlancoMeneses,KateLytvynets,KatySmith,Mo´nicaCarvajal,NathanEvans,RichardOrtega,RodrigoRacanicci,SarahSmith,andShaneSolomon.

### References

Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. (2023). Gpt-4 technical report. arXiv preprint
arXiv:2303.08774.
12

<!-- Page 13 -->

Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M.,
Hauth,A.,etal.(2023). Gemini: afamilyofhighlycapablemultimodalmodels. arXivpreprint
arXiv:2312.11805.
Baek, J., Aji, A.F., andSaffari, A.(2023). Knowledge-augmentedlanguagemodelpromptingfor
zero-shotknowledgegraphquestionanswering. arXivpreprintarXiv:2306.04136.
Ban,T.,Chen,L.,Wang,X.,andChen,H.(2023).Fromquerytoolstocausalarchitects:Harnessing
largelanguagemodelsforadvancedcausaldiscoveryfromdata.
Barlaug,N.andGulla,J.A.(2021). Neuralnetworksforentitymatching: Asurvey. ACMTransactionsonKnowledgeDiscoveryfromData(TKDD),15(3):1–37.
Baumel,T.,Eyal,M.,andElhadad,M.(2018). Queryfocusedabstractivesummarization: Incorporating query relevance, multi-document coverage, and summary length constraints into seq2seq
models. arXivpreprintarXiv:1801.07704.
Blondel, V. D., Guillaume, J.-L., Lambiotte, R., and Lefebvre, E. (2008). Fast unfolding of
communities in large networks. Journal of statistical mechanics: theory and experiment,

## 2008(10):P10008.

Brown,T.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,J.D.,Dhariwal,P.,Neelakantan,A.,Shyam,
P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in
neuralinformationprocessingsystems,33:1877–1901.
Cheng, X., Luo, D., Chen, X., Liu, L., Zhao, D., and Yan, R. (2024). Lift yourself up: Retrievalaugmented text generation with self-memory. Advances in Neural Information Processing Systems,36.
Christen,P.andChristen,P.(2012). Thedatamatchingprocess. Springer.
Chung, J., Pedigo, B. D., Bridgeford, E. W., Varjavand, B. K., Helm, H. S., and Vogelstein, J. T.
(2019). Graspy:Graphstatisticsinpython. JournalofMachineLearningResearch,20(158):1–7.
Dang,H.T.(2006). Duc2005:Evaluationofquestion-focusedsummarizationsystems. InProceedingsoftheWorkshoponTask-FocusedSummarizationandQuestionAnswering,pages48–55.
Elmagarmid, A. K., Ipeirotis, P. G., and Verykios, V. S. (2006). Duplicate record detection: A
survey. IEEETransactionsonknowledgeanddataengineering,19(1):1–16.
Es, S., James, J., Espinosa-Anke, L., and Schockaert, S. (2023). Ragas: Automated evaluation of
retrievalaugmentedgeneration. arXivpreprintarXiv:2309.15217.
Etzioni,O.,Cafarella,M.,Downey,D.,Kok,S.,Popescu,A.-M.,Shaked,T.,Soderland,S.,Weld,
D. S., and Yates, A. (2004). Web-scale information extraction in knowitall: (preliminary results). InProceedingsofthe13thInternationalConferenceonWorldWideWeb,WWW’04,page
100–110,NewYork,NY,USA.AssociationforComputingMachinery.
Feng,Z.,Feng,X.,Zhao,D.,Yang,M.,andQin,B.(2023).Retrieval-generationsynergyaugmented
largelanguagemodels. arXivpreprintarXiv:2310.05149.
Fortunato,S.(2010). Communitydetectioningraphs. Physicsreports,486(3-5):75–174.
Gao,Y.,Xiong,Y.,Gao,X.,Jia,K.,Pan,J.,Bi,Y.,Dai,Y.,Sun,J.,andWang,H.(2023). Retrievalaugmentedgenerationforlargelanguagemodels: Asurvey. arXivpreprintarXiv:2312.10997.
He,X.,Tian,Y.,Sun,Y.,Chawla,N.V.,Laurent,T.,LeCun,Y.,Bresson,X.,andHooi,B.(2024).
G-retriever: Retrieval-augmented generation for textual graph understanding and question answering. arXivpreprintarXiv:2402.07630.
Huang, J., Chen, X., Mishra, S., Zheng, H. S., Yu, A. W., Song, X., and Zhou, D. (2023). Large
languagemodelscannotself-correctreasoningyet. arXivpreprintarXiv:2310.01798.
13

<!-- Page 14 -->

Jacomy, M., Venturini, T., Heymann, S., andBastian, M.(2014). Forceatlas2, acontinuousgraph
layout algorithm for handy network visualization designed for the gephi software. PLoS ONE
9(6): e98679.https://doi.org/10.1371/journal.pone.0098679.
Jin, D., Yu, Z., Jiao, P., Pan, S., He, D., Wu, J., Philip, S. Y., and Zhang, W. (2021). A survey of
communitydetectionapproaches: Fromstatisticalmodelingtodeeplearning. IEEETransactions
onKnowledgeandDataEngineering,35(2):1149–1170.
Kang,M.,Kwak,J.M.,Baek,J.,andHwang,S.J.(2023). Knowledgegraph-augmentedlanguage
modelsforknowledge-groundeddialoguegeneration. arXivpreprintarXiv:2305.18846.
Khattab, O., Santhanam, K., Li, X. L., Hall, D., Liang, P., Potts, C., and Zaharia, M. (2022).
Demonstrate-search-predict: Composingretrievalandlanguagemodelsforknowledge-intensive
nlp. arXivpreprintarXiv:2212.14024.
Kim,D.,Xie,L.,andOng,C.S.(2016).Probabilisticknowledgegraphconstruction:Compositional
and incremental approaches. In Proceedings of the 25th ACM International on Conference on
Information and Knowledge Management, CIKM ’16, page 2257–2262, New York, NY, USA.
AssociationforComputingMachinery.
Kim,G.,Kim,S.,Jeon,B.,Park,J.,andKang,J.(2023). Treeofclarifications: Answeringambiguousquestionswithretrieval-augmentedlargelanguagemodels.arXivpreprintarXiv:2310.14696.
Klein, G., Moon, B., and Hoffman, R. R. (2006). Making sense of sensemaking 1: Alternative
perspectives. IEEEintelligentsystems,21(4):70–73.
Kosinski,M.(2024). Evaluatinglargelanguagemodelsintheoryofmindtasks. Proceedingsofthe
NationalAcademyofSciences,121(45):e2405460121.
Kuratov,Y.,Bulatov,A.,Anokhin,P.,Sorokin,D.,Sorokin,A.,andBurtsev,M.(2024). Insearch
ofneedlesina11mhaystack: Recurrentmemoryfindswhatllmsmiss.
LangChain(2024). Langchaingraphs. https://langchain-graphrag.readthedocs.io/en/latest/.
Laskar, M. T. R., Hoque, E., and Huang, J. (2020). Query focused abstractive summarization via
incorporating query relevance and transfer learning with transformer models. In Advances in
ArtificialIntelligence: 33rdCanadianConferenceonArtificialIntelligence, CanadianAI2020,
Ottawa,ON,Canada,May13–15,2020,Proceedings33,pages342–348.Springer.
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Ku¨ttler, H., Lewis, M., Yih,
W.-t.,Rockta¨schel,T.,etal.(2020). Retrieval-augmentedgenerationforknowledge-intensivenlp
tasks. AdvancesinNeuralInformationProcessingSystems,33:9459–9474.
Liu,N.F.,Lin,K.,Hewitt,J.,Paranjape,A.,Bevilacqua,M.,Petroni,F.,andLiang,P.(2023). Lost
inthemiddle: Howlanguagemodelsuselongcontexts. arXiv:2307.03172.
LlamaIndex (2024). GraphRAG Implementation with LlamaIndex - V2. https://github.com/runllama/llama index/blob/main/docs/docs/examples/cookbooks/GraphRAG v2.ipynb.
Madaan,A.,Tandon,N.,Gupta,P.,Hallinan,S.,Gao,L.,Wiegreffe,S.,Alon,U.,Dziri,N.,Prabhumoye,S.,Yang,Y.,etal.(2024). Self-refine: Iterativerefinementwithself-feedback. Advances
inNeuralInformationProcessingSystems,36.
Manakul,P.,Liusie,A.,andGales,M.J.(2023). Selfcheckgpt: Zero-resourceblack-boxhallucinationdetectionforgenerativelargelanguagemodels. arXivpreprintarXiv:2303.08896.
Mao, Y., He, P., Liu, X., Shen, Y., Gao, J., Han, J., and Chen, W. (2020). Generation-augmented
retrievalforopen-domainquestionanswering. arXivpreprintarXiv:2009.08553.
Martin,S.,Brown,W.M.,Klavans,R.,andBoyack,K.(2011). Openord: Anopen-sourcetoolbox
forlargegraphlayout. SPIEConferenceonVisualizationandDataAnalysis(VDA).
Melnyk,I.,Dognin,P.,andDas,P.(2022). Knowledgegraphgenerationfromtext.
14

<!-- Page 15 -->

Metropolitansky, D. and Larson, J. (2025). Towards effective extraction and evaluation of factual
claims.
Microsoft(2023). Theimpactoflargelanguagemodelsonscientificdiscovery: apreliminarystudy
usinggpt-4.
Mooney,R.J.andBunescu,R.(2005). Miningknowledgefromtextusinginformationextraction.
SIGKDDExplor.Newsl.,7(1):3–10.
NebulaGraph(2024). Nebulagraphlaunchesindustry-firstgraphrag: Retrieval-augmentedgenerationwithllmbasedonknowledgegraphs. https://www.nebula-graph.io/posts/graph-RAG.
Neo4J (2024). Get started with graphrag: Neo4j’s ecosystem tools. https://neo4j.com/developerblog/graphrag-ecosystem-tools/.
Newman, M. E. (2006). Modularity and community structure in networks. Proceedings of the
nationalacademyofsciences,103(23):8577–8582.
Ni,J.,Shi,M.,Stammbach,D.,Sachan,M.,Ash,E.,andLeippold,M.(2024). AFaCTA:Assisting
theannotationoffactualclaimdetectionwithreliableLLMannotators.InKu,L.-W.,Martins,A.,
andSrikumar,V.,editors,Proceedingsofthe62ndAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:LongPapers),pages1890–1912,Bangkok,Thailand.Association
forComputationalLinguistics.
OpenAI(2023). Chatgpt: Gpt-4languagemodel.
Padmakumar,V.andHe,H.(2024). Doeswritingwithlanguagemodelsreducecontentdiversity?

## Iclr.

Pedregosa,F.,Varoquaux,G.,Gramfort,A.,Michel,V.,Thirion,B.,Grisel,O.,Blondel,M.,Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M.,
Perrot, M., and Duchesnay, E. (2011). Scikit-learn: Machine learning in python. Journal of
MachineLearningResearch,12:2825–2830.
Ram, O., Levine, Y., Dalmedigos, I., Muhlgay, D., Shashua, A., Leyton-Brown, K., and Shoham,
Y.(2023). In-contextretrieval-augmentedlanguagemodels. TransactionsoftheAssociationfor
ComputationalLinguistics,11:1316–1331.
Ranade, P. and Joshi, A. (2023). Fabula: Intelligence report generation using retrieval-augmented
narrativeconstruction. arXivpreprintarXiv:2310.13848.
Salminen, J., Liu, C., Pian, W., Chi, J., Ha¨yha¨nen, E., andJansen, B.J.(2024). Deusexmachina
andpersonasfromlargelanguagemodels: Investigatingthecompositionofai-generatedpersona
descriptions. In Proceedings of the CHI Conference on Human Factors in Computing Systems,
pages1–20.
Sarthi, P., Abdullah, S., Tuli, A., Khanna, S., Goldie, A., and Manning, C. D. (2024). Raptor:
Recursiveabstractiveprocessingfortree-organizedretrieval. arXivpreprintarXiv:2401.18059.
Scott,K.(2024). BehindtheTech. https://www.microsoft.com/en-us/behind-the-tech.
Shao, Z., Gong, Y., Shen, Y., Huang, M., Duan, N., and Chen, W. (2023). Enhancing retrievalaugmented large language models with iterative retrieval-generation synergy. arXiv preprint
arXiv:2305.15294.
Shin,J.,Hedderich,M.A.,Rey,B.J.,Lucero,A.,andOulasvirta,A.(2024).Understandinghumanai workflows for generating personas. In Proceedings of the 2024 ACM Designing Interactive
SystemsConference,pages757–781.
Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K., and Yao, S. (2024). Reflexion: Language
agentswithverbalreinforcementlearning. AdvancesinNeuralInformationProcessingSystems,
36.
15

<!-- Page 16 -->

Su, D., Xu, Y., Yu, T., Siddique, F. B., Barezi, E. J., and Fung, P. (2020). Caire-covid: A questionansweringandquery-focusedmulti-documentsummarizationsystemforcovid-19scholarly
informationmanagement. arXivpreprintarXiv:2005.03975.
Tan,Z.,Zhao,X.,andWang,W.(2017). Representationlearningoflarge-scaleknowledgegraphs
viaentityfeaturecombinations. InProceedingsofthe2017ACMonConferenceonInformation
andKnowledgeManagement,CIKM’17,page1777–1786,NewYork,NY,USA.Associationfor
ComputingMachinery.
Tang, Y.andYang, Y.(2024). MultiHop-RAG:Benchmarkingretrieval-augmentedgenerationfor
multi-hopqueries. arXivpreprintarXiv:2401.15391.
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S.,
Bhargava, P., Bhosale, S., etal.(2023). Llama2: Openfoundationandfine-tunedchatmodels.
arXivpreprintarXiv:2307.09288.
Traag, V. A., Waltman, L., and Van Eck, N. J. (2019). From Louvain to Leiden: guaranteeing
well-connectedcommunities. ScientificReports,9(1).
Trajanoska, M., Stojanov, R., and Trajanov, D. (2023). Enhancing knowledge graph construction
usinglargelanguagemodels. ArXiv,abs/2305.04676.
Trivedi, H., Balasubramanian, N., Khot, T., and Sabharwal, A. (2022). Interleaving retrieval
with chain-of-thought reasoning for knowledge-intensive multi-step questions. arXiv preprint
arXiv:2212.10509.
Wang,J.,Liang,Y.,Meng,F.,Sun,Z.,Shi,H.,Li,Z.,Xu,J.,Qu,J.,andZhou,J.(2023a). Ischatgpt
agoodnlgevaluator? apreliminarystudy. arXivpreprintarXiv:2303.04048.
Wang,S.,Khramtsova,E.,Zhuang,S.,andZuccon,G.(2024).Feb4rag:Evaluatingfederatedsearch
inthecontextofretrievalaugmentedgeneration. arXivpreprintarXiv:2402.11891.
Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D.
(2022). Self-consistencyimproveschainofthoughtreasoninginlanguagemodels. arXivpreprint
arXiv:2203.11171.
Wang, Y., Lipka, N., Rossi, R. A., Siu, A., Zhang, R., and Derr, T. (2023b). Knowledge graph
promptingformulti-documentquestionanswering.
Xu, Y. and Lapata, M. (2021). Text summarization with latent queries. arXiv preprint
arXiv:2106.00104.
Yang,Z.,Qi,P.,Zhang,S.,Bengio,Y.,Cohen,W.W.,Salakhutdinov,R.,andManning,C.D.(2018).
HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Conference on
EmpiricalMethodsinNaturalLanguageProcessing(EMNLP).
Yao,J.-g.,Wan,X.,andXiao,J.(2017). Recentadvancesindocumentsummarization. Knowledge
andInformationSystems,53:297–336.
Yao, L., Peng, J., Mao, C., and Luo, Y. (2023). Exploring large language models for knowledge
graphcompletion.
Yates, A., Banko, M., Broadhead, M., Cafarella, M., Etzioni, O., and Soderland, S. (2007). TextRunner: Open information extraction on the web. In Carpenter, B., Stent, A., and Williams,
J. D., editors, Proceedings of Human Language Technologies: The Annual Conference of the
NorthAmericanChapteroftheAssociationforComputationalLinguistics(NAACL-HLT),pages
25–26,Rochester,NewYork,USA.AssociationforComputationalLinguistics.
Yuan,X.,Li,J.,Wang,D.,Chen,Y.,Mao,X.,Huang,L.,Xue,H.,Wang,W.,Ren,K.,andWang,
J.(2024). S-eval: Automaticandadaptivetestgenerationforbenchmarkingsafetyevaluationof
largelanguagemodels. arXivpreprintarXiv:2405.14191.
Zhang, J. (2023). Graph-toolformer: To empower llms with graph reasoning ability via prompt
augmentedbychatgpt. arXivpreprintarXiv:2304.11116.
16

<!-- Page 17 -->

Zhang,Y.,Zhang,Y.,Gan,Y.,Yao,L.,andWang,C.(2024a).Causalgraphdiscoverywithretrievalaugmentedgenerationbasedlargelanguagemodels. arXivpreprintarXiv:2402.15301.
Zhang,Z.,Chen,J.,andYang,D.(2024b). Darg: Dynamicevaluationoflargelanguagemodelsvia
adaptivereasoninggraph. arXivpreprintarXiv:2406.17271.
Zheng,L.,Chiang,W.-L.,Sheng,Y.,Zhuang,S.,Wu,Z.,Zhuang,Y.,Lin,Z.,Li,Z.,Li,D.,Xing,
E., etal.(2024). Judgingllm-as-a-judgewithmt-benchandchatbotarena. AdvancesinNeural
InformationProcessingSystems,36.
Zhu, Y., Wang, X., Chen, J., Qiao, S., Ou, Y., Yao, Y., Deng, S., Chen, H., andZhang, N.(2024).
Llmsforknowledgegraphconstructionandreasoning: Recentcapabilitiesandfutureopportunities.
17

<!-- Page 18 -->


### A EntityandRelationshipExtractionApproach

The following prompts, designed for GPT-4, are used in the default GraphRAG initialization
pipeline:
• DefaultGraphExtractionPrompt
• ClaimExtractionPrompt

### A.1 EntityExtraction

WedothisusingamultipartLLMpromptthatfirstidentifiesallentitiesinthetext,includingtheir
name, type, and description, before identifying all relationships between clearly related entities,
includingthesourceandtargetentitiesandadescriptionoftheirrelationship. Bothkindsofelement
instanceareoutputinasinglelistofdelimitedtuples.

### A.2 Self-Reflection

Thechoiceofpromptengineeringtechniqueshasastrongimpactonthequalityofknowledgegraph
extraction (Zhuetal.,2024), anddifferenttechniqueshavedifferentcostsintermsoftokensconsumed and generated by the model. Self-reflection is a prompt engineering technique where the
LLM generates an answer, and is then prompted to evaluate its output for correctness, clarity, or
completeness, then finally generate an improved response based on that evaluation (Huang et al.,
2023; Madaan et al., 2024; Shinn et al., 2024; Wang et al., 2022). We leverage self-reflection in
knowledge graph extraction, and explore ways how removing self-reflection affects performance
andcost.
Using larger chunk size is less costly in terms of calls to the LLM. However, the LLM tends to
extractfewentitiesfromchunksoflargersize. Forexample,inasampledataset(HotPotQA,Yang
etal.,2018),GPT-4extractedalmosttwiceasmanyentityreferenceswhenthechunksizewas600
tokensthanwhenitwas2400. Toaddressthisissue,wedeployaself-reflectionpromptengineering
approach. After entities are extracted from a chunk, we provide the extracted entities back to the
LLM, prompting it to “glean” any entities that it may have missed. This is a multi-stage process
in which we first ask the LLM to assess whether all entities were extracted, using a logit bias of
100toforceayes/nodecision. IftheLLMrespondsthatentitiesweremissed,thenacontinuation
indicating that “MANY entities were missed in the last extraction” encourages the LLM to detect
thesemissingentities. Thisapproachallowsustouselargerchunksizeswithoutadropinquality
(Figure 3) or the forced introduction of noise. We interate self-reflection steps up to a specified
maximumnumberoftimes.
30000
20000
10000
0
0 1 2 3
Numberofself-reflectioniterationsperformed
detcetedsecnereferytitnE
600chunksize
1200chunksize
2400chunksize
Figure3: HowtheentityreferencesdetectedintheHotPotQAdataset(Yangetal.,2018)
varieswithchunksizeandself-reflectioniterationsforourgenericentityextractionpromptwith
gpt-4-turbo.
B ExampleCommunityDetection
18

<!-- Page 19 -->

(a)Rootcommunitiesatlevel0 (b)Sub-communitiesatlevel1
Figure 4: Graph communities detected using the Leiden algorithm (Traag et al., 2019) over the
MultiHop-RAG(TangandYang,2024)datasetasindexed. Circlesrepresententitynodeswithsize
proportional to their degree. Node layout was performed via OpenORD (Martin et al., 2011) and
ForceAtlas2(Jacomyetal.,2014). Nodecolorsrepresententitycommunities,shownattwolevels
of hierarchical clustering: (a) Level 0, corresponding to the hierarchical partition with maximum
modularity,and(b)Level1,whichrevealsinternalstructurewithintheseroot-levelcommunities.

### C ContextWindowSelection

The effect of context window size on any particular task is unclear, especially for models like
gpt-4-turbo with a large context size of 128k tokens. Given the potential for information to
be“lostinthemiddle”oflongercontexts(Kuratovetal.,2024;Liuetal.,2023),wewantedtoexploretheeffectsofvaryingthecontextwindowsizeforourcombinationsofdatasets,questions,and
metrics.Inparticular,ourgoalwastodeterminetheoptimumcontextsizeforourbaselinecondition
(SS) and then use this uniformly for all query-time LLM use. To that end, we tested four context
window sizes: 8k, 16k, 32k and 64k. Surprisingly, the smallest context window size tested (8k)
wasuniversallybetterforallcomparisonsoncomprehensiveness(averagewinrateof58.1%),while
performingcomparablywithlargercontextsizesondiversity(averagewinrate=52.4%),andempowerment(averagewinrate=51.3%). Givenourpreferenceformorecomprehensiveanddiverse
answers,wethereforeusedafixedcontextwindowsizeof8ktokensforthefinalevaluation.
19

<!-- Page 20 -->


### D ExampleAnswerComparison

Table5: Examplequestion,answers,andLLM-generatedassessmentsfortheNewsarticledataset.
Question Whichpublicfiguresarerepeatedlymentionedacrossvariousentertainmentarticles?
Graph Theentertainmentindustryisvastanddiverse,encompassingfilm,television,music,sports,and
RAG digitalmedia.Certainpublicfiguresstandoutduetotheirsignificantcontributionsandinfluence
acrossthesesectors.Thefollowingsummaryhighlightskeyindividualswhoarerepeatedly
mentionedinvariousentertainmentarticles,reflectingtheirimpactandpresencewithintheindustry.
ActorsandDirectors[...]PublicFiguresinControversy[...]MusiciansandExecutives[...]

### AthletesandCoaches[...]InfluencersandEntrepreneurs[...]

Therepeatedmentionofthesefiguresinentertainmentarticlessignifiestheirongoingrelevanceand
thepublic’sinterestintheirwork.Theirinfluencespansacrossvariousaspectsofentertainment,from
shapingculturalnarrativesinfilmandtelevisiontodrivingtrendsinmusicanddigitalmedia.These
individualsnotonlycontributetotheirrespectivefieldsbutalsoinfluencethebroadercultural
landscape,oftenbecomingcentralfiguresinsocialdiscussionsandpublicdiscourse.
Na¨ıve PublicfigureswhoarerepeatedlymentionedacrossvariousentertainmentarticlesincludeTaylor
RAG Swift,TravisKelce,BritneySpears,andJustinTimberlake.Theseindividualshavebeenhighlighted
forvariousreasons,rangingfromtheirprofessionalachievementstotheirpersonallives.
TaylorSwift[...]TravisKelce[...]BritneySpears[...]JustinTimberlake[...]
Thesefiguresarefrequentlycoveredduetotheirhigh-profilestatusandthepublic’sinterestintheir
careersandpersonallives.Theiractivities,whetherinmusic,sports,orpersonalrelationships,have
significantculturalandeconomicimpacts,asevidencedbythemediacoverageandpublicreactions.
LLM Comprehensiveness:Winner=1(GraphRAG).Answer1isbetterbecauseitprovidesamore
Decision comprehensiveanddetailedlistofpublicfiguresfromawiderrangeofentertainmentsectors,
includingfilm,television,music,sports,gaming,anddigitalmedia.Italsoincludesexamplesoftheir
contributionsandtheimpacttheyhaveontheirrespectivefields,aswellasmentionsofcontroversies
andtheirimplications.Answer2,whiledetailedinitscoverageofafewindividuals,islimitedtoa
smallernumberofpublicfiguresandfocusesprimarilyontheirpersonallivesandrelationshipsrather
thanabroadspectrumoftheirprofessionalinfluenceacrosstheentertainmentindustry.
Diversity:Winner=1(GraphRAG).Answer1isbetterbecauseitprovidesamorevariedandrich
responsebycoveringawiderangeofpublicfiguresfromdifferentsectorsoftheentertainment
industry,includingfilm,television,music,sports,gaming,anddigitalmedia.Itoffersinsightsinto
thecontributionsandinfluenceofthesefigures,aswellascontroversiesandtheirimpactonpublic
discourse.Theansweralsocitesspecificdatasourcesforeachmentionedfigure,indicatingadiverse
rangeofevidencetosupporttheclaims.Incontrast,Answer2focusesonasmallergroupofpublic
figures,primarilyfromthemusicindustryandsports,andreliesheavilyonasinglesourcefordata,
whichmakesitlessdiverseinperspectivesandinsights.
Empowerment:Winner=1(GraphRAG).Answer1isbetterbecauseitprovidesacomprehensive
andstructuredoverviewofpublicfiguresacrossvarioussectorsoftheentertainmentindustry,
includingfilm,television,music,sports,anddigitalmedia.Itlistsmultipleindividuals,providing
specificexamplesoftheircontributionsandthecontextinwhichtheyarementionedinentertainment
articles,alongwithreferencestodatareportsforeachclaim.Thisapproachhelpsthereader
understandthebreadthofthetopicandmakeinformedjudgmentswithoutbeingmisled.Incontrast,
Answer2focusesonasmallergroupofpublicfiguresandprimarilydiscussestheirpersonallivesand
relationships,whichmaynotprovideasbroadanunderstandingofthetopic.WhileAnswer2also
citessources,itdoesnotmatchthedepthandvarietyofAnswer1.
Directness:Winner=2(Na¨ıveRAG).Answer2isbetterbecauseitdirectlylistsspecificpublic
figureswhoarerepeatedlymentionedacrossvariousentertainmentarticles,suchasTaylorSwift,
TravisKelce,BritneySpears,andJustinTimberlake,andprovidesconciseexplanationsfortheir
frequentmentions.Answer1,whilecomprehensive,includesalotofdetailedinformationabout
variousfiguresindifferentsectorsofentertainment,which,whileinformative,doesnotdirectly
answerthequestionwiththesamelevelofconcisenessandspecificityasAnswer2.
20

<!-- Page 21 -->


### E SystemPrompts

E.1 ElementInstanceGeneration
---Goal---
Given a text document that is potentially relevant to this activity and a list of entity types, identify
all entities of those types from the text and all relationships among the identified entities.
---Steps---

## Identify all entities. For each identified entity, extract the following information:

- entityname: Name of the entity, capitalized
- entitytype: One of the following types: [{entitytypes}]
- entitydescription: Comprehensive description of the entity’s attributes and activities
Format each entity as ("entity"{tupledelimiter}<entityname>{tupledelimiter}<entitytype>{tuple
delimiter}<entitydescription>

## From the entities identified in step 1, identify all pairs of (sourceentity, targetentity) that

are *clearly related* to each other
For each pair of related entities, extract the following information:
- sourceentity: name of the source entity, as identified in step 1
- targetentity: name of the target entity, as identified in step 1
- relationshipdescription: explanation as to why you think the source entity and the target entity are
related to each other
- relationshipstrength: a numeric score indicating strength of the relationship between the source entity
and target entity
Format each relationship as ("relationship"{tupledelimiter}<sourceentity>{tupledelimiter}<target
entity>{tupledelimiter}<relationshipdescription>{tupledelimiter}<relationshipstrength>)

## Return output in English as a single list of all the entities and relationships identified in steps 1

and 2. Use **{recorddelimiter}** as the list delimiter.

## When finished, output {completiondelimiter}

---Examples---
Entitytypes: ORGANIZATION,PERSON

### Input:

The Fed is scheduled to meet on Tuesday and Wednesday, with the central bank planning to release its
latest policy decision on Wednesday at 2:00 p.m. ET, followed by a press conference where Fed Chair
Jerome Powell will take questions. Investors expect the Federal Open Market Committee to hold its
benchmark interest rate steady in a range of 5.25%-5.5%.

### Output:

("entity"{tupledelimiter}FED{tupledelimiter}ORGANIZATION{tupledelimiter}The Fed is the Federal Reserve,
which is setting interest rates on Tuesday and Wednesday)
{recorddelimiter}
("entity"{tupledelimiter}JEROME POWELL{tupledelimiter}PERSON{tupledelimiter}Jerome Powell is the chair
of the Federal Reserve)
{recorddelimiter}
("entity"{tupledelimiter}FEDERAL OPEN MARKET COMMITTEE{tupledelimiter}ORGANIZATION{tupledelimiter}The
Federal Reserve committee makes key decisions about interest rates and the growth of the United States
money supply)
{recorddelimiter}
("relationship"{tupledelimiter}JEROME POWELL{tupledelimiter}FED{tupledelimiter}Jerome Powell is the
Chair of the Federal Reserve and will answer questions at a press conference{tupledelimiter}9)
{completiondelimiter}
...Moreexamples...
---Real Data---
Entitytypes: {entitytypes}

### Input:

{inputtext}

### Output:

E.2 CommunitySummaryGeneration
---Role---
You are an AI assistant that helps a human analyst to perform general information discovery. Information
discovery is the process of identifying and assessing relevant information associated with certain
entities (e.g., organizations and individuals) within a network.
21

<!-- Page 22 -->

---Goal---
Write a comprehensive report of a community, given a list of entities that belong to the community as well
as their relationships and optional associated claims. The report will be used to inform decision-makers
about information associated with the community and their potential impact. The content of this report
includes an overview of the community’s key entities, their legal compliance, technical capabilities,
reputation, and noteworthy claims.
---Report Structure---

### The report should include the following sections:

- TITLE: community’s name that represents its key entities - title should be short but specific. When
possible, include representative named entities in the title.
- SUMMARY: An executive summary of the community’s overall structure, how its entities are related to each
other, and significant information associated with its entities.
- IMPACT SEVERITY RATING: a float score between 0-10 that represents the severity of IMPACT posed by
entities within the community. IMPACT is the scored importance of a community.
- RATING EXPLANATION: Give a single sentence explanation of the IMPACT severity rating.
- DETAILED FINDINGS: A list of 5-10 key insights about the community. Each insight should have a short
summary followed by multiple paragraphs of explanatory text grounded according to the grounding rules
below. Be comprehensive.
Return output as a well-formed JSON-formatted string with the following format:
{{
"title": <reporttitle>,
"summary": <executivesummary>,
"rating": <impactseverityrating>,
"ratingexplanation": <ratingexplanation>,
"findings": [
{{
"summary":<insight1summary>,
"explanation": <insight1explanation>
}},
{{
"summary":<insight2summary>,
"explanation": <insight2explanation>
}}
]
}}
---Grounding Rules---
Points supported by data should list their data references as follows:
"This is an example sentence supported by multiple data references [Data: <dataset name> (record ids);
<dataset name> (record ids)]."
Do not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record
ids and add "+more" to indicate that there are more.

### For example:

"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (1),
Entities (5, 7); Relationships (23); Claims (7, 2, 34, 64, 46, +more)]."
where 1, 5, 7, 23, 2, 34, 46, and 64 represent the id (not the index) of the relevant data record.
Do not include information where the supporting evidence for it is not provided.
---Example---

### Input:


### Entities

id,entity,description
5,VERDANT OASIS PLAZA,Verdant Oasis Plaza is the location of the Unity March
6,HARMONY ASSEMBLY,Harmony Assembly is an organization that is holding a march at Verdant Oasis Plaza

### Relationships

id,source,target,description
37,VERDANT OASIS PLAZA,UNITY MARCH,Verdant Oasis Plaza is the location of the Unity March
38,VERDANT OASIS PLAZA,HARMONY ASSEMBLY,Harmony Assembly is holding a march at Verdant Oasis Plaza
39,VERDANT OASIS PLAZA,UNITY MARCH,The Unity March is taking place at Verdant Oasis Plaza
40,VERDANT OASIS PLAZA,TRIBUNE SPOTLIGHT,Tribune Spotlight is reporting on the Unity march taking place at

### Verdant Oasis Plaza

41,VERDANT OASIS PLAZA,BAILEY ASADI,Bailey Asadi is speaking at Verdant Oasis Plaza about the march
43,HARMONY ASSEMBLY,UNITY MARCH,Harmony Assembly is organizing the Unity March
Output:
22

<!-- Page 23 -->

{{
"title": "Verdant Oasis Plaza and Unity March",
"summary": "The community revolves around the Verdant Oasis Plaza, which is the location of the Unity
March. The plaza has relationships with the Harmony Assembly, Unity March, and Tribune Spotlight, all of
which are associated with the march event.",
"rating": 5.0,
"ratingexplanation": "The impact severity rating is moderate due to the potential for unrest or conflict
during the Unity March.",
"findings": [
{{
"summary": "Verdant Oasis Plaza as the central location",
"explanation": "Verdant Oasis Plaza is the central entity in this community, serving as the location for
the Unity March. This plaza is the common link between all other entities, suggesting its significance
in the community. The plaza’s association with the march could potentially lead to issues such as
public disorder or conflict, depending on the nature of the march and the reactions it provokes. [Data:
Entities (5), Relationships (37, 38, 39, 40, 41,+more)]"
}},
{{
"summary": "Harmony Assembly’s role in the community",
"explanation": "Harmony Assembly is another key entity in this community, being the organizer of the
march at Verdant Oasis Plaza. The nature of Harmony Assembly and its march could be a potential source of
threat, depending on their objectives and the reactions they provoke. The relationship between Harmony
Assembly and the plaza is crucial in understanding the dynamics of this community. [Data: Entities(6),
Relationships (38, 43)]"
}},
{{
"summary": "Unity March as a significant event",
"explanation": "The Unity March is a significant event taking place at Verdant Oasis Plaza. This event
is a key factor in the community’s dynamics and could be a potential source of threat, depending on the
nature of the march and the reactions it provokes. The relationship between the march and the plaza is
crucial in understanding the dynamics of this community. [Data: Relationships (39)]"
}},
{{
"summary": "Role of Tribune Spotlight", "explanation": "Tribune Spotlight is reporting on the Unity
March taking place in Verdant Oasis Plaza. This suggests that the event has attracted media attention,
which could amplify its impact on the community. The role of Tribune Spotlight could be significant in
shaping public perception of the event and the entities involved. [Data: Relationships (40)]"
}}
]
}}
---Real Data---
Use the following text for your answer. Do not make anything up in your answer.

### Input:

{inputtext}
...ReportStructureandGroundingRulesRepeated...

### Output:

E.3 CommunityAnswerGeneration
---Role---
You are a helpful assistant responding to questions about a dataset by synthesizing perspectives from
multiple analysts.
---Goal---
Generate a response of the target length and format that responds to the user’s question, summarize
all the reports from multiple analysts who focused on different parts of the dataset, and incorporate any
relevant general knowledge.
Note that the analysts’ reports provided below are ranked in the **descending order of helpfulness**.
If you don’t know the answer, just say so. Do not make anything up.
The final response should remove all irrelevant information from the analysts’ reports and merge the
cleaned information into a comprehensive answer that provides explanations of all the key points and
implications appropriate for the response length and format.
Add sections and commentary to the response as appropriate for the length and format. Style the response
in markdown.
The response shall preserve the original meaning and use of modal verbs such as "shall", "may" or "will".
The response should also preserve all the data references previously included in the analysts’ reports,
23

<!-- Page 24 -->

but do not mention the roles of multiple analysts in the analysis process.
Do not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record
ids and add "+more" to indicate that there are more.

### For example:

"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (2,
7, 34, 46, 64, +more)]. He is also CEO of company X [Data: Reports (1, 3)]"
where 1, 2, 3, 7, 34, 46, and 64 represent the id (not the index) of the relevant data record.
Do not include information where the supporting evidence for it is not provided.
---Target response length and format---
{responsetype}
---Analyst Reports---
{reportdata}
...GoalandTargetresponselengthandformatrepeated...
Add sections and commentary to the response as appropriate for the length and format. Style the response
in markdown.

### Output:

E.4 GlobalAnswerGeneration
---Role---
You are a helpful assistant responding to questions about data in the tables provided.
---Goal---
Generate a response of the target length and format that responds to the user’s question, summarize
all relevant information in the input data tables appropriate for the response length and format, and
incorporate any relevant general knowledge.
If you don’t know the answer, just say so. Do not make anything up.
The response shall preserve the original meaning and use of modal verbs such as "shall", "may" or "will".
Points supported by data should list the relevant reports as references as follows:
"This is an example sentence supported by data references [Data: Reports (report ids)]"
Note:thepromptsforSS(semanticsearch)andTS(textsummarization)conditionsuse”Sources”inplaceof”Reports”above.
Do not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record
ids and add "+more" to indicate that there are more.

### For example:

"Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Reports (2,
7, 64, 46, 34, +more)]. He is also CEO of company X [Data: Reports (1, 3)]"
where 1, 2, 3, 7, 34, 46, and 64 represent the id (not the index) of the relevant data report in the
provided tables.
Do not include information where the supporting evidence for it is not provided.
At the beginning of your response, generate an integer score between 0-100 that indicates how **helpful**
is this response in answering the user’s question. Return the score in this format: <ANSWERHELPFULNESS>
scorevalue </ANSWERHELPFULNESS>.
---Target response length and format---
{responsetype}
---Data tables---
{contextdata}
...GoalandTargetresponselengthandformatrepeated...
Output:
24

<!-- Page 25 -->


### F EvaluationPrompts

F.1 RelativeAssessmentPrompt
---Role---
You are a helpful assistant responsible for grading two answers to a question that are provided by two
different people.
---Goal---
Given a question and two answers (Answer 1 and Answer 2), assess which answer is better according to
the following measure:
{criteria}

### Your assessment should include two parts:

- Winner: either 1 (if Answer 1 is better) and 2 (if Answer 2 is better) or 0 if they are fundamentally
similar and the differences are immaterial.
- Reasoning: a short explanation of why you chose the winner with respect to the measure described above.
Format your response as a JSON object with the following structure:
{{
"winner": <1, 2, or 0>,
"reasoning": "Answer 1 is better because <your reasoning>."
}}
---Question---
{question}
---Answer 1---
{answer1}
---Answer 2---
{answer2}
Assess which answer is better according to the following measure:
{criteria}

### Output:

F.2 RelativeAssessmentMetrics

## Criteria = {

"comprehensiveness": "How much detail does the answer provide to cover all the aspects and details of the
question? A comprehensive answer should be thorough and complete, without being redundant or irrelevant.
For example, if the question is ’What are the benefits and drawbacks of nuclear energy?’, a comprehensive
answer would provide both the positive and negative aspects of nuclear energy, such as its efficiency,
environmental impact, safety, cost, etc. A comprehensive answer should not leave out any important points
or provide irrelevant information. For example, an incomplete answer would only provide the benefits of
nuclear energy without describing the drawbacks, or a redundant answer would repeat the same information
multiple times.",
"diversity": "How varied and rich is the answer in providing different perspectives and insights
on the question? A diverse answer should be multi-faceted and multi-dimensional, offering different
viewpoints and angles on the question. For example, if the question is ’What are the causes and effects
of climate change?’, a diverse answer would provide different causes and effects of climate change, such
as greenhouse gas emissions, deforestation, natural disasters, biodiversity loss, etc. A diverse answer
should also provide different sources and evidence to support the answer. For example, a single-source
answer would only cite one source or evidence, or a biased answer would only provide one perspective or
opinion.",
"directness": "How specifically and clearly does the answer address the question? A direct answer should
provide a clear and concise answer to the question. For example, if the question is ’What is the capital
of France?’, a direct answer would be ’Paris’. A direct answer should not provide any irrelevant or
unnecessary information that does not answer the question. For example, an indirect answer would be ’The
capital of France is located on the river Seine’.",
"empowerment": "How well does the answer help the reader understand and make informed judgements about
the topic without being misled or making fallacious assumptions. Evaluate each answer on the quality of
answer as it relates to clearly explaining and providing reasoning and sources behind the claims in the
answer."
}
25

<!-- Page 26 -->


### G StatisticalAnalysis

Table 6: Pairwise comparisons of six conditions on four metrics across 125 questions and two
datasets. For each question and metric, the winning condition received a score of 100, the losing
conditionreceivedascoreof0,andintheeventofatie,eachconditionwasscored50. Thesescores
werethenaveragedoverfiveevaluationrunsforeachcondition. ResultsofShapiro-Wilktestsindicated that the data did not follow a normal distribution. Thus, non-parametric tests (Wilcoxon
signed-rank tests) were employed to assess the performance differences between pairs of conditions,withHolm-Bonferronicorrectionappliedtoaccountformultiplepairwisecomparisons. The
correctedp-valuesthatindicatedstatisticallysignificantdifferencesarehighlightedinbold.

### PodcastTranscripts NewsArticles

Condition1 Condition2 Mean1 Mean2 Z-value p-value Mean1 Mean2 Z-value p-value

### C0 Ts 50.24 49.76 -0.06 1 55.52 44.48 -2.03 0.17


### C1 Ts 51.92 48.08 -1.56 0.633 58.8 41.2 -3.62 0.002

C2 TS 57.28 42.72 -4.1 <0.001 62.08 37.92 -5.07 <0.001

### C3 Ts 56.48 43.52 -3.42 0.006 63.6 36.4 -5.63 <0.001


### C0 Ss 71.92 28.08 -6.2 <0.001 71.76 28.24 -6.3 <0.001

C1 SS 75.44 24.56 -7.45 <0.001 74.72 25.28 -7.78 <0.001

### C2 Ss 77.76 22.24 -8.17 <0.001 79.2 20.8 -8.34 <0.001

Comprehensiveness C3 SS 78.96 21.04 -8.12 <0.001 79.44 20.56 -8.44 <0.001
TS SS 83.12 16.88 -8.85 <0.001 79.6 20.4 -8.27 <0.001
C0 C1 53.2 46.8 -1.96 0.389 51.92 48.08 -0.45 0.777

### C0 C2 50.24 49.76 -0.23 1 53.68 46.32 -1.54 0.371

C1 C2 51.52 48.48 -1.62 0.633 57.76 42.24 -4.01 <0.001
C0 C3 49.12 50.88 -0.56 1 52.16 47.84 -0.86 0.777

### C1 C3 50.32 49.68 -0.66 1 55.12 44.88 -2.94 0.016

C2 C3 52.24 47.76 -1.97 0.389 58.64 41.36 -3.68 0.002
C0 TS 50.24 49.76 -0.11 1 46.88 53.12 -1.38 0.676

### C1 Ts 50.48 49.52 -0.12 1 54.64 45.36 -1.88 0.298

C2 TS 57.12 42.88 -2.84 0.036 55.76 44.24 -2.16 0.184

### C3 Ts 54.32 45.68 -2.39 0.1 60.16 39.84 -4.07 <0.001


### C0 Ss 76.56 23.44 -7.12 <0.001 62.08 37.92 -3.57 0.003

C1 SS 75.44 24.56 -7.33 <0.001 64.96 35.04 -4.92 <0.001

### C2 Ss 80.56 19.44 -8.21 <0.001 70.56 29.44 -6.29 <0.001

Diversity C3 SS 80.8 19.2 -8.3 <0.001 69.12 30.88 -5.53 <0.001
TS SS 82.08 17.92 -8.43 <0.001 67.2 32.8 -4.85 <0.001

### C0 C1 49.76 50.24 -0.13 1 39.68 60.32 -3.61 0.003

C0 C2 46.32 53.68 -1.5 0.669 40.96 59.04 -3.14 0.012
C1 C2 44.08 55.92 -3.27 0.011 50.24 49.76 -0.22 1

### C0 C3 44 56 -2.6 0.065 41.04 58.96 -3.47 0.004

C1 C3 45.44 54.56 -2.98 0.026 49.52 50.48 -0.01 1

### C2 C3 48.48 51.52 -0.96 1 50.96 49.04 -0.39 1

C0 TS 40.96 59.04 -4.3 <0.001 42.24 57.76 -3.32 0.012

### C1 Ts 45.2 54.8 -3.76 0.002 50 50 -0.12 1

C2 TS 47.68 52.32 -2.2 0.281 49.52 50.48 -0.22 1

### C3 Ts 48.72 51.28 -1.27 1 51.68 48.32 -1.2 1

C0 SS 42.96 57.04 -3.71 0.003 42.72 57.28 -3.12 0.022
C1 SS 47.68 52.32 -1.5 0.936 51.36 48.64 -0.84 1

### C2 Ss 50.72 49.28 -0.55 1 49.84 50.16 -0.2 1

Empowerment C3 SS 48.96 51.04 -0.57 1 49.52 50.48 -0.08 1
TS SS 57.52 42.48 -4.1 <0.001 52.88 47.12 -1.1 1

### C0 C1 48.72 51.28 -1.23 1 42.4 57.6 -3.9 0.001

C0 C2 46.64 53.36 -2.54 0.12 44.8 55.2 -2.16 0.336

### C1 C2 49.28 50.72 -1.73 0.682 52 48 -1.45 1

C0 C3 47.6 52.4 -1.78 0.682 44.32 55.68 -3.45 0.008

### C1 C3 50 50 0 1 51.44 48.56 -1.02 1


### C2 C3 50.72 49.28 -0.86 1 50.4 49.6 -0.22 1

C0 TS 44.96 55.04 -4.09 <0.001 45.2 54.8 -3.68 0.003
C1 TS 47.92 52.08 -2.41 0.126 46.64 53.36 -2.91 0.04

### C2 Ts 48.8 51.2 -2.23 0.179 48.32 51.68 -2.12 0.179


### C3 Ts 48.08 51.92 -2.23 0.179 48.32 51.68 -2.56 0.074

C0 SS 35.12 64.88 -6.17 <0.001 41.44 58.56 -4.82 <0.001
C1 SS 40.32 59.68 -4.83 <0.001 45.2 54.8 -3.19 0.017

### C2 Ss 40.4 59.6 -4.67 <0.001 44.88 55.12 -3.65 0.003

Directness C3 SS 40.48 59.52 -4.69 <0.001 45.6 54.4 -2.86 0.043

### Ts Ss 43.6 56.4 -3.96 <0.001 46 54 -2.68 0.066

C0 C1 46.96 53.04 -2.87 0.037 47.6 52.4 -2.17 0.179
C0 C2 48.4 51.6 -2.06 0.197 48.48 51.52 -1.61 0.321
C1 C2 49.84 50.16 -1 0.952 49.28 50.72 -1.6 0.321
C0 C3 48.4 51.6 -1.8 0.29 47.2 52.8 -2.62 0.071
C1 C3 49.76 50.24 0 1 48.8 51.2 -1.29 0.321
C2 C3 50 50 0 1 48.8 51.2 -1.84 0.262
26

## Tables

**Table (Page 10):**

| 50 | 17 | 28 | 25 | 22 | 21 |
|---|---|---|---|---|---|
| 83 | 50 | 50 | 48 | 43 | 44 |
| 72 | 50 | 50 | 53 | 50 | 49 |
| 75 | 52 | 47 | 50 | 52 | 50 |
| 78 | 57 | 50 | 48 | 50 | 52 |
| 79 | 56 | 51 | 50 | 48 | 50 |


**Table (Page 10):**

| 50 | 18 | 23 | 25 | 19 | 19 |
|---|---|---|---|---|---|
| 82 | 50 | 50 | 50 | 43 | 46 |
| 77 | 50 | 50 | 50 | 46 | 44 |
| 75 | 50 | 50 | 50 | 44 | 45 |
| 81 | 57 | 54 | 56 | 50 | 48 |
| 81 | 54 | 56 | 55 | 52 | 50 |


**Table (Page 10):**

| 50 | 42 | 57 | 52 | 49 | 51 |
|---|---|---|---|---|---|
| 58 | 50 | 59 | 55 | 52 | 51 |
| 43 | 41 | 50 | 49 | 47 | 48 |
| 48 | 45 | 51 | 50 | 49 | 50 |
| 51 | 48 | 53 | 51 | 50 | 51 |
| 49 | 49 | 52 | 50 | 49 | 50 |


**Table (Page 10):**

| 50 | 56 | 65 | 60 | 60 | 60 |
|---|---|---|---|---|---|
| 44 | 50 | 55 | 52 | 51 | 52 |
| 35 | 45 | 50 | 47 | 48 | 48 |
| 40 | 48 | 53 | 50 | 50 | 50 |
| 40 | 49 | 52 | 50 | 50 | 50 |
| 40 | 48 | 52 | 50 | 50 | 50 |


**Table (Page 10):**

| 50 | 20 | 28 | 25 | 21 | 21 |
|---|---|---|---|---|---|
| 80 | 50 | 44 | 41 | 38 | 36 |
| 72 | 56 | 50 | 52 | 54 | 52 |
| 75 | 59 | 48 | 50 | 58 | 55 |
| 79 | 62 | 46 | 42 | 50 | 59 |
| 79 | 64 | 48 | 45 | 41 | 50 |


**Table (Page 10):**

| 50 | 33 | 38 | 35 | 29 | 31 |
|---|---|---|---|---|---|
| 67 | 50 | 53 | 45 | 44 | 40 |
| 62 | 47 | 50 | 40 | 41 | 41 |
| 65 | 55 | 60 | 50 | 50 | 50 |
| 71 | 56 | 59 | 50 | 50 | 51 |
| 69 | 60 | 59 | 50 | 49 | 50 |


**Table (Page 10):**

| 50 | 47 | 57 | 49 | 50 | 50 |
|---|---|---|---|---|---|
| 53 | 50 | 58 | 50 | 50 | 48 |
| 43 | 42 | 50 | 42 | 45 | 44 |
| 51 | 50 | 58 | 50 | 52 | 51 |
| 50 | 50 | 55 | 48 | 50 | 50 |
| 50 | 52 | 56 | 49 | 50 | 50 |


**Table (Page 10):**

| 50 | 54 | 59 | 55 | 55 | 54 |
|---|---|---|---|---|---|
| 46 | 50 | 55 | 53 | 52 | 52 |
| 41 | 45 | 50 | 48 | 48 | 47 |
| 45 | 47 | 52 | 50 | 49 | 49 |
| 45 | 48 | 52 | 51 | 50 | 49 |
| 46 | 48 | 53 | 51 | 51 | 50 |


**Table (Page 18):**

|  |  | 600chunksize 1200chunksize |  |  |  |
|---|---|---|---|---|---|
|  |  | 600chunksize 1200chunksize |  |  |  |
|  |  | 2400chunksize |  |  |  |
|  |  |  |  |  |  |
