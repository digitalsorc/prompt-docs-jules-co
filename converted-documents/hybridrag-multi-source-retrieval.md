---
title: "HybridRAG Multi Source Retrieval"
original_file: "./HybridRAG_Multi_Source_Retrieval.pdf"
document_type: "research"
conversion_date: "2025-11-29"
topics: ["prompt-engineering", "llm", "rag", "chain-of-thought", "react"]
keywords: ["adaptive", "step", "retrieval", "rag", "multi", "single", "query", "time", "acc", "queries"]
summary: "<!-- Page 1 -->

Adaptive-RAG: Learning to Adapt Retrieval-Augmented

### Large Language Models through Question Complexity

SoyeongJeong1 JinheonBaek2 SukminCho1 SungJuHwang1,2 JongC.Park1*

### SchoolofComputing1 GraduateSchoolofAI2


### KoreaAdvancedInstituteofScienceandTechnology1,2

{starsuzi,jinheon.baek,nelllpic,sjhwang82,jongpark}@kaist.ac.kr
Abstract
51

### Retrieval-AugmentedLargeLanguageModels 50

(LLMs),whichincorporatethenon-parametric 49
knowledgefromexternalknowledgebasesinto 48"
related_documents: []
---

# HybridRAG Multi Source Retrieval

<!-- Page 1 -->

Adaptive-RAG: Learning to Adapt Retrieval-Augmented

### Large Language Models through Question Complexity

SoyeongJeong1 JinheonBaek2 SukminCho1 SungJuHwang1,2 JongC.Park1*

### SchoolofComputing1 GraduateSchoolofAI2


### KoreaAdvancedInstituteofScienceandTechnology1,2

{starsuzi,jinheon.baek,nelllpic,sjhwang82,jongpark}@kaist.ac.kr
Abstract
51

### Retrieval-AugmentedLargeLanguageModels 50

(LLMs),whichincorporatethenon-parametric 49
knowledgefromexternalknowledgebasesinto 48

### LLMs,haveemergedasapromisingapproach 47

toenhancingresponseaccuracyinseveraltasks, 0.5 1.0 1.5 2.0 2.5 3.0 3.5

### Time per Query

suchasQuestion-Answering(QA).However,
eventhoughtherearevariousapproachesdealingwithqueriesofdifferentcomplexities,they
eitherhandlesimplequerieswithunnecessary
computational overhead or fail to adequately
address complex multi-step queries; yet, not
alluserrequestsfallintoonlyoneofthesimple or complex categories. In this work, we
proposeanoveladaptiveQAframeworkthat
candynamicallyselectthemostsuitablestrategyfor(retrieval-augmented)LLMsfromthe
simplesttothemostsophisticatedonesbased
on the query complexity. Also, this selection process is operationalized with a classifier,whichisasmallerLMtrainedtopredict
thecomplexitylevelofincomingquerieswith
automaticallycollectedlabels,obtainedfrom
actual predicted outcomes of models and inherent inductive biases in datasets. This approachoffersabalancedstrategy, seamlessly
adaptingbetweentheiterativeandsingle-step
retrieval-augmentedLLMs,aswellasthenoretrieval methods, in response to a range of
query complexities. We validate our model
on a set of open-domain QA datasets, covering multiple query complexities, and show
that ours enhances the overall efficiency and
accuracy of QA systems, compared to relevantbaselinesincludingtheadaptiveretrieval
approaches. Code is available at: https://
github.com/starsuzi/Adaptive-RAG.
1 Introduction
Recent Large Language Models (LLMs) (Brown
etal.,2020;OpenAI,2023;Touvronetal.,2023;
Anil et al., 2023) have shown overwhelming performancesacrossdiversetasks,includingquestion-
* Correspondingauthor

## )1F(

ecnamrofreP

### Performance vs Time with GPT-3.5

Adaptive-RAG (Ours) Multi-step Approach
No Retrieval Adaptive Retrieval

### Single-step Approach


### Figure1:QAperformance(F1)andefficiency(Time/Query)

fordifferentretrieval-augmentedgenerationapproaches.We
usetheGPT-3.5-Turbo-InstructasthebaseLLM.
answering(QA)(Yangetal.,2018;Kwiatkowski
et al., 2019). However, they still generate factuallyincorrectanswerssincetheirknowledgesolely
relies on their parametric memory (Kasai et al.,
2022;Mallenetal.,2023). Meanwhile,memorizingallthe(ever-changing)worldknowledgemay
notbepossible. Toaddressthisproblem,retrievalaugmentedLLMs(Borgeaudetal.,2022;Izacard
et al., 2023; Shi et al., 2023), which incorporate
non-parametric knowledge into LLMs with additionalretrievalmodules,havegainedmuchincreasing attention. Specifically, these models access
a knowledge base, which serves as an extensive
repository of information across various subjects
anddisciplines,toretrieveinformationrelevantto
thegiveninput,andthenincorporatetheretrieved
informationintoLLMs,whichenablesthemtostay
accurateandcurrentwiththeworldknowledge.
A particularly salient application of retrievalaugmentedLLMsistohandlingQAtasks,whose
goal is to provide correct answers in response to
userqueries,especiallythoseofhighcomplexity.

### Earlyworkonretrieval-augmentedLLMsfocuses

primarilyonsingle-hopqueries(Lazaridouetal.,
2022; Ram et al., 2023), whose answers are typically found within a single document; therefore,
this approach involves retrieving a relevant document based on the query and subsequently integratingthisinformationintoQAmodelstoformulate a response. However, unlike this single-hop
QA,somequeriesrequireconnectingandaggregating multiple documents, which are, furthermore,
4202
raM
82
]LC.sc[
2v30441.3042:viXra

<!-- Page 2 -->

(A) Single-Step Approach (B) Multi-Step Approach (C) Our Adaptive Approach
k times
S W i h m e p n l i e s Q th u e e b r ir y t : h day Documents S W i h m e p n l i e s Q th u e e b r ir y t : h day Documents S Pa tr r a is i g is h t t h f e o r c w ap a i r ta d l Q of u w e h r a y t : ? Answer
of Michael F. Phelps? Retrieval of Michael F. Phelps? (Intermediate)
Answer Answers Simple Query: Documents

### Inefficient When is the birthday


### Complex Query: of Michael F. Phelps? Answer


### What currency is in Documents k times Classifier k times

Billy Giles’ birthplace? Retrieval Answer C W Bi o l h ly m a t G p c i l l u e e r s x re ’ Q b n i c r u y th e i p r s l y a i : n c e? (I D nt o e c r u m m ed en ia t t s e) C W Bi o l h ly m a t G p c i l l u e e r s x re ’ Q b n i c r u y th e i p r s l y a i : n c e? Retrieval (I D nt o e c r u m m ed en ia t t s e)

### Inaccurate Answers Answers

Figure2:Aconceptualcomparisonofdifferentretrieval-augmentedLLMapproachestoquestionanswering.(A)Inresponseto
aquery,thissingle-stepapproachretrievesrelevantdocumentsandthengeneratesananswer.However,itmaynotbesufficient
forcomplexqueriesthatrequiremulti-stepreasoning.(B)Thismulti-stepapproachiterativelyretrievesdocumentsandgenerates
intermediateanswers,whichispowerfulyetlargelyinefficientforthesimplequerysinceitrequiresmultipleaccessestoboth
LLMsandretrievers.(C)Ouradaptiveapproachcanselectthemostsuitablestrategyforretrieval-augmentedLLMs,ranging
fromiterative,tosingle,toevennoretrievalapproaches,basedonthecomplexityofgivenqueriesdeterminedbyourclassifier.
often not answerable through a single-step pro- Inthiswork,consideringdiversecomplexitylevcessofretrieval-and-response. Anexamplequery els of real-world queries, we argue that previous
is ‘When did the people who captured Malakoff one-size-fits-allapproachesmightbeinadequateto
cometotheregionwherePhilipsburgislocated?’, coverallofthem. Instead,weproposetoselectthe
whichrequiresfourreasoningstepstosolve. There- most suitable strategy from a range of (retrievalfore, to effectively handle such complex queries, augmented)LLMs,eachofwhichistailoredtothe
recentstudieshaveconcentratedlargelyonmulti- specific complexity of the input query. Notably,
stepandmulti-reasoningQA,whichrequiresitera- a critical step in this process is pre-defining the
tiveaccessestobothLLMsandretrieversmultiple query complexity, which is instrumental in detertimes (Press et al., 2023; Trivedi et al., 2023), at mining the most fitting model to it. In this work,
thecostofheavycomputationaloverheads. weoperationalizethisprocesswithanovelclassifier,whichisasmallermodeltrainedtopredictthe
Yet,weshouldrethink: Inareal-worldscenario,
complexitylevelofincomingqueries(seeFigure2
are all the requests from users complex? Instead,
(c)). Moreover,weautomaticallycollectitstraining
usersmightoftenasksimpleandstraightforward
datasetswithouthumanlabeling,byleveragingthe
questions,whileonlyoccasionallyaskingcomplex
predictedoutcomes(i.e.,whichmodelsaccurately
ones. Specifically, a query such as ‘Paris is the
respondtowhichqueries)aswellasbycapitalizing
capital of what?’ is likely to be asked more freontheinherentbiasesinexistingdatasets(i.e.,samquently, compared to the aforementioned multiplesinthedatasetsaredesignedeitherforsinglestep query, and this simpler query might also be
steporformulti-stepQAscenarios). Thisproposed
easilyansweredbytheLLMsthemselves,without
methodcanofferarobustmiddlegroundamongthe
accessing external knowledge. In other words, a
iterativeLLMaugmentationmethodsforcomplex
multi-stepQAapproachcouldgiverisetounnecqueries, single-step methods for simpler queries,
essarycomputationaloverheadforsimplequeries,
andevenno-retrieval-augmentedmethodsforthe
eventhoughitwouldbevitalforcomplexqueries
moststraightforwardqueries(answerablebyLLMs
(see Figure 2 (A)). On the other hand, handling
themselves),thussignificantlyenhancingtheovercomplexquerieswithsingle-step-retrievaloreven
allefficiencyandaccuracy, asshowninFigure1.
non-retrieval strategies would be largely insuffi-
WerefertoourframeworkasAdaptiveRetrievalcient(Figure2(B)).Thissuggeststheneedforan
AugmentedGeneration(Adaptive-RAG).
adaptiveQAsystem,whichcandynamicallyadjust
the operational strategies of retrieval-augmented We validate Adaptive-RAG using benchmark
LLMsbasedonthequerycomplexity. Whilesome open-domainQAdatasets,coveringawiderange
recentapproachesarecapableofdoingthisbased of query complexity from single-hop (Rajpurkar
onthefrequencyofentitiesinqueries(Mallenetal., etal.,2016;Joshietal.,2017;Kwiatkowskietal.,
2023) or on the generated outputs from models 2019) to multi-hop (Yang et al., 2018; Ho et al.,
for multi-step QA (Trivedi et al., 2023), they are 2020; Trivedi et al., 2022b) queries. The experstill suboptimal: the former methods are overly imental results show that ours significantly imsimplistic, failing to consider multi-hop queries; proves the overall accuracy and efficiency, commeanwhile,thelatterareexcessivelycomplex,ter- paredtotheprioradaptivestrategies,onmultiple
minatinganswersolvingstepsafterseveralrounds LLMs,suchasGPT-3.5(Brownetal.,2020)and
ofmoduleaccess. FLAN-T5series(Chungetal.,2022).

<!-- Page 3 -->

Ourcontributionsandfindingsarethreefold: to this decomposition-based approach, other recentstudies,suchasYaoetal.(2023)andTrivedi
• Wepointouttherealisticscenarioofqueriesof
etal.(2023),exploredtheinterleavingofChain-ofvaryingcomplexities,andfindoutthatexisting

### Thoughtreasoning(Weietal.,2022b)—amethod

retrieval-augmentedgenerationapproachestend
wherealogicalsequenceofthoughtsisgenerated
tobeoverlysimpleorcomplex.
—withdocumentretrieval,repeatedlyapplyingthis
• Weadaptretrieval-augmentedLLMstothequery
processuntilthereasoningchaingeneratestheancomplexityassessedbytheclassifier,whichenswer. Inaddition,Jiangetal.(2023)introducedan
ablestheutilizationofthemostsuitableapproach
approachtorepeatedlyretrievingnewdocuments
tailoredtoeachquery.
ifthetokenswithingeneratedsentenceshavelow
• WeshowthatourAdaptive-RAGishighlyeffecconfidence. However,theaforementionedmethods
tive and efficient, balancing between the comoverlooked the fact that, in real-world scenarios,
plexityandthesimplicityfordiversequeries.
queriesareofawidevarietyofcomplexities. Therefore, it would be largely inefficient to iteratively
2 RelatedWork
accessLLMsandretrieversforeveryquery,which
mightbesimpleenoughwithasingleretrievalstep
Open-domainQA Open-domainQAisthetask
orevenonlywithanLLMitself.
of accurately answering a query by sourcing for
query-relevant documents, and then interpreting

### AdaptiveRetrieval Tohandlequeriesofvarying

them to provide answers (Chen et al., 2017; Zhu
complexities,theadaptiveretrievalstrategyaimsto
et al., 2021), which, thus, generally involves two
dynamicallydecidewhethertoretrievedocuments
modules: aretriever(Karpukhinetal.,2020;Xiong
ornot, basedoneachquery’scomplexity. Inthis
etal.,2021)andareader(Yangetal.,2019;Izacvein, Mallen et al. (2023) proposed to decide the
ard and Grave, 2021; Jeong et al., 2023). Along
query’scomplexitylevelbasedonthefrequencyof
with the emergence of LLMs with superior reaitsentitiesandsuggestedusingtheretrievalmodsoningcapabilitiesthankstotheirbillion-sizedpaules only when the frequency falls below a cerrameters (Wei et al., 2022a), a synergy between
tain threshold. However, this approach, focusing
LLMsandretrievershasledtosignificantadvancesolelyonthebinarydecisionofwhethertoretrieve
ments (Lazaridou et al., 2022; Ram et al., 2023).
or not, may not be sufficient for more complex

### Specifically, this integration has been shown to

queriesthatrequiremultiplereasoningsteps. AdenhanceOpen-domainQAbymitigatingthehalluditionally, Qi et al. (2021) proposed an approach
cinationproblemfromLLMsthroughstrengthened
thatperformsafixedsetofoperations(retrieving,
reasoning abilities of the reader, as well as utilizreading,andreranking)multipletimesuntiltheaning the retrieved, external documents (Cho et al.,
swerisderivedforthegivenquery,whichisbuilt
2023). Despitetheseadvancementsforsingle-hop
upontraditionalBERT-likeLMs. However,unlike
retrieval-augmentedLLMs,however,thecomplexourAdaptive-RAGwhichpre-determinesthequery
ityofsomequeriesneedsamorecomplexstrategy.
complexityandadaptstheoperationalbehaviorof
Multi-hopQA Multi-hopQAisanextensionof anyoff-the-shelfLLMsaccordingly,thisapproach
conventional Open-domain QA, which addition- applies the same fixed operations to every query
allyrequiresthesystemtocomprehensivelygather regardless of its complexity but also necessitates
andcontextualizeinformationfrommultipledocu- additionalspecifictrainingtoLMs. Concurrentto
ments(ofteniteratively),toanswermorecomplex ourwork,Asaietal.(2024)suggestedtrainingasoqueries(Trivedietal.,2022a;Yangetal.,2018). In phisticatedmodeltodynamicallyretrieve,critique,
therealmofmulti-hopQA,theapproachtoitera- andgeneratethetext. Nevertheless,wearguethat
tivelyaccessbothLLMsandtheretrievalmodule alltheaforementionedadaptiveretrievalmethods
isgenerallyemployed. Specifically,Khattabetal. thatrelyonasinglemodelmightbesuboptimalin
(2022), Press et al. (2023), Pereira et al. (2023) handling a variety of queries of a range of differand Khot et al. (2023) proposed to first decom- entcomplexitiessincetheytendtobeeitheroverly
posethemulti-hopqueriesintosimplersingle-hop simpleorcomplexforalltheinputqueries,which
queries,repeatedlyaccesstheLLMsandretriever demandsanewapproachthatcanselectthemost
to solve these sub-queries, and merge their solu- suitablestrategyofretrieval-augmentedLLMstaitionstoformulateacompleteanswer. Incontrast loredtothequerycomplexity.

<!-- Page 4 -->

3 Method ThisprocessallowsLLMstogainaccesstoexternalinformationcontainedind,whichcanprovide
Inthissection,wedescribeourapproachtoadaptthesupplementarycontextthattheinternalknowlingretrieval-augmentedLLMs,bypre-determining
edgeofLLMlacks,whichcansubsequentlyimprove
thequerycomplexityandthenselectingthemost
theaccuracyandconcurrencyofLLMsforQA.
fittingstrategiesforretrieval-augmentedLLMs.
Multi-stepApproachforQA Eventhoughthe
3.1 Preliminaries
aforementionedsingle-stepapproachofferssignif-
Webeginwithpreliminaries,formallyintroducing icant improvements over non-retrieval for q that
differentstrategiesofretrieval-augmentedLLMs. requiresexternalknowledge,itencountersnotable
limitations, particularly when dealing with com-

### NonRetrievalforQA LetusfirstdefineanLLM

plexqueriesthatnecessitatesynthesizinginformaasamodelLLM,whichtakesasequenceoftokens
tionfrommultiplesourcedocumentsandreasoning
x = [x ,x ,...,x ]asaninputandthengenerates
1 2 n overthem. Thisiswhereamulti-stepapproachand
asequenceoftokensy = [y ,y ,...,y ]asanout-
1 2 n reasoningforQAbecomeessential.
put,whichisformalizedasfollows: y = LLM(x).
In this multi-step approach, LLM interacts with

### Then, in our problem setup for QA, x and y be-

Retrieverinseveralrounds,progressivelyrefincome the input query (q) from the user and the
ingitsunderstandingofq,untilitformulatesthefigeneratedanswer(a¯)fromtheLLM,respectively:
nalanswerfromfindingsaccumulatedacrossthese
q = x and a¯ = y. Also, subsequently, the most
multiple steps. Specifically, the process begins
naïveLLM-poweredQAmodelcanberepresented
withtheinitialqueryq,andateveryretrievalstep
asfollows: a¯ = LLM(q). Ideally, a¯ shouldmatch
i,newdocumentsd areretrievedfromDandthen
i
the actual correct answer a. This non-retrievalincorporated into the input of LLMs, as follows:
basedQAmethodishighlyefficientandcouldbe
a¯ = LLM(q,d ,c ), wheretheadditionalcontext
i i i
asomewhatpromisingapproachtohandlingeasy
c can be composed of previous documents and
i
queries, as the size of LLMs becomes extremely
outcomes (d ,d ,...,d ,a¯ ,a¯ ,...,a¯ ), and
1 2 i−1 1 2 i−1
large with its effect on storing a large amount of
d = Retriever(q,c ;D)1. We would like to
i i
knowledge. However,thisapproachislargelyprobnotethatthisiterative,multi-stepprocessenables
lematic on queries that require precise or concur-
LLMtoconstructamorecomprehensiveandextenrent knowledge ofspecific people, events, or any
sivefoundationtosolvequerieseffectively,specifsubjectsbeyondtheLLMs’internalknowledge.
ically adept at complex multi-hop queries where
answersdependoninterconnectedpiecesofinfor-

### Single-step Approach for QA To address the

mation. However,itisimportanttorecognizethat
aforementionedscenarioswhereLLMmaystruggle
thismulti-stepapproachcanberesource-intensive
withqueriesthatarenotanswerablebyLLMitself,
duetotherepeatedaccessestoRetrieverandLLM,
we can utilize the external knowledge d, which
whichentailsubstantialcomputationalcosts.
includesusefulinformationforqueries,retrieved
fromtheexternalknowledgesourceD thatcould
3.2 Adaptive-RAG:Adaptive
be an encyclopedia (e.g., Wikipedia) consisting

### Retrieval-AugmentedGeneration

of millions of documents. Specifically, to obtain
such d from D, a specific retrieval model is nec- Wenowintroduceouradaptiveretrieval-augmented
essary, which returns documents based on their LLMs,whicharebuiltuponthreedifferentstraterelevance with the given query. This process can giesdescribedintheprevioussection,andwhich
beformulatedasfollows: d = Retriever(q;D), are designed to select the most suitable strategy
where Retriever is the retrieval model, with accordingtothecomplexityofqueries.
d ∈ D. Here, we can use any off-the-shelf re-

### Adapting Retrieval-Augmented LLMs Note

triever (Robertson et al., 1994; Karpukhin et al.,
that in real-world scenarios, not all q from users
2020).
have the same level of complexity, necessitating

### Aftertheretrievalstepisdone, wenowhavea

pairofqueryqanditsrelevantdocumentsd. Then, 1ItisworthnotingthatimplementationsoftheLLMand
inordertoaugmentLLMswiththisretrievedexter- retrievervaryacrossdifferentmulti-stepretrieval-augmented
LLMapproaches(Trivedietal.,2023;Pressetal.,2023;Yao
nalknowledge,wecanincorporateitintotheinput
etal.,2023);therefore,thecontextc mayincorporatenone,
i
ofLLMs, representedasfollows: a¯ = LLM(q,d). some,orallofthepreviousdocumentsandanswers.

<!-- Page 5 -->

tailoredstrategiesforhandlingeachquery. Inother complexitybasedontheresultsfromthreedifferent
words, employing the most basic, non-retrieval- retrieval-augmented LLM strategies, in order to
basedapproachLLM(q)torespondtothecomplex determine the label by its needs. For example, if
query q would be also ineffective (Figure 2, A); thesimplestnon-retrieval-basedapproachcorrectly
conversely, using a more elaborate multi-step ap- generatestheanswer,thelabelforitscorresponding
proach LLM(q,d,c) for simple q would be ineffi- queryisassigned‘A’.Also,tobreakthetiebetween
cient(Figure2,B).Therefore,ouradaptiveframe- differentmodelsinprovidingthelabeltothequery,
workisdesignedtodynamicallyadjustthequery- we provide a higher priority to a simpler model.
handling strategy of retrieval-augmented LLMs, Inotherwords,ifbothsingle-stepandmulti-step
whichisachievedbydeterminingthecomplexityof approachesproducethesamecorrectanswerwhile
eachquerybeforeattemptingasolution. Notably, the non-retrieval-based approach fails, we assign
this framework can offer a robust middle ground label‘B’toitscorrespondingquery.
with a range of solutions, from the simplest ap- However, this labeling strategy has a limitaproachforthemoststraightforwardqueries,tothe tioninthatnotallthequeriesareassignedlabels,
one-stepapproachformoderatequeries,andupto since the three retrieval-augmented approaches
themostcomprehensiveandrigorousapproachfor may all fail to generate the correct answer. On
complexqueries. Inaddition,sincetheoperations the other hand, the benchmark datasets may alof LLM and Retriever remain consistent regard- readyhavemeaningfulinductivebiasesaboutthe
less of inputs to them, our method can seeming- mostappropriateretrieval-augmentedLLMstratelesslygobackandforthacrossqueriesofdifferent gies for their queries, considering the ways they
complexities,withoutchangingtheinternalmodel arecreated(e.g.,QAdatasetsthatrequiresequenarchitectureorparametersduringadaption. tialreasoningusuallynecessitateamulti-stepapproach; while queries of those with labeled sin-
QueryComplexityAssessment Tooperationalgledocumentscanbeideallyanswerablewiththe
izeouradaptiveretrieval-augmentedLLMframesingle-stepapproach). Therefore,forthosequeries
work,weshoulddeterminethequerycomplexity,
thatremainunlabeledafterthefirstlabelingstep,
and to achieve this, we propose to model a comwe assign ‘B’ to queries in single-hop datasets
plexityclassifier,whosegoalistoreturntheapproand ‘C’ to queries in multi-hop datasets. Finally,
priatecomplexitylevelofthegivenquery. Specifwe train Classifier with these automaticallyically,giventhequeryq,ourclassifiercanbeforcollectedquery-complexitypairs3,byusingacrossmulated as follows: o = Classifier(q), where
entropy loss. Then, at inference, we can deter-
Classifier is a smaller Language Model that is
minethecomplexityofthequery,whichisoneof
trainedtoclassifyoneofthreedifferentcomplexity
{‘A’, ‘B’, ‘C’}, by forwarding it to Classifier:
levelsandoisitscorrespondingclasslabel. Inour
o = Classifier(q).
classifier design, there are three class labels: ‘A’,
‘B’,and‘C’,where‘A’indicatesthatq isstraight-
4 ExperimentalSetups
forward and answerable by LLM(q) itself, ‘B’ indicatesthatq hasthemoderatecomplexitywhere In this section, we explain datasets, models, metatleastasingle-stepapproachLLM(q,d)isneeded,
rics,andimplementationdetails. Weprovideaddiand‘C’indicatesthatq iscomplex,requiringthe tionaldetailsinAppendixA.
mostextensivesolutionLLM(q,d,c)2.
4.1 Datasets

### TrainingStrategy Theremainingstepistotrain

the smaller Language Model for Classifier, to Inordertosimulatearealisticscenario,wheredifaccuratelypredictitscomplexityoinresponseto ferent queries have varying complexities, we use
thegivenqueryq. Yet,thereisnoannotateddataset boththesingle-hopandmulti-hopQAdatasetssiavailable for query-complexity pairs. Hence, we multaneously,intheunifiedexperimentalsetting.
propose to automatically construct the training
Single-hopQA Forsimplerqueries,weusethree
datasetwithtwoparticularstrategies.
benchmarksingle-hopQAdatasets,whichconsist

### Tobespecific,wefirstaimatlabelingthequery

2Weconsiderthreelevelsofquerycomplexity,andleave 3Asweautomaticallyassignclassifierlabels,theremight
theexplorationofmorefine-grainedcomplexitiesasfuture beerrorsinlabelingandmightbemoreadvancedstrategiesto
work. automaticallyassignlabels,whichweleaveasfuturework.

<!-- Page 6 -->

Table1:Averagedresultsonacollectionofbenchmarkdatasetsforopen-domainquestionansweringincludingthesingle-hop
andmulti-hopqueries,withdifferentLLMs.Self-RAG∗istrainedwithadifferentbaseLLM,namelyLLaMA2(Touvronetal.,
2023);therefore,wecomparetheresultsofFLAN-T5-XL(3B)withtheresultsfromSelf-RAGwithLLaMA2(7B)andthe
resultsofotherswiththeresultsfromSelf-RAGwithLLaMA2(13B).Weemphasizeourresultsinbold,foreasycomparisons.

### FLAN-T5-XL(3B) FLAN-T5-XXL(11B) GPT-3.5(Turbo)

Types Methods EM F1 Acc Step Time EM F1 Acc Step Time EM F1 Acc Step Time
NoRetrieval 14.87 21.12 15.97 0.00 0.11 17.83 25.14 19.33 0.00 0.08 35.77 48.56 44.27 0.00 0.71

### Simple

Single-stepApproach 34.83 44.31 38.87 1.00 1.00 37.87 47.63 41.90 1.00 1.00 34.73 46.99 45.27 1.00 1.00
AdaptiveRetrieval 23.87 32.24 26.73 0.50 0.56 26.93 35.67 29.73 0.50 0.54 35.90 48.20 45.30 0.50 0.86
Adaptive Self-RAG∗ 9.90 20.79 31.57 0.72 0.43 10.87 22.98 34.13 0.74 0.23 10.87 22.98 34.13 0.74 1.50
Adaptive-RAG(Ours) 37.17 46.94 42.10 2.17 3.60 38.90 48.62 43.77 1.35 2.00 37.97 50.91 48.97 1.03 1.46
Complex Multi-stepApproach 39.00 48.85 43.70 4.69 8.81 40.13 50.09 45.20 2.13 3.80 38.13 50.87 49.70 2.81 3.33
Oracle Adaptive-RAGw/Oracle 45.00 56.28 49.90 1.28 2.11 47.17 58.60 52.20 0.84 1.10 47.70 62.80 58.57 0.50 1.03
ofqueriesandtheirassociateddocumentscontain- task performance and efficiency along with their
ing answers, namely 1) SQuAD v1.1 (Rajpurkar trade-offs. Thus, we report the results with five
etal.,2016),2)NaturalQuestions(Kwiatkowski metrics,wherethreeofthemmeasuretheeffectiveetal.,2019),and3)TriviaQA(Joshietal.,2017). nessandtheothertwomeasuretheefficiency. In
particular, for effectiveness, we use F1, EM, and

### Multi-hopQA Toconsidermorecomplexquery

Accuracy(Acc),followingthestandardevaluation
scenarios,weusethreebenchmarkmulti-hopQA
protocol (Mallen et al., 2023; Baek et al., 2023;
datasets,whichrequiresequentialreasoningover

### Asaietal.,2024),whereF1measuresthenumber

multipledocuments,namely1)MuSiQue(Trivedi
of overlapping words between the predicted anet al., 2022a), 2) HotpotQA (Yang et al., 2018),
swerandthegroundtruth,EMmeasureswhether
and3)2WikiMultiHopQA(Hoetal.,2020).
theyarethesame,andAccmeasureswhetherthe
4.2 Models predictedanswercontainstheground-truthanswer.
Forefficiency,wemeasurethenumberofretrieval-

### We compare our Adaptive-RAG against relevant

and-generatestepsandtheaveragetimeforanswermodels,includingthreeretrieval-augmentedLLM
ingeachqueryrelativetotheone-stepapproach.
strategies (in Section 3.1) and the adaptive retrievalapproaches(Mallenetal.,2023;Asaietal.,
4.4 ImplementationDetails
2024),whichcanbegroupedintooneofthreecategories: Simple,Adaptive,andComplex. Specif- ForafaircomparisonandfollowingMallenetal.
ically, Simple approaches include the 1) No Re- (2023)andTrivedietal.(2023),weusethesameretrievaland2)Single-stepApproach-basedmeth- triever,aterm-basedsparseretrievalmodelknown
ods. Adaptiveapproachesincludethe3)Adaptive asBM25(Robertsonetal.,1994),acrossalldiffer-
Retrieval(Mallenetal.,2023),4)Self-RAG(Asai entmodels. Fortheexternaldocumentcorpus,we
et al., 2024), and our 5) Adaptive-RAG, which usedifferentsourcesdependingonthedatasettype:
can adaptively perform retrieval based on the theWikipediacorpuspreprocessedbyKarpukhin
question complexity. For the 6) Multi-step Ap- et al. (2020) for single-hop datasets, and the preproach, we use the most sophisticated state-of- processedcorpusbyTrivedietal.(2023)formultithe-art method (Trivedi et al., 2023), iteratively hop datasets. Regarding the LLMs that are used
accessingboththeretrieverandLLMwithChain- to generate answers, we use the FLAN-T5 series
of-Thoughtreasoning(Weietal.,2022b),forevery models (Chung et al., 2022) of XL with 3B paquery. Notethatmodelsacrossdifferentcategories rameters and XXL with 11B parameters, and the
are not directly comparable. Yet, in the ideal set- GPT-3.5 model (gpt-3.5-turbo-instruct). For the
ting,Adaptiveapproachesshouldbemoreeffective retrieval-augmented LLM design, we follow the
thanthoseintheSimplecategorywhilesimultane- implementationdetailsfromTrivedietal.(2023),
ouslybeingmoreefficientthantheComplexone. whichincludeinputprompts,instructions,andthe
Therefore, we also report the performance in an number of test samples for evaluation (e.g., 500
idealscenario,7)Adaptive-RAGw/Oracle,using samplesperdataset). InourAdaptive-RAG,forthe
theoracleclassifierwithourAdaptive-RAG. query-complexity classifier, we use and train the
T5-Largemodel(Raffeletal.,2020). Specifically,
4.3 EvaluationMetrics
theclassifieristrainedusingtheepochthatshows
When it comes to evaluating adaptive models, it thebestperformanceuntil100trainingiterations
is essential to simultaneously consider both the fromthevalidationset,withthelearningrateof3e-

<!-- Page 7 -->

Table2:ResultsoneachofacollectionofdatasetswithFLAN-T5-XL(3B)astheLLM.Weemphasizeourresultsinbold.

### SQuAD NaturalQuestions TriviaQA

Data Types Methods EM F1 Acc Step Time EM F1 Acc Step Time EM F1 Acc Step Time
NoRetrieval 3.60 10.50 5.00 0.00 0.11 14.20 19.00 15.60 0.00 0.13 25.00 31.80 27.00 0.00 0.13

### Simple

Single-stepApproach 27.80 39.30 34.00 1.00 1.00 37.80 47.30 44.60 1.00 1.00 53.60 62.40 60.20 1.00 1.00
AdaptiveRetrieval 13.40 23.10 17.60 0.50 0.55 28.20 36.00 33.00 0.50 0.56 38.40 46.90 42.60 0.50 0.56
Single-step Adaptive Self-RAG∗ 2.20 11.20 18.40 0.63 0.50 31.40 39.00 33.60 0.63 0.17 12.80 29.30 57.00 0.68 0.45
Adaptive-RAG(Ours) 26.80 38.30 33.00 1.37 2.02 37.80 47.30 44.60 1.00 1.00 52.20 60.70 58.20 1.23 1.54
Complex Multi-stepApproach 24.40 35.60 29.60 4.52 9.03 38.60 47.80 44.20 5.04 10.18 53.80 62.40 60.20 5.28 9.22
Oracle Adaptive-RAGw/Oracle 32.00 45.60 38.20 1.24 1.60 47.40 57.10 53.60 1.10 1.55 61.60 70.20 66.40 0.79 1.10

### MuSiQue HotpotQA 2WikiMultiHopQA

Data Types Methods EM F1 Acc Step Time EM F1 Acc Step Time EM F1 Acc Step Time
NoRetrieval 2.40 10.70 3.20 0.00 0.11 16.60 22.71 17.20 0.00 0.11 27.40 32.04 27.80 0.00 0.10

### Simple

Single-stepApproach 13.80 22.80 15.20 1.00 1.00 34.40 46.15 36.40 1.00 1.00 41.60 47.90 42.80 1.00 1.00
AdaptiveRetrieval 6.40 15.80 8.00 0.50 0.55 23.60 32.22 25.00 0.50 0.55 33.20 39.44 34.20 0.50 0.55
Multi-step Adaptive Self-RAG∗ 1.60 8.10 12.00 0.73 0.51 6.80 17.53 29.60 0.73 0.45 4.60 19.59 38.80 0.93 0.49
Adaptive-RAG(Ours) 23.60 31.80 26.00 3.22 6.61 42.00 53.82 44.40 3.55 5.99 40.60 49.75 46.40 2.63 4.68
Complex Multi-stepApproach 23.00 31.90 25.80 3.60 7.58 44.60 56.54 47.00 5.53 9.38 49.60 58.85 55.40 4.17 7.37
Oracle Adaptive-RAGw/Oracle 24.80 38.50 27.00 1.98 3.99 51.20 64.00 54.80 1.59 2.77 53.00 62.30 59.40 1.01 1.69

## Flan-T5-Xl Flan-T5-Xxl

60 55 60 55
Adaptive Retrieval Adaptive Retrieval Self-RAG Self-RAG 50 Adaptive-RAG (Ours) 50 Adaptive-RAG (Ours)
50 50
40 40
45 45
30 30
40 40
20 20
10 35 10 35
F1 Classifier Acc. F1 Classifier Acc. No One Multi
oN
enO
itluM

### Confusion Matrix

0.6 0.31 0.47 0.22
0.4
0.1 0.66 0.23
0.2
0.03 0.31 0.65
Figure3:PerformanceonQAandquery-complexityassessmentofdifferentadaptiveapproachesforretrieval-augmentedLLMs
withFLAN-T5XL(Left)andXXL(Center).Forlabelingthecomplexityofqueries,weusethesilverdataannotatedfromthe
predictionoutcomesofmodels(describedinSection3.2).Wealsoprovidetheconfusionmatrixacrossthreelabels(Right).
5andtheAdamW(LoshchilovandHutter,2019) effectivenessoverthecompetitors(Table1). This
as an optimizer. Regarding its training data, we indicates that merely focusing on the decision of
sample and annotate 400 queries from 6 datasets whethertoretrieveornotissuboptimal. Also,as
basedonitsinductivebias(single-hopforone-step showninTable2,suchsimpleadaptivestrategies
approachandmulti-hopformulti-step). Inaddition, are particularly inadequate for handling complex
weusepredictedoutcomesofthreedifferentstrate- queries in multi-hop datasets, which require aggies over 400 queries sampled from each dataset. gregatedinformationandreasoningovermultiple
Notethatthosequeriesusedforclassifiertraining documents. Meanwhile,ourapproachcanconsider
donotoverlapwiththetestingqueriesforQA. amorefine-grainedqueryhandlingstrategybyfurtherincorporatinganiterativemoduleforcomplex
5 ExperimentalResultsandAnalyses
queries. Furthermore, in a realistic setting, we
shouldtakeintoaccountnotonlyeffectivenessbut
Inthissection,weshowtheoverallexperimental
alsoefficiency. AsshowninTable1,comparedto
resultsandofferin-depthanalysesofourmethod.
thecomplexmulti-stepstrategy,ourproposedadap-
MainResults Firstofall,Table1showsourmain tivestrategyissignificantlymoreefficientacross
resultsaveragedoverallconsidereddatasets,which all model sizes. This is meaningful in this era of
corroborate our hypothesis that simple retrieval- LLMs,wherethecostofaccessingthemisacritical
augmented strategies are less effective than the factorforpracticalapplicationsandscalability. Ficomplex strategy, while the complex one is sig- nally,toseetheupperboundofourAdaptive-RAG,
nificantlymoreexpensivethanthesimpleones. In wereportitsperformanceswiththeoracleclassifier
addition,wereportthemoregranularresultswith wheretheclassificationperformanceisperfect. As
FLAN-T5-XLoneachofthesingle-hopandmulti- shown in Table 1 and Table 2, we observe that it
hop datasets in Table 2 (and more with different achieves the best performance while being much
LLMsinTable7andTable8ofAppendix),which moreefficientthanourAdaptive-RAGwithoutthe
areconsistentwiththeresultsobservedinTable1. oracle classifier. These results support the valid-
However,inareal-worldscenario,notallusers ity and significance of our proposal for adapting
ask queries with the same level of complexity, retrieval-augmentedLLMstrategiesbasedonquery
whichemphasizestheimportanceoftheneedfor complexity,andfurthersuggestthedirectiontodeadaptivestrategies. Notethatamongtheadaptive velopmoreimprovedclassifierstoachieveoptimal
strategies, our Adaptive-RAG shows remarkable performance.

<!-- Page 8 -->

Table3:Theexactelapsedtimeperqueryandthepercentage Table4: ResultsonQAandcomplexityclassificationwith
ofthepredictedlabelsfromtheclassifieroverallsamples. varyingthedataannotationstrategiesfortrainingtheclassifier.
Labels Time/Query(Sec.) Percentage(%) QA Classifier(Accuracy)
TrainingStrategies F1 Step All No One Multi

### No(A) 0.35 8.60

Adaptive-RAG(Ours) 46.94 1084 54.52 30.52 66.28 65.45

### One(B) 3.08 53.33

w/oBinary 43.43 640 60.30 62.19 65.70 39.55
Multi(C) 27.18 38.07 w/oSilver 48.79 1464 40.00 0.00 53.98 75.91
ClassifierPerformance Tounderstandhowthe biasindatasets(seeSection3.2). AsTable4shows,
proposed classifier works, we analyze its perfor- comparedtothetrainingstrategyrelyingsolelyon
manceacrossdifferentcomplexitylabels. AsFig- the data derived from inductive bias, ours is sigure 3 (Left and Center) shows, the classification nificantlymoreefficient. Thisefficiencyispartly
accuracyofourAdaptive-RAGisbetterthanthose becauseoursalsotakesintoaccountthecasethat
of the other adaptive retrieval baselines, which does not consider any documents at all, as also
leadstooverallQAperformanceimprovements. In impliedbytheclassificationaccuracy;meanwhile,
otherwords,thisresultindicatesthatourAdaptive- queriesintheexistingdatasetsdonotcapturethe
RAGiscapableofmoreaccuratelyclassifyingthe informationonwhethertheretrievalisrequiredor
complexitylevelswithvariousgranularities,which not. Ontheotherhand,inthecaseofonlyusingthe
include not performing retrieval, performing re- silverdataannotatedfromthecorrectpredictions,
trievalonlyonce,andperformingretrievalmultiple whileitsoverallclassificationaccuracyishigh,the
times. Inadditiontothetruepositiveperformance overall QA performance implies that relying on
of our classifier averaged over all those three la- the silver data may not be optimal. This may be
bels in Figure 3 (Left and Center), we further re- because this silver data does not cover complexportitsconfusionmatrixinFigure3(Right). We itylabelsoverincorrectlypredictedqueries,which
notethattheconfusionmatrixrevealssomenotable leadstolowergeneralizationeffectonqueriesreltrends: ‘C (Multi)’ is sometimes misclassified as evanttothem. Meanwhile, byalsoincorporating
‘B(One)’(about31%)and‘B(One)’as‘C(Multi)’ complexitylabelsfromdatasetbias(single-hopvs
(about 23%); ‘A (No)’ is misclassified often as multi-hop),theclassifierbecomesmoreaccuratein
‘B (One)’ (about 47%) and less frequently as ‘C predictingmulti-hopqueries,leadingtothebetter
(Multi)’(about22%). Whiletheoverallresultsin performance. Itisworthnotingthatourautomatic
Figure 3 show that our classifier effectively cate- labelingstrategiesaretwoparticularinstantiations
gorizes the three labels, further refining it based for training the classifier, and that there could be
on such misclassification would be a meaningful otherinstantiations,whichweleaveasfuturework.
directionforfuturework.

### Analyses on Classifier Size To investigate the

AnalysesonEfficiencyforClassifier WhileTa- sensitivityofourclassifieraccordingtoitsvarying
ble1showstherelativeelapsedtimeforeachofthe sizes,weconductedfurtherexperiments. Asshown
threedifferentRAGstrategies,wefurtherprovide inTable6,weobservenosignificantperformance
theexactelapsedtimeperqueryforourAdaptive- differencesamongclassifiersofvarioussizes,even
RAGandthedistributionforpredictedlabelsfrom withreducedcomplexityandfewerparametersin
ourquery-complexityclassifierinTable3. Similar smallerclassifiers. Thisindicatesthatourproposed
totheresultsoftheelapsedtimeinTable1(relative classifier can contribute to resource-efficient settime), Table 3 (exact time) shows that efficiency tings in real-use cases with smaller sizes without
canbesubstantiallyimprovedbyidentifyingsim- compromisingtheperformance.
pleorstraightforwardqueries.

### CaseStudy Weconductacasestudytoqualita-

Analyses on Training Data for Classifier We tively compare our Adaptive-RAG against Adaphave shown that the classifier plays an important tive Retrieval. Table 5 shows the classified comroleinadaptiveretrieval. Here,wefurtheranalyze plexity and the query handling patterns for both
thedifferentstrategiesfortrainingtheclassifierby simpleandcomplexquestions. First,forthesimple
ablatingourfulltrainingstrategy,whichincludes single-hopquestion,ourAdaptive-RAGidentifies
two approaches: generating silver data from pre- thatitisanswerablebyonlyusingtheLLM’sparadictedoutcomesofmodelsandutilizinginductive metric knowledge about ‘Google’. By contrast,

<!-- Page 9 -->

Table5:CasestudywithGPT-3.5(Turbo),wherewerepresentthefactualerrorinredandtheaccurateinformationinblue.

### Dataset Question AdaptiveRetrieval Adaptive-RAG(Ours)

NQ Whichfamouscorpo- QueryType: B(Single-stepApproach) QueryType: A(NonRetrieval)
(Single-hop) ratelogochangedtoa Reasoning: The famous corporate logo that Reasoning: The famous corporate logo that
flat colour/color sans changedtoaflatcolorsans-seriffontinitsfirst changedtoaflatcolorsansseriffontinitsfirstmaseriffontinitsfirstma- majorchangesince1999isMicrosoft. Microsoft jorchangesince1999isGoogle.Googlechanged
jorchangesince1999? changeditslogoin2012,replacingtheitalicized, itslogoin2015.Sotheansweris:Google.
multi-coloredfontwithasimple,flat,greyfont.So
theansweris:Microsoft.
MuSiQue Whoisthechildofthe QueryType: A(NonRetrieval) QueryType: C(Multi-stepApproach)
(Multi-hop) Italiannavigatorwho Reasoning: CésarGaytanwasbornintheconti- Reasoning:CésarrGaytanwasborninGuadalajara,
explored the eastern nentofNorthAmerica.TheItaliannavigatorwho Jalisco,Mexico.TheItaliannavigatorwhoexplored
coastofthecontinent exploredtheeasterncoastofNorthAmericaforthe theeasterncoastofthecontinentfortheEnglishis
CésarGaytanwasborn EnglishwasGiovanniCaboto,alsoknownasJohn JohnCabot.JohnCabot’ssonisSebastianCabot.
infortheEnglish? Cabot. Sotheansweris: GiovanniCaboto/John Sotheansweris:SebastianCabot.
Cabot.
Table6:Resultswithvaryingmodelsizesforclassifiers. onacollectionofopen-domainQAdatasets,cover-
QA Classifier(Accuracy) ingthemultiplequerycomplexitiesincludingboth
Sizes F1 Step All No One Multi the single- and multi-hop questions. The results
demonstratethatourAdaptive-RAGenhancesthe

### Small(60M) 45.83 964 53.48 26.65 70.62 53.18

Base(223M) 45.97 983 53.41 26.42 69.46 56.82 overallaccuracyandefficiencyofQAsystems,al-
Large(770M) 46.94 1084 54.52 30.52 66.28 65.45 locatingmoreresourcestohandlecomplexqueries
while efficiently handling simpler queries, com-
Adaptive Retrieval fetches additional documents, pared to the existing one-size-fits-all approaches
leading to longer processing times and occasion- thattendtobeeitherminimalistormaximalistover
allyproducingincorrectresponsesduetotheinclu- varyingquerycomplexities.
sionofpartiallyirrelevantinformationabout‘Mi-

### Limitations

crosoft’. Meanwhile, facedwithacomplexquestion,Adaptive-RAGseeksoutrelevantinformation,

### WhileourAdaptive-RAGshowsclearadvantages

includingdetailslike‘asonofJohnCabot’,which
ineffectivenessandefficiencybydeterminingthe
maynothavebeenstoredinLLMs,whileAdaptive
query complexity and then leveraging the most

### Retrieval fails to request such information from

suitable approach for tackling it, it is important
externalsources,resultingininaccurateanswers.
torecognizethattherestillexistpotentialavenues
forimprovingtheclassifierfromtheperspectives
6 Conclusion
of its training datasets and architecture. Specifi-
Inthiswork,weproposedtheAdaptiveRetrieval- cally,astherearenoavailabledatasetsfortraining
Augmented Generation framework, referred to thequery-complexityclassifier,weautomatically
as Adaptive-RAG, to handle queries of various createnewdatabasedonthemodelpredictionoutcomplexities. Specifically, Adaptive-RAG is de- comesandtheinductivedatasetbiases. However,
signed to dynamically adjust its query handling our labeling process is one specific instantiation
strategiesintheunifiedretrieval-augmentedLLM oflabelingthequerycomplexity,anditmayhave
based on the complexity of queries that they en- thepotentialtolabelqueriesincorrectlydespiteits
counter,whichspansacrossaspectrumofthenon- effectiveness. Therefore, future work may create
retrieval-based approach for the most straightfor- newdatasetsthatareannotatedwithadiverserange
ward queries, to the single-step approach for the ofquerycomplexities,inadditiontothelabelsof
queriesofmoderatecomplexity,andfinallytothe question-answer pairs. Also, as the performance
multi-stepapproachforthecomplexqueries. The gapbetweentheidealclassifierinTable1andthe
core step of our Adaptive-RAG lies in determin- currentclassifierinFigure3indicates,thereisstill
ing the complexity of the given query, which is roomtoimprovetheeffectivenessoftheclassifier.
instrumental in selecting the most suitable strat- Inotherwords,ourclassifierdesignbasedonthe
egyforitsanswer. Tooperationalizethisprocess, smallerLMistheinitial,simplestinstantiationfor
wetrainedasmallerLanguageModelwithquery- classifyingthequerycomplexity,andbasedupon
complexity pairs, which are automatically anno- it, future work may improve the classifier architatedfromthepredictedoutcomesandtheinductive tectureanditsperformance,whichwillpositively
biasesindatasets. WevalidatedourAdaptive-RAG contributetotheoverallQAperformance.

<!-- Page 10 -->

EthicsStatement HannanehHajishirzi.2024. Self-RAG:Learningto
retrieve,generate,andcritiquethroughself-reflection.
The experimental results on Adaptive-RAG vali- InTheTwelfthInternationalConferenceonLearning
dateitsapplicabilityinrealisticscenarios,wherea Representations.
widerangeofdiverseuserqueriesexist. Nonethe-

### JinheonBaek,SoyeongJeong,MinkiKang,JongPark,

less,giventhepotentialdiversityofreal-worlduser andSungJuHwang.2023. Knowledge-augmented
inputs,itiscrucialtoalsoconsiderscenarioswhere languagemodelverification. InProceedingsofthe
2023ConferenceonEmpiricalMethodsinNatural
these inputs might be offensive or harmful. We
LanguageProcessing,EMNLP2023,Singapore,Deshouldbeawarethatsuchinputscouldleadtothe
cember6-10,2023,pages1720–1736.Association
retrieval of offensive documents and the genera- forComputationalLinguistics.
tion of inappropriate responses by the retrieval-
SebastianBorgeaud,ArthurMensch,JordanHoffmann,
augmented LLMs. To address this challenge, de-

### TrevorCai,ElizaRutherford,KatieMillican,George

velopingmethodstodetectandmanageoffensive vandenDriessche, Jean-BaptisteLespiau, Bogdan
orinappropriatecontentinbothuserinputsandre- Damoc,AidanClark,DiegodeLasCasas,Aurelia
Guy, Jacob Menick, Roman Ring, Tom Hennigan,
trieveddocumentswithintheretrieval-augmented

### SaffronHuang,LorenMaggiore,ChrisJones,Albin

framework is essential. We believe that this is a

### Cassirer, AndyBrock, MichelaPaganini, Geoffrey

criticalareaforfuturework. Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan,JackW.Rae,ErichElsen,andLaurentSifre.
Acknowledgements 2022. Improvinglanguagemodelsbyretrievingfrom
trillionsoftokens. InInternationalConferenceon
ThisworkwassupportedbyInstituteforInforma- MachineLearning,ICML2022,17-23July2022,BaltionandcommunicationsTechnologyPromotion timore,Maryland,USA,volume162ofProceedings
of Machine Learning Research, pages 2206–2240.
(IITP)grantfundedbytheKoreagovernment(No.

## Pmlr.

2018-0-00582,Predictionandaugmentationofthe
credibilitydistributionvialinguisticanalysisand TomB.Brown,BenjaminMann,NickRyder,Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
automated evidence document collection), Basic

### Neelakantan,PranavShyam,GirishSastry,Amanda

Science Research Program through the National

### Askell, Sandhini Agarwal, Ariel Herbert-Voss,

ResearchFoundationofKorea(NRF)fundedbythe Gretchen Krueger, Tom Henighan, Rewon Child,
MinistryofEducation(RS-2023-00275747),and Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,

### ClemensWinter,ChristopherHesse,MarkChen,Eric

the Artificial intelligence industrial convergence
Sigler,MateuszLitwin,ScottGray,BenjaminChess,
clusterdevelopmentprojectfundedbytheMinistry

### Jack Clark, Christopher Berner, Sam McCandlish,

of Science and ICT (MSIT, Korea) & Gwangju Alec Radford, Ilya Sutskever, and Dario Amodei.
MetropolitanCity. 2020. Languagemodelsarefew-shotlearners. InAdvancesinNeuralInformationProcessingSystems33:
AnnualConferenceonNeuralInformationProcessing Systems 2020, NeurIPS 2020, December 6-12,

### References

2020,virtual.
RohanAnil,AndrewM.Dai,OrhanFirat,MelvinJohn-

### DanqiChen,AdamFisch,JasonWeston,andAntoine

son, Dmitry Lepikhin, Alexandre Passos, Siamak
Bordes. 2017. Reading wikipedia to answer open-
Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
domainquestions. InProceedingsofthe55thAnnual

### Chen, Eric Chu, Jonathan H. Clark, Laurent El


### Meeting of the Association for Computational Lin-

Shafey,YanpingHuang,KathyMeier-Hellstern,Gauguistics, ACL 2017, Vancouver, Canada, July 30 -
ravMishra,EricaMoreira,MarkOmernick,Kevin
August4,Volume1: LongPapers,pages1870–1879.
Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao,
AssociationforComputationalLinguistics.

### YuanzhongXu,YujingZhang,GustavoHernández

Ábrego,JunwhanAhn,JacobAustin,PaulBarham, Sukmin Cho, Jeongyeon Seo, Soyeong Jeong, and
JanA.Botha,JamesBradbury,SiddharthaBrahma, JongC.Park.2023. Improvingzero-shotreaderby
KevinBrooks,MicheleCatasta,YongCheng,Colin reducingdistractionsfromirrelevantdocumentsin
Cherry,ChristopherA.Choquette-Choo,Aakanksha open-domainquestionanswering. InFindingsofthe
Chowdhery,ClémentCrepy,ShachiDave,Mostafa AssociationforComputationalLinguistics: EMNLP
Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, 2023,Singapore,December6-10,2023,pages3145–
Nan Du, Ethan Dyer, Vladimir Feinberg, Fangxi- 3157.AssociationforComputationalLinguistics.
aoyu Feng, Vlad Fienber, Markus Freitag, Xavier
Garcia,SebastianGehrmann,LucasGonzalez,and HyungWonChung,LeHou,ShayneLongpre,Barret
etal.2023. Palm2technicalreport. arXivpreprint Zoph,YiTay,WilliamFedus,EricLi,XuezhiWang,
arXiv:2305.10403. MostafaDehghani,SiddharthaBrahma,AlbertWebson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-
AkariAsai,ZeqiuWu,YizhongWang,AvirupSil,and gun,XinyunChen,AakankshaChowdhery,Sharan

<!-- Page 11 -->

Narang,GauravMishra,AdamsYu,VincentY.Zhao, 2022. RealtimeQA:what’stheanswerrightnow?
YanpingHuang,AndrewM.Dai,HongkunYu,Slav arXivpreprintarXiv:2207.13332.

### Petrov, EdH.Chi, JeffDean, JacobDevlin, Adam

Roberts, DennyZhou, QuocV.Le, andJasonWei. Omar Khattab, Keshav Santhanam, Xiang Lisa

## Scalinginstruction-finetunedlanguagemodels. Li, David Hall, Percy Liang, Christopher Potts,

arXivpreprintarXiv:2210.11416. and Matei Zaharia. 2022. Demonstrate-searchpredict: Composing retrieval and language mod-
XanhHo,Anh-KhoaDuongNguyen,SakuSugawara, els for knowledge-intensive NLP. arXiv preprint
andAkikoAizawa.2020. ConstructingAmulti-hop arXiv.2212.14024,abs/2212.14024.
QAdatasetforcomprehensiveevaluationofreasoningsteps. InProceedingsofthe28thInternational TusharKhot,HarshTrivedi,MatthewFinlayson,Yao
ConferenceonComputationalLinguistics,COLING Fu,KyleRichardson,PeterClark,andAshishSab-
2020, Barcelona, Spain (Online), December 8-13, harwal.2023. Decomposedprompting: Amodular
2020,pages6609–6625.InternationalCommitteeon approachforsolvingcomplextasks. InTheEleventh
ComputationalLinguistics. International Conference on Learning Representations, ICLR2023, Kigali, Rwanda, May1-5, 2023.
GautierIzacardandEdouardGrave.2021. Leveraging OpenReview.net.
passageretrievalwithgenerativemodelsforopendomainquestionanswering. InProceedingsofthe16th TomKwiatkowski, JennimariaPalomaki, OliviaRed-
ConferenceoftheEuropeanChapteroftheAssoci- field,MichaelCollins,AnkurParikh,ChrisAlberti,
ationforComputationalLinguistics: MainVolume, DanielleEpstein,IlliaPolosukhin,JacobDevlin,Ken-
EACL2021,Online,April19-23,2021,pages874– tonLee,KristinaToutanova,LlionJones,Matthew

### AssociationforComputationalLinguistics. Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob


### Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-

Gautier Izacard, Patrick S. H. Lewis, Maria Lomeli, ralquestions: Abenchmarkforquestionanswering
Lucas Hosseini, Fabio Petroni, Timo Schick, Jane research. TransactionsoftheAssociationforCompu-
Dwivedi-Yu,ArmandJoulin,SebastianRiedel,and tationalLinguistics,7:452–466.

### Edouard Grave. 2023. Atlas: Few-shot learning

withretrievalaugmentedlanguagemodels. J.Mach. Angeliki Lazaridou, Elena Gribovskaya, Wojciech
Learn.Res.,24:251:1–251:43. Stokowiec, and Nikolai Grigorev. 2022. Internetaugmented language models through few-shot
SoyeongJeong,JinheonBaek,SukminCho,SungJu prompting for open-domain question answering.
Hwang,andJongPark.2023. Test-timeself-adaptive arXivpreprintarXiv:2203.05115.
smalllanguage modelsfor questionanswering. In
FindingsoftheAssociationforComputationalLin- Belinda Z. Li, Sewon Min, Srinivasan Iyer, Yashar
guistics: EMNLP2023,Singapore,December6-10, Mehdad,andWen-tauYih.2020. Efficientone-pass
2023,pages15459–15469.AssociationforComputa- end-to-endentitylinkingforquestions. InProceedtionalLinguistics. ingsofthe2020ConferenceonEmpiricalMethodsin

### NaturalLanguageProcessing,EMNLP2020,Online,

ZhengbaoJiang,FrankF.Xu,LuyuGao,ZhiqingSun, November16-20,2020,pages6433–6441.Associa-
Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie tionforComputationalLinguistics.

### Callan,andGrahamNeubig.2023. Activeretrieval

augmentedgeneration. InEMNLP2023. Ilya Loshchilov and Frank Hutter. 2019. Decoupled
weight decay regularization. In 7th International
MandarJoshi,EunsolChoi,DanielS.Weld,andLuke ConferenceonLearningRepresentations,ICLR2019,
Zettlemoyer.2017. Triviaqa: Alargescaledistantly New Orleans, LA, USA, May 6-9, 2019. OpenResupervisedchallengedatasetforreadingcomprehen- view.net.
sion. InProceedingsofthe55thAnnualMeetingof
theAssociationforComputationalLinguistics,ACL AlexMallen,AkariAsai,VictorZhong,RajarshiDas,
2017,Vancouver,Canada,July30-August4,Volume Daniel Khashabi, and Hannaneh Hajishirzi. 2023.
1: LongPapers,pages1601–1611.Associationfor When not to trust language models: Investigating
ComputationalLinguistics. effectivenessofparametricandnon-parametricmemories. InProceedingsofthe61stAnnualMeetingof
VladimirKarpukhin,BarlasOguz,SewonMin,Patrick theAssociationforComputationalLinguistics(Vol-
S.H.Lewis,LedellWu,SergeyEdunov,DanqiChen, ume1: LongPapers),ACL2023,Toronto,Canada,
andWen-tauYih.2020. Densepassageretrievalfor July9-14,2023,pages9802–9822.Associationfor
open-domainquestionanswering. InProceedingsof ComputationalLinguistics.
the2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,EMNLP2020,November OpenAI.2023. GPT-4technicalreport. arXivpreprint
16-20,2020.AssociationforComputationalLinguis- arXiv:2303.08774.
tics.

### Adam Paszke, Sam Gross, Francisco Massa, Adam

JungoKasai,KeisukeSakaguchi,YoichiTakahashi,Ro- Lerer, James Bradbury, Gregory Chanan, Trevor
nan Le Bras, Akari Asai, Xinyan Yu, Dragomir R. Killeen, Zeming Lin, Natalia Gimelshein, Luca
Radev,NoahA.Smith,YejinChoi,andKentaroInui. Antiga,AlbanDesmaison,AndreasKöpf,EdwardZ.

<!-- Page 12 -->

Yang,ZacharyDeVito,MartinRaison,AlykhanTe- augmented black-box language models. arXiv
jani,SasankChilamkurthy,BenoitSteiner,LuFang, preprintarXiv:2301.12652.

### JunjieBai,andSoumithChintala.2019. Pytorch: An

imperativestyle,high-performancedeeplearningli- Hugo Touvron, Louis Martin, Kevin Stone, Peter Albrary. InAdvancesinNeuralInformationProcessing bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Systems32: AnnualConferenceonNeuralInforma- Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
tionProcessingSystems2019,pages8024–8035. Bhosale,DanBikel,LukasBlecher,CristianCanton-

### Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu,

JayrAlencarPereira,RobsondoNascimentoFidalgo, JudeFernandes,JeremyFu,WenyinFu,BrianFuller,
RobertodeAlencarLotufo, andRodrigoFrassetto CynthiaGao,VedanujGoswami,NamanGoyal,An-
Nogueira.2023. Visconde:Multi-documentQAwith thonyHartshorn,SagharHosseini,RuiHou,Hakan
GPT-3andneuralreranking. InAdvancesinInforma- Inan,MarcinKardas,ViktorKerkez,MadianKhabsa,
tionRetrieval-45thEuropeanConferenceonInfor- IsabelKloumann,ArtemKorenev,PunitSinghKoura,
mationRetrieval,ECIR2023,Dublin,Ireland,April Marie-AnneLachaux,ThibautLavril,JenyaLee,Di-
2-6, 2023, Proceedings, Part II, volume 13981 of anaLiskovich,YinghaiLu,YuningMao,XavierMar-
LectureNotesinComputerScience,pages534–543. tinet,TodorMihaylov,PushkarMishra,IgorMoly-
Springer. bog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein,RashiRungta,KalyanSaladi,AlanSchelten,
OfirPress,MuruZhang,SewonMin,LudwigSchmidt, Ruan Silva, Eric Michael Smith, Ranjan Subrama-
NoahA.Smith,andMikeLewis.2023. Measuring nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tayandnarrowingthecompositionalitygapinlanguage lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
models. InFindingsoftheAssociationforComputa- ZhengYan,IliyanZarov,YuchenZhang,AngelaFan,
tionalLinguistics: EMNLP2023. Melanie Kambadur, Sharan Narang, Aurélien Rodriguez,RobertStojnic,SergeyEdunov,andThomas
PengQi,HaejunLee,TgSido,andChristopherD.Man- Scialom.2023. Llama2: Openfoundationandfinening. 2021. Answering open-domain questions of tunedchatmodels. arXivpreprintarXiv:2307.09288.
varying reasoning steps from text. In Proceedings
of the 2021 Conference on Empirical Methods in HarshTrivedi,NiranjanBalasubramanian,TusharKhot,
Natural Language Processing, EMNLP 2021, Vir- and Ashish Sabharwal. 2022a. Musique: MultitualEvent/PuntaCana,DominicanRepublic,7-11 hopquestionsviasingle-hopquestioncomposition.
November,2021,pages3599–3614.Associationfor Trans.Assoc.Comput.Linguistics,10:539–554.
ComputationalLinguistics.

### HarshTrivedi,NiranjanBalasubramanian,TusharKhot,

ColinRaffel,NoamShazeer,AdamRoberts,Katherine andAshishSabharwal.2022b. ♪MuSiQue: Multi-
Lee,SharanNarang,MichaelMatena,YanqiZhou, hopquestionsviasingle-hopquestioncomposition.
WeiLi,andPeterJ.Liu.2020. Exploringthelimits TransactionsoftheAssociationforComputational
oftransferlearningwithaunifiedtext-to-texttrans- Linguistics,10:539–554.
former. J.Mach.Learn.Res.,21:140:1–140:67.

### HarshTrivedi,NiranjanBalasubramanian,TusharKhot,

PranavRajpurkar,JianZhang,KonstantinLopyrev,and andAshishSabharwal.2023. Interleavingretrieval
Percy Liang. 2016. Squad: 100, 000+ questions with chain-of-thought reasoning for knowledgeformachinecomprehensionoftext. InProceedings intensive multi-step questions. In Proceedings of
of the 2016 Conference on Empirical Methods in the61stAnnualMeetingoftheAssociationforCom-
NaturalLanguageProcessing,EMNLP2016,Austin, putational Linguistics (Volume 1: Long Papers),
Texas,USA,November1-4,2016,pages2383–2392. ACL2023,Toronto,Canada,July9-14,2023,pages
TheAssociationforComputationalLinguistics. 10014–10037. Association for Computational Linguistics.

### OriRam,YoavLevine,ItayDalmedigos,DorMuhlgay,

Amnon Shashua, Kevin Leyton-Brown, and Yoav Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,
Shoham.2023. In-contextretrieval-augmentedlan- Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
guagemodels. TransactionsoftheAssociationfor MaartenBosma,DennyZhou,DonaldMetzler,EdH.
ComputationalLinguistics. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy

### Liang,JeffDean,andWilliamFedus.2022a. Emer-

Stephen E. Robertson, Steve Walker, Susan Jones, gentabilitiesoflargelanguagemodels. Trans.Mach.
Micheline Hancock-Beaulieu, and Mike Gatford. Learn.Res.,2022.

## OkapiatTREC-3. InProceedingsofTheThird

Text REtrieval Conference, TREC 1994, Gaithers- JasonWei,XuezhiWang,DaleSchuurmans,Maarten
burg,Maryland,USA,November2-4,1994,volume Bosma,BrianIchter,FeiXia,EdH.Chi,QuocV.Le,
500-225 of NIST Special Publication, pages 109– andDennyZhou.2022b. Chain-of-thoughtprompt-

## NationalInstituteofStandardsandTechnology ing elicits reasoning in large language models. In

(NIST). NeurIPS.
Weijia Shi, Sewon Min, Michihiro Yasunaga, Min- Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
joon Seo, Rich James, Mike Lewis, Luke Zettle- Chaumond,ClementDelangue,AnthonyMoi,Piermoyer,andWen-tauYih.2023. REPLUG:retrieval- ricCistac,TimRault,RémiLouf,MorganFuntowicz,

<!-- Page 13 -->

JoeDavison,SamShleifer,PatrickvonPlaten,Clara

### Ma,YacineJernite,JulienPlu,CanwenXu,TevenLe


### Scao, Sylvain Gugger, Mariama Drame, Quentin

Lhoest,andAlexanderM.Rush.2020. Transformers: State-of-the-artnaturallanguageprocessing. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System

### Demonstrations,EMNLP2020-Demos,pages38–


### AssociationforComputationalLinguistics.


### LeeXiong,ChenyanXiong,YeLi,Kwok-FungTang,


### Jialin Liu, Paul N. Bennett, Junaid Ahmed, and

ArnoldOverwijk.2021. Approximatenearestneighbor negative contrastive learning for dense text retrieval. In9thInternationalConferenceonLearning
Representations,ICLR2021,VirtualEvent,Austria,
May3-7,2021.OpenReview.net.

### WeiYang,YuqingXie,AileenLin,XingyuLi,Luchen

Tan, Kun Xiong, Ming Li, and Jimmy Lin. 2019.

### End-to-end open-domain question answering with

bertserini. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language

### Technologies,NAACL-HLT2019,Minneapolis,MN,

USA,June2-7,2019,Demonstrations,pages72–77.
AssociationforComputationalLinguistics.

### ZhilinYang,PengQi,SaizhengZhang,YoshuaBengio,

WilliamCohen,RuslanSalakhutdinov,andChristopher D. Manning. 2018. HotpotQA: A dataset for
diverse, explainablemulti-hopquestionanswering.
In Proceedings of the 2018 Conference on EmpiricalMethodsinNaturalLanguageProcessing,pages
2369–2380,Brussels,Belgium.AssociationforComputationalLinguistics.
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Shafran,KarthikR.Narasimhan,andYuanCao.2023.
React: Synergizingreasoningandactinginlanguage
models. InTheEleventhInternationalConference
on Learning Representations, ICLR 2023, Kigali,
Rwanda,May1-5,2023.OpenReview.net.

### Fengbin Zhu, Wenqiang Lei, Chao Wang, Jianming

Zheng, Soujanya Poria, and Tat-Seng Chua. 2021.

### Retrievingandreading: Acomprehensivesurveyon

open-domain question answering. arXiv preprint
arXiv:2101.00774.

<!-- Page 14 -->

50
40
30
20
0 2 4 6 8 10
Time per Query

## )1F(

ecnamrofreP

### Performance vs Time with FLAN-T5-XL

Adaptive-RAG (Ours) Multi-step Approach 50
Single-step Approach
40
Adaptive Retrieval
30

### No Retrieval

0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0

### Time per Query


### Figure4:QAperformance(F1)andefficiency(Time/Query)

fordifferentretrieval-augmentedgenerationapproaches.We
usetheFLAN-T5-XL(3B)asthebaseLLM.
A AdditionalExperimentalSetups

### A.1 Datasets

We use publicly open datasets for both singlehop and multi-hop QA datasets, referring to
asKarpukhinetal.(2020)andTrivedietal.(2023),
respectively. We describe the characteristics of
eachdataset:
1)SQuADv1.1(Rajpurkaretal.,2016)iscreated
throughaprocesswhereannotatorswritequestions
basedonthedocumentstheyread.
2)NaturalQuestions(Kwiatkowskietal.,2019)is
constructedbyrealuserqueriesonGoogleSearch.
3) TriviaQA (Joshi et al., 2017) comprises trivia
questionssourcedfromvariousquizwebsites.
4)MuSiQue(Trivedietal.,2022a)iscollectedby
compositingmultiplesingle-hopqueries,toform
queriesspanning2-4hops.
5)HotpotQA(Yangetal.,2018)isconstructedby
havingannotatorscreatequestionsthatlinkmultipleWikipediaarticles.
6)2WikiMultiHopQA(Hoetal.,2020)isderived
fromWikipediaanditsassociatedknowledgegraph
path,needing2-hops.

### A.2 Models


### Wedescribethedetailsofmodelsasfollows:

1)NoRetrieval. ThisapproachusesonlytheLLM
itself,togeneratetheanswertothegivenquery.
2)Single-stepApproach. Thisapproachfirstretrievestherelevantknowledgewiththegivenquery
fromtheexternalknowledgesourcesandthenaugments the LLM with this retrieved knowledge to
generatetheanswer,whichiteratesonlyonce.
3)AdaptiveRetrieval. Thisbaseline(Mallenetal.,
2023) adaptively augments the LLM with the retrieval module, only when the entities appearing
inqueriesarelesspopular. Toextractentities,we
use the available entity-linking method (Li et al.,
2020),namelyBLINK,forquestions.
4) Self-RAG. This baseline (Asai et al., 2024)

## )1F(

ecnamrofreP

### Performance vs Time with FLAN-T5-XXL

Adaptive-RAG (Ours) Multi-step Approach
Single-step Approach
Adaptive Retrieval

### No Retrieval


### Figure5:QAperformance(F1)andefficiency(Time/Query)

fordifferentretrieval-augmentedgenerationapproaches.We
usetheFLAN-T5-XXL(11B)asthebaseLLM.
trainstheLLMtoadaptivelyperformretrievaland
generation,wheretheretrievalisconductedonceit
predictsthespecialretrievaltokenaboveacertain
threshold,andtheanswergenerationfollows.
5) Adaptive-RAG. This is our model that adaptively selects the retrieval-augmented generation
strategy, smoothly oscillating between the nonretrieval,single-stepapproach,andmulti-stepapproaches4 withoutarchitecturalchanges,basedon
thequerycomplexityassessedbytheclassifier.
6)Multi-stepApproach. Thisapproach(Trivedi
etal.,2023)isthemulti-stepretrieval-augmented

### LLM,whichiterativelyaccessesboththeretriever

andLLMwithinterleavedChain-of-Thoughtreasoning(Weietal.,2022b)repeatedlyuntilitderives
thesolutionorreachesthemaximumstepnumber.
7)Adaptive-RAGw/OracleThisisanidealscenario of our Adaptive-RAG equipped with an oracleclassifierthatperfectlycategorizesthequery
complexity.

### A.3 ImplementationDetails


### For computing resources, we use A100 GPUs

with 80GB memory. In addition, due to the significantcostsassociatedwithevaluatingretrievalaugmentedgenerationmodels,weperformexperimentswithasinglerun. Finally,weimplemented
models using PyTorch (Paszke et al., 2019) and
Transformerslibrary(Wolfetal.,2020).

### B AdditionalExperimentalResults

PerformancevsTime Wefurtherprovideacomparison of different retrieval-augmented generationapproacheswithFLAN-T5-XLandFLAN-T5-

### XXLmodelsinFigure4andFigure5,respectively,

inthecontextofperformanceandefficiencytradeoffs. SimilartotheobservationmadefromtheGPT-
3.5modelinFigure1,ourproposedAdaptive-RAG
issignificantlymoreeffectiveaswellasefficient.
4Forthemulti-stepapproach,weusethestate-of-the-art
questionansweringstrategyfromIRCoT(Trivedietal.,2023).

<!-- Page 15 -->

Table7:ResultsoneachofacollectionofdatasetswithFLAN-T5-XXL(11B)astheLLM.Weemphasizeourresultsinbold.

### SQuAD NaturalQuestions TriviaQA

Data Types Methods EM F1 Acc Step Time EM F1 Acc Step Time EM F1 Acc Step Time
NoRetrieval 7.00 14.40 8.40 0.00 0.08 18.80 25.50 20.40 0.00 0.08 32.80 39.20 35.40 0.00 0.08

### Simple

Single-stepApproach 28.80 40.80 35.00 1.00 1.00 41.40 51.20 47.60 1.00 1.00 56.00 64.70 61.80 1.00 1.00
AdaptiveRetrieval 15.60 25.60 20.00 0.50 0.54 31.00 39.70 35.00 0.50 0.54 44.80 52.20 48.60 0.50 0.54
Single-step Adaptive Self-RAG∗ 1.60 11.90 20.80 0.59 0.31 39.20 47.10 42.40 0.75 0.09 14.60 33.70 60.20 0.76 0.22
Adaptive-RAG(Ours) 27.80 39.80 34.00 1.17 1.50 41.20 51.00 47.40 1.00 1.00 52.00 60.30 57.20 1.03 1.33
Complex Multi-stepApproach 24.60 36.90 30.20 2.13 3.83 39.60 49.60 46.40 2.16 3.94 52.60 61.10 59.40 2.17 4.03
Oracle Adaptive-RAGw/Oracle 32.80 46.90 38.20 0.85 0.94 51.20 61.00 57.00 0.71 0.91 63.40 71.30 68.20 0.51 0.60

### MuSiQue HotpotQA 2WikiMultiHopQA

Data Types Methods EM F1 Acc Step Time EM F1 Acc Step Time EM F1 Acc Step Time
NoRetrieval 4.20 13.40 5.40 0.00 0.08 17.40 25.44 18.40 0.00 0.09 26.80 32.93 28.00 0.00 0.08

### Simple

Single-stepApproach 16.80 25.70 19.20 1.00 1.00 37.60 49.27 39.60 1.00 1.00 46.60 54.13 48.20 1.00 1.00
AdaptiveRetrieval 8.40 17.80 10.20 0.50 0.54 26.60 36.01 27.80 0.50 0.54 35.20 42.68 36.80 0.50 0.54
Multi-step Adaptive Self-RAG∗ 1.20 8.20 11.80 0.68 0.27 5.60 17.86 30.60 0.76 0.26 3.00 19.14 39.00 0.90 0.25
Adaptive-RAG(Ours) 20.60 28.50 23.20 1.89 3.12 44.20 54.78 46.80 1.58 2.53 47.60 57.36 54.00 1.46 2.55
Complex Multi-stepApproach 19.40 27.50 21.80 2.09 3.66 47.00 57.81 49.40 2.08 3.73 57.60 67.65 64.00 2.17 3.63
Oracle Adaptive-RAGw/Oracle 24.20 37.20 26.60 1.22 1.71 52.20 64.80 54.60 0.92 1.33 59.20 70.40 68.60 0.82 1.14
Table8:ResultsoneachofacollectionofdatasetswithGPT-3.5(Turbo)astheLLM.Weemphasizeourresultsinbold.

### SQuAD NaturalQuestions TriviaQA

Data Types Methods EM F1 Acc Step Time EM F1 Acc Step Time EM F1 Acc Step Time
NoRetrieval 16.00 29.20 23.80 0.00 0.62 39.80 55.70 55.00 0.00 0.56 64.00 75.60 75.80 0.00 0.68

### Simple

Single-stepApproach 18.00 33.80 29.20 1.00 1.00 32.40 46.80 54.80 1.00 1.00 55.20 66.50 65.80 1.00 1.00
AdaptiveRetrieval 15.40 30.00 24.40 0.50 0.81 36.40 51.20 56.60 0.50 0.78 62.00 71.90 72.20 0.50 0.84
Single-step Adaptive Self-RAG∗ 1.60 11.90 20.80 0.59 1.91 39.20 47.10 42.40 0.75 0.52 14.60 33.70 60.20 0.76 1.59
Adaptive-RAG(Ours) 19.80 34.40 30.00 0.87 1.21 36.80 52.00 56.60 0.68 0.86 62.40 73.80 73.80 0.22 0.79
Complex Multi-stepApproach 17.40 31.50 26.20 2.50 3.24 35.60 49.70 57.80 2.58 3.79 54.80 67.10 68.00 2.30 2.65
Oracle Adaptive-RAGw/Oracle 28.00 45.90 39.40 0.54 0.93 50.00 65.40 67.00 0.28 0.8 70.80 81.00 80.00 0.11 0.73

### MuSiQue HotpotQA 2WikiMultiHopQA

Data Types Methods EM F1 Acc Step Time EM F1 Acc Step Time EM F1 Acc Step Time
NoRetrieval 20.40 31.30 24.40 0.00 0.81 37.40 51.04 43.20 0.00 0.74 37.00 48.50 43.40 0.00 0.90

### Simple

Single-stepApproach 16.40 26.70 23.60 1.00 1.00 39.60 50.44 45.60 1.00 1.00 46.80 57.69 52.60 1.00 1.00
AdaptiveRetrieval 18.80 30.30 24.80 0.50 0.90 38.60 50.70 43.20 0.50 0.87 44.20 55.11 50.60 0.50 0.95
Multi-step Adaptive Self-RAG∗ 1.20 8.20 11.80 0.68 1.66 5.60 17.86 30.60 0.76 1.67 3.00 19.14 39.00 0.90 1.81
Adaptive-RAG(Ours) 21.80 32.60 29.60 1.90 2.29 40.40 52.56 47.00 0.93 1.48 46.60 60.09 56.80 1.59 2.23
Complex Multi-stepApproach 23.00 32.50 31.60 3.41 3.61 45.80 58.36 52.20 2.73 3.18 52.20 66.08 62.40 3.36 3.35
Oracle Adaptive-RAGw/Oracle 29.60 44.70 35.60 0.90 1.45 55.60 69.90 62.80 0.54 1.08 52.20 69.90 66.60 0.65 1.21
PerformanceperDataset InadditiontodetailingtheperformanceofeachdatasetwiththeFLAN-

### T5-XLmodel,asshowninTable2,wealsopresent

the results for each dataset with the FLAN-T5-

### XXLandGPT-3.5modelsinTable2andTable8,

respectively. The experimental results show that
ourAdaptive-RAGconsistentlybalancesbetween
efficiency and accuracy. It is worth noting that
while the GPT-3.5 model performs effectively in
addressing straightforward queries even without
document retrieval, it benefits significantly from
ourAdaptive-RAGintermsofeffectivenesswhen
solvingcomplexmulti-hopqueries.

## Tables

**Table (Page 1):**

|  |  |  |  |  |  |  |  |
|---|---|---|---|---|---|---|---|
| Ad | aptive-RA | G (Ours) |  | Mu | lti-step App | roach |  |
|  |  |  |  |  |  |  |  |
|  | No R A | etrieval daptive Ret | rieval |  |  |  |  |
|  |  | Single-ste | p Approach |  |  |  |  |
|  |  |  |  |  |  |  |  |


**Table (Page 2):**

| (A) Single-Step Approach (B) Multi-Step Approach (C) Our Adaptive Approach k times S W i h m e p n l i e s Q th u e e b r ir y t : h day Documents S W i h m e p n l i e s Q th u e e b r ir y t : h day Documents S Pa tr r a is i g is h t t h f e o r c w ap a i r ta d l Q of u w e h r a y t : ? Answer of Michael F. Phelps? Retrieval of Michael F. Phelps? (Intermediate) Answer Answers Simple Query: Documents Inefficient When is the birthday Complex Query: of Michael F. Phelps? Answer What currency is in Documents k times Classifier k times Billy Giles’ birthplace? Retrieval Answer C W Bi o l h ly m a t G p c i l l u e e r s x re ’ Q b n i c r u y th e i p r s l y a i : n c e? (I D nt o e c r u m m ed en ia t t s e) C W Bi o l h ly m a t G p c i l l u e e r s x re ’ Q b n i c r u y th e i p r s l y a i : n c e? Retrieval (I D nt o e c r u m m ed en ia t t s e) Inaccurate Answers Answers |  |
|---|---|
|  | Documents l (Intermediate) Answers |


**Table (Page 7):**

| FLAN-T5-XL 60 55 Adaptive Retrieval Self-RAG 50 Adaptive-RAG (Ours) 50 40 45 30 40 20 10 35 F1 Classifier Acc. |  |  |  |  |  |
|---|---|---|---|---|---|
|  |  |  |  |  |  |
|  |  |  |  |  |  |
|  |  | Clas | sifier | Acc. |  |
|  |  | QA and odel Los gar 40 bia | and XX s(de hchi ding 0 qu s(s | quer L(C scri lov its eri ingl |  |


**Table (Page 7):**

| FLAN-T5-XXL 60 55 Adaptive Retrieval Self-RAG 50 Adaptive-RAG (Ours) 50 40 45 30 40 20 10 35 F1 Classifier Acc. |  |  |  |  |  |
|---|---|---|---|---|---|
|  |  |  |  |  |  |
|  |  |  |  |  |  |
|  |  |  |  |  |  |
|  |  | Clas | sifier | Acc. |  |
|  |  | iffer lexit provi fect dic het ow | enta yof deth ive ates hert nin | dapt quer eco ness tha ore Ta |  |


**Table (Page 7):**

| 0.31 | 0.47 | 0.22 |
|---|---|---|
| 0.1 | 0.66 | 0.23 |
| 0.03 | 0.31 | 0.65 |


**Table (Page 7):**

|  |  |  |
|---|---|---|
|  |  |  |
|  | F1 |  |
|  |  |  |


**Table (Page 7):**

|  |  |  |
|---|---|---|
|  | F1 |  |
|  |  |  |


**Table (Page 14):**

|  |  |  |  |  |  |
|---|---|---|---|---|---|
|  | Adaptive-R Singl | AG (Ours) e-step Approa | M ch | ulti-step Appr | oach |
|  | Adaptive | Retrieval |  |  |  |
|  | No Retrieva | l |  |  |  |
|  |  |  |  |  |  |


**Table (Page 14):**

|  | Adapt | ive-RAG | (Ours) |  |  |  |  |  |
|---|---|---|---|---|---|---|---|---|
|  |  | Single-s | tep App | roach | Multi-s | tep Appr | oach |  |
|  | Adapti | ve Retrie | val |  |  |  |  |  |
| No Re | trieval |  |  |  |  |  |  |  |
