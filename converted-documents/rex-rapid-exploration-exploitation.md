---
title: "REX Rapid Exploration Exploitation"
original_file: "./REX_Rapid_Exploration_Exploitation.pdf"
document_type: "research"
conversion_date: "2025-11-29"
topics: ["prompt-engineering", "llm", "rag", "chain-of-thought", "react"]
keywords: ["rex", "ucb", "action", "reward", "llm", "step", "final", "page", "agent", "ucl"]
summary: "<!-- Page 1 -->


### PublishedasaconferencepaperatICLR2024


## Rex: Rapid Exploration And Exploitation


## For Ai Agents

RitheshMurthy†, ShelbyHeinecke†, JuanCarlosNiebles†, ZhiweiLiu†,
LeXue†, WeiranYao†, YihaoFeng†, ZeyuanChen†, AkashGokul†,
DevanshArpit†, RanXu†, PhilMui⋄, HuanWang†♦,
CaimingXiong†♦, SilvioSavarese†♦
†SalesforceResearch,USA
⋄CTOOffice,Salesforce,USA
♦CorrespondingAuthors: {huan.wang, cxiong, ssavarese}@salesforce.com

## Abstract

AIagentsleveragingthecapabilitiesofLargeL"
related_documents: []
---

# REX Rapid Exploration Exploitation

<!-- Page 1 -->


### PublishedasaconferencepaperatICLR2024


## Rex: Rapid Exploration And Exploitation


## For Ai Agents

RitheshMurthy†, ShelbyHeinecke†, JuanCarlosNiebles†, ZhiweiLiu†,
LeXue†, WeiranYao†, YihaoFeng†, ZeyuanChen†, AkashGokul†,
DevanshArpit†, RanXu†, PhilMui⋄, HuanWang†♦,
CaimingXiong†♦, SilvioSavarese†♦
†SalesforceResearch,USA
⋄CTOOffice,Salesforce,USA
♦CorrespondingAuthors: {huan.wang, cxiong, ssavarese}@salesforce.com

## Abstract

AIagentsleveragingthecapabilitiesofLargeLanguageModels(LLMs)andReinforcement Learning (RL) techniques have garnered growing attention due to
theircommendableperformanceinautonomouslyexecutingreal-worldtasks. EffectiveexplorationoftheactionspaceisparamountforthesuccessfulaccomplishmentofdiversetasksbytheseAIagents. Inthispaper,weproposeanenhanced
approachforRapidExplorationandeXploitationofactionspaceforLLM-based
AI agents, called REX. Existing LLM-driven agents have inherent limitations,
suchasaheavyrelianceonprecisedescriptionsfordecision-making,andthelack
of a systematic approach to leverage try-and-fail procedures akin to traditional
RL.REXintroducesanadditionallayerofrewardsandintegratesconceptssimilartoUpperConfidenceBound(UCB)scores,leadingtomorerobustandefficient
AI agent performance. This approach has the advantage of enabling the utilization ofoffline behaviorsfrom logs andallowing seamlessintegration with existingfoundationmodelswhileitdoesnotrequireanymodelfine-tuning. Through
comparativeanalysiswithexistingmethodssuchasChain-of-Thought(CoT)and
Reflexion,REX-basedmethodsdemonstratecomparableperformanceand,incertain cases, even surpass the results achieved by these existing techniques. Notably,REX-basedmethodsexhibitremarkablereductionsinexecutiontimewhile
systematically exploring the action space of AI agents, enhancing their practical
applicabilityacrossadiversesetofscenarios.

## 1 Introduction

AIagentsdrivenbyLargeLanguageModels(LLMs)havebecomeaveryactiveresearchtopicrecently. A series of applications, such as AutoGPT (Gravitas, 2023), BabyAGI (Nakajima, 2023),
AgentGPT(age,2023),WebGPT(Nakanoetal.,2022),OpenAGI(Geetal.,2023),MetaGPTHong
etal.(2023),etc. havebeenproposedandwelladoptedinvariousapplicationscenarios. AIagents
builtuponLLMstypicallytransformuserinputsintoastandardizedpromptusingpredefinedtemplates. These templates generally necessitate users to furnish comprehensive descriptions of their
task objectives and plans. This framework allows the language model to comprehend the action
spaceeffectively. InthecontextofLLM-basedAIagents,onecrucialaspectofeffectiveengineeringliesinexplicitlyspecifyingtheprompt. Thisspecificationservesasablueprintforshapingthe
LLM’soutputformatinastructuredmanner. Thisstructuredformatenablesthesystemtoprecisely
extract the actions chosen by the model. Subsequently, based on the agent’s actions, the system
operates similarly to conventional reinforcement learning setup by executing state transitions and
offeringpotentialrewardsorfeedbacktotheagent.
1
4202
naJ
62
]IA.sc[
2v26980.7032:viXra

<!-- Page 2 -->


### PublishedasaconferencepaperatICLR2024

Despite the significant accomplishments of LLM-based AI agents across various application domains, it is evident that they are still in a nascent stage of development (Kaddour et al., 2023;
Zhang et al., 2023), leaving ample room for enhancement. One specific area that warrants meticulous attention and refinement pertains to the incorporation of feedback and rewards to augment
their decision-making prowess. Prominent RL algorithms, such as Policy Gradient (Sutton et al.,
2000),ProximalPolicyOptimizationAlgorithm(PPO)(Schulmanetal.,2017),TrustRegionPolicy
Optimization (TRPO) (Schulman et al., 2015), and Advantage Actor Critic methods (Mnih et al.,
2016) have the capability to update the model parameters of these agents based on the feedback
and rewards emanating from their environments. However, it is important to note that the process
of updating the model parameters in the context of LLM-based AI agents entails substantial data
collection, is time-intensive, and incurs significant costs. Furthermore, given that each unique environmentmaynecessitatedistinctadjustments,fine-tuningLLMsforeveryenvironmentcouldbe
challengingespeciallyforreal-worldapplications. Evenminoralterationsinthetasksetup,theenvironment, or the action space can mandate a complete re-fine-tuning of the entire LLM. In light
ofrecentempiricaldiscoveriesunderscoringtheeffectivenessofIn-ContextLearning(ICL)(Dong
etal.,2023;Daietal.,2023;Akyu¨reketal.,2023;Panetal.,2023),numeroussolutionsbasedonICL
have been put forth. Nevertheless, ICL-based LLM agents confront several limitations: (1) Lack
of systematic rewards incorporation: While they can generate actions based on input data, they
oftenstruggletosystematicallyintegraterewardsignals,hinderingtheirabilitytolearnandoptimize
their performance; (2) Exploration-Exploitation trade-off: LLMs face formidable challenges in
strikingthedelicatebalancebetweenexplorationandexploitation. Achievingoptimalperformance
necessitatesexploringnovelstrategiestounearthpotentiallysuperiorrewardswhilesimultaneously
exploitingexistingknowledgetomaximizeimmediategains;(3)Accuracy&Latency:Incomplex
problem-solvingscenarioswithmultiplesequentialsteps,requiringLLMstogenerateallthesteps
simultaneouslyprovesproblematic. Thisapproachnotonlyresultsininaccuraciesduetotheexpansiveactionspaceandmyriadstatesinvolvedbutalsonecessitatesmoreextensiveinstructionswithin
the prompt. Furthermore, invoking the LLM separately for each intermediate step in the solution
wouldincursignificanttimeoverheads,renderingitatime-consumingendeavor.
To overcome these constraints, this paper introduces an novel approach, Rapid Exploration and
eXploitationforAIagents(REX),thatisdesignedtoempowerLLMswiththecapabilitytoseamlesslyintegraterewardsintotheirmodelsandeffectivelymanagetheexploration-exploitationtradeoff,allwhilesignificantlyacceleratingtheoveralllearningprocess. Insummary,ourcontributions
canbeencapsulatedasfollows: (1)IntegrationofrewardsintothepromptutilizingtheUpper
ConfidenceBound(UCB)framework,therebyenablingasystematicexplorationandexploitation
oftheactionspace;(2)Concurrentpursuitofexplorationandexploitationacrossallstepsofthe
solution,ensuringswiftprogresswithoutcompromisingaccuracy;(3)Systematicallyinfluencing
the logits of LLMs through what we term UCL (UCB applied for Logits of LLM), providing
enhancedcontroloverthegenerationofactions.

## 2 Related Work

AsignificantamountofefforthavebeenmadetoharnessLLMstobuildAIagents. CoT(Weietal.,
2022)encouragesthemodeltosolvethetaskstep-by-stepbeforearrivingatthefinalanswer. ReAct
(Yaoetal.,2023)advocatesutilizingthereasoningandactioncapabilitiesofLLMstopromoteinteractiveengagementwiththeenvironment, exemplifiedbytheutilizationoftheWikipediasearch
API.Ontheotherhand,Reflexion(Shinnetal.,2023)collectsfeedbacki.e. reflectsonit’sdecision
makingandthenimprovises. TheRAPsystem(Haoetal.,2023)transformstheLLMintoadualpurposeentity,servingasbothaworldmodelandareasoningagent. ItintegratesMonteCarloTree
Searchforstrategicexplorationwithintheexpansivedomainofreasoning,guidedbyenvironmental rewards. These techniques has inspired various applications like AutoGPT (Yang et al., 2023),
BabyAGI(Nakajima,2023),WebGPT(Nakanoetal.,2021),HuggingGPT(Shenetal.,2023),Generativeagents(Parketal.,2023),Langchain(Chase,2023),etc.
Nonetheless,themajorityofthesemethodologiesexhibitlimitationsineffectivelyassimilatingenvironmentalfeedbackandoptimizingactionselectionfromtheavailableactionspace. Amongthese
techniques, Reflexion and RAP partially address this issue. Reflexion(+CoT), endeavors to ameliorate this challenge by constructing a rationale for decision-making. However, when confronted
withactionsofconsiderablesimilarity, thereexistsanotableriskofmodelconfusion. Incontrast,
2

<!-- Page 3 -->


### PublishedasaconferencepaperatICLR2024

RAPexhibitsacommendableabilitytojudiciouslyselectactionsthroughtheutilizationofMonte
CarloTreeSearch(MCTS).Nevertheless,itsrelianceontheretrievaloftokenlogitvaluesforreward
calculation precludes the utilization of widely adopted APIs such as ChatGPT. Additionally, RAP
necessitates a significantly higher frequency of queries to the LLM compared to alternative techniques. Conversely, REX offers a distinct advantage by employing UCB scores to unequivocally
identify actions. This approach substantially reduces the likelihood of ambiguity among actions
withintheactionspaceandacceleratesthedecision-makingprocess,asdescribedinsection4.

## 3 Monte Carlo Tree Search

In this section we will discuss Monte Carlo Tree Search (MCTS) which is the backbone of REX.
Inrecentyears,MCTShasgainedsignificantattentionasapowerfulalgorithmfordecision-making
invariousdomains,particularlyingame-playingapplications. MCTShasdemonstratedremarkable
performance in games such as Chess, Go, and Poker, surpassing human expertise in some cases.
However, its applications are not limited to games alone, as MCTS can be adapted to tackle complex decision-making problems in diverse fields. At its core, MCTS is a simulation-based search
algorithmthatleveragesrandomsamplingtoexplorethesearchspaceandmakeinformeddecisions.
ItcombineselementsoftreesearchandMonteCarlosamplingtoiterativelybuildasearchtreeand
estimatethevalueofeachpossibleaction. ThemainstepsoftheMCTSalgorithmcanbesummarized as follows: (1) Selection: Starting from the root of the search tree, the algorithm traverses
downthetreebyselectingactionsthatbalanceexplorationandexploitation. Itusesaselectionpolicy,suchastheUpperConfidenceBound,todeterminethemostpromisingnodes;(2)Expansion:
Oncealeafnodeisreached,thealgorithmexpandsthetreebyrandomlypickinganodefromaset
ofpossiblechildnodes;(3)Simulation: MCTSperformsMonteCarlosimulations(alsoknownas
rollouts)fromnewlyexpandednode. Thesesimulationsplayoutrandomsequencesofactionsuntil
reachingaterminalstate,yieldinganoutcomeorreward;(4)Backpropagation: Afterasimulation
iscompleted,theresultisbackpropagatedupthetree. Thestatisticsofthevisitednodes,suchasthe
numberofvisitsandaccumulatedrewards,areupdatedaccordingly. Thesefourstepsarerepeated
iteratively for a specified number of iterations or until a time limit is reached. As more iterations
areperformed,thesearchtreeevolvesandprovidesincreasinglyaccurateestimatesofthevalueof
differentactions. Figure1showsthepictorialrepresentationofMCTS.

### Repeat N times

Selection Expansion Simulation Backpropagation

### Terminal Node

Figure1: ThefourmajorstepsofMCTSisdepictedintheabovefigure. Thesestepsareexecuted
sequentially‘N’times.
3

<!-- Page 4 -->

PublishedasaconferencepaperatICLR2024
Repeat N times Repeat N times
Selection

### Selection

Expansion Backpropagation Backpropagation

### Expansion

Simulation

### Repeat

d times

### Terminal Node Terminal Node Terminal Node Terminal Node

(a) REX-UCB The Selection, Expansion, and Simu- (b)REX-UCLTheSelectionandExpansionstepsare
lationstepsinMCTSiscombinedtoformonesingle amalgamated into a single step; however, due to the
stepinREX. adjustment of logits, all the steps are not simultaneouslygenerated; instead, eachstepisgeneratedindividually.

### Figure2: REX


### LLM Action LLM Action

Prompt Environment UCL Prompt Environment
scores

### Memory Reward Memory Reward

(a) REX-UCB: Action denotes the solution pro- (b) REX-UCL: Action denotes the solution producedbytheLLM,whileEnvironmentrepresents ducedbytheLLM,whileEnvironmentrepresents
the context within which the task is defined. The the context within which the task is defined. The
determination and subsequent updating of the Re- determination and subsequent updating of the Reward within the Memory (UCB table) are con- ward within the Memory (UCL table) are contintingent upon the task’s characteristics. Following gentuponthetask’scharacteristics. Followingthis,
this, the UCB scores are translated into HIGH or theUCLscoresaredirectlyprovidedtoLLMtoin-
LOWvaluesandaresubsequentlyintegratedintothe fluence the tokens associated with the desired ac-
Prompt(Moredetailsonpromptdesignisdiscussed tionsequence(MoredetailsonpromptdesignisdisinAppendixB.1) cussedinAppendixB.2)

### Figure3: REXFlowchart


## 4 Proposed Methodology


## 1 Rapidexplorationandexploitation: Rex

ThemajordrawbacksofvanillaMCTSalgorithmisthatit’scomputationallyexpensiveandfailsto
arriveatthesolutionquickly. ToaddresstheseissuesweproposeanacceleratedversionofMCTS,
calledREX,byusingLLMandIn-ContextLearning(ICL).LLM’sabilitytolearnfromthecontext
and to generate multiple actions at once makes it an ideal candidate for this methodology. In our
setup,LLMistheagent. Figure2ashowsthepictorialrepresentationofthisalgorithmandFigure
3adepictstheflowchart. ThefourmajorstepsofMCTScanbecompressedtotwostepsinREXas
follows:
4

<!-- Page 5 -->


### PublishedasaconferencepaperatICLR2024


## Selection+Expansion+Simulation: Ratherthanprogressingstepbystepfromtheinitial state, taking actions, and moving to the next state until reaching a terminal state, the

newalgorithmconsidersallpossibleactionssimultaneouslyforeachstepinthesolution.
Itpredictstheentiresolution,includingthesequenceofintermediatestepsandthefinalanswer,inonego. Thisapproachremovestheneedforexplicitstatetransitionsandmultiple,
independentpredictions.

## Backpropagation:Basedonthecorrectnessofthefinalsolution,thefeedbacki.e.rewards

arepropagatedbacktoalltheintermediatestepsinthesolution. Thisnewinformationis
usedintheprompttoproposebettersolution.
MCTS employs the UCB as its selection policy. Correspondingly, the initial iteration of REX,
referred to as REX-UCB, also adopts UCB as its selection policy. Depending on the choice of
selection policy, various iterations of REX can be formulated. In this research paper, we have
conductedexperimentswithtwoadditionalvariants,specificallydenotedasREX-RandREX-UCL,
andtheirdetaileddescriptionscanbefoundinSections4.3and4.4,respectively.

## 2 Algorithm1: Rex-Ucb

REX-UCBisthedefaultversionofREX,whereUCBisusedasselectionpolicy.

## Approach: ThisapproachcombinestheprinciplesofMCTS,CoT,andICL.Inoursetup,

we define the term question or problem as the task we aim to solve. We use the
termsolutiontoencompassboththesequence-of-intermediate-stepsand
the final-answer. The final-answer represents the ultimate response generated
by the model. On the other hand, the sequence-of-intermediate-steps refers
to the steps that lead to the final-answer. We use action and step interchangeably;ineithercaseitreferstoasinglestepinsequence-of-intermediate-steps.
we refer state to represent the step-index (e.g. step-1, step-2, etc.) in
sequence-of-intermediate-steps.

## Reward Assignment: After each pass, the language model’s solution is evaluated.

If the final-answer is correct, a reward of +1 is assigned to each step in
sequence-of-intermediate-stepsinthesolution,indicatinga‘Highrewardaction.’ Conversely, if the final-answer is incorrect, a reward of 0 is assigned to each
stepinsequence-of-intermediate-stepsinthesolution,inthatpass,representing a ‘Low reward action.’ Based on the nature of the task and environment, we design
rewardlearningfromenvironmentorLLMasfollows:
• RewardLearningfromEnvironment: InBlocksworlddataset(discussedinsection
5.2), where the expected answer is already known, a reward of +1 is assigned if the
final-answermatchesthetargetblockconfiguration;otherwise,therewardis0
• Reward Learning from LLM: In the GSM8K dataset (discussed in section 5.3), where the expected answer is not known, the final answer’s
correctness is determined by the LLM i.e we provide the question,
sequence-of-intermediate-steps,andfinal-answerfromthecurrent
pass and query the LLM asking if the solution is correct? If yes, a reward of +1 is
assignedelserewardis0

## UCBScoring:Ineachpass,theUCBscoreiscalculatedforeachactiontakenbytheagent.

Thisscoringmechanismencouragesthelanguagemodeltoexploredifferentactionswhile
favoringthosewithhigherpotentialrewards. Intheeq. 1,srepresentsthestate,anda
representstheactiontakenats. N(s)isthenumberoftimesagenthasproducedsin
it’s solution and N(s, a) is the number of times the agent took an action a from a
states. Cisconstanttobalancetheexplorationvs. exploitationtradeoff. Qˆ(s, a)isthe
cumulativerewardfortakingaactionaatstates.
(cid:114)
lnN(s)
UCB(s, a)=Qˆ(s, a)+C∗ (1)

### N(s, a)


## Reinforcement Learning and Policy Optimization: The UCB scores are mapped to

HIGHorLOWtokensandincorporatedintotheprompts,thisapproachleveragesreinforcementlearningtechniquestoencouragethelanguagemodeltogeneratemoreHIGHreward
5

<!-- Page 6 -->


### PublishedasaconferencepaperatICLR2024

actionsandavoidLOWrewardactions. Theagentlearnsfromtherewardsassociatedwith
eachpassandadjustsitspolicyaccordingly. Thegoalistooptimizetheagent’sdecisionmakingprocessovertime,leadingtoimprovedperformanceandmoreaccuratesolutions.

## SelectionofOptimalActionSequence: Afteraspecifiednumberofpasses,thesequence

ofactionsthatyieldsthehighestcumulativerewardateachstageisselectedasthefinalsolution. Thisselectionprocessensuresthatthelanguagemodel’sdecision-makingisguided
bythereinforcementlearningframeworkandprioritizesactionsthatmaximizethecumulativereward,thuspromotingthegenerationofaccuratesolutions.
MoredetailsonthestructureofthepromptandasimpleexampleisprovidedinAppendixB&C.

### Algorithm1REX-UCB

Require: Problem P, final-answer Z, sequence-of-intermediate-steps H, action space A, state
space S, action space for state s denoted as A(s) where s ∈ S, number of passes N, reward
function Q: S x A → R, upper confidence bound function U: S x A → R, expected reward
functionE: S xA→{HIGH,LOW},expected-answerX,rewardR
1: AGENT←Agent() /*initializeanagent*/
2: fori←1toN do
3: U(s,a)←CALCULATEUCB(s,a), ∀(s,a)∈Q
4: forsinS do
5: ifU(s,a)==max U(s,a) thenE(s,a)←HIGH elseE(s,a)←LOW /*action
aϵA(s)
withthehighestUCBscoreismappedtoHIGHandotheractionsaremappedtoLOW */
6: endfor
7: H,Z ←AGENT.SOLVE(P,E) /*agentpredictsintermediatestepsandfinalanswer*/
8: ifX isavailablethen
9: ifZ ==X thenR←+1 elseR←0
10: else
11: valid answer←AGENT.VALIDATEACTION(P,H,Z)
12: ifvalid answerisTRUE thenR←+1 elseR←0
13: endif
14: Q(s,a)←Q(s,a)+R, ∀(s,a)∈H
15: endfor

## 3 Algorithm2: Rex-R

REX-RcloselyresemblesREX-UCB,differonlyinthemethodusedtodeterminetheHIGH/LOW
expected rewards. While REX-UCB relies on UCB scores, REX-R utilizes simple rewards of +1
and0forthispurpose. ThisisequivalenttosettingC=0ineq. 1. Asaresult,theexplorationaspect
becomesvoid;however,duetoourmappingofscorestoHIGH/LOWandourencouragementforthe
modeltoexclusivelypursueHIGHactions,acertaindegreeofexplorationcontinuestobeineffect.

## 4 Algorithm3: Rex-Ucl

In this section we propose a novel paradigm to systematically modify the logits of LLMs called
UCL(UCBappliedforLogitsofLLM).ThepictorialrepresentationisshowninFigure2bandthe
respectiveflowchartisshown infigure3b. This approach isavariantofREX,featuring adistinct
modification in its methodology. While REX-UCB employs In-Context Learning, this variation
involvesadjustingthelogitsofactionstoinfluencethedecision-makingprocessoftheLLMinstead.
Incertaincases,languagemodelsmaygeneratealternativeactionsinsteadofstrictlyfollowingthe
instructionsinprompt. Toaddressthis, UCLadjuststhelogitscorrespondingtotokensassociated
withtheactions,ensuringthattheintendedactionsareconsistentlychosen.

## Approach: This approach builds upon the foundation of the REX model by introducing

novelscoringmechanismsusingUCBcalledUCL.Inordertomodifythelogitsforagiven
query,weadheretoaone-timealterationconstraintandthereforeexecuteaqueryforeach
stepinthesolution. Thisapproachdiffersfromthepreviousmethod,whereasinglecall
wasmadetoretrievetheentiresolution,aswenowmakeaseparatecallforeachindividual
6

<!-- Page 7 -->


### PublishedasaconferencepaperatICLR2024

state. While this approach involves multiple queries to LLMs, akin to the standard
MCTS method, it still demonstrates greater speed in comparison to other MCTS-based
agents,suchasRAP.

## Reward Assignment: Reward assignment for REX-UCL is exactly same as the reward

assignmentforREX-UCB.

## UCLScoring: TheUCLscoreiscalculatedbasedoneq. 2. Thisscoreisusedtoupdate

theloglikelihoodsoftokenscorrespondingtoactionsforthecurrentstateinthelanguage
model. BymanipulatingthetokenlogitswithUCLscoresduringgeneration,theagentis
compelledtoexecutetheactionyieldingthehighestreward. TheconstantsB&Kcontrol
theextenttowhichlogitsoftheLLMareoffset.

### UCB(s, a)

UCL(s, a)=B∗ln (2)

## K


## SelectionofOptimalActionSequence: Similartopreviousapproaches,afteraspecified

numberofpasses,thesequenceofactionsthatyieldsthehighestcumulativerewardateach
stageisselectedasthefinalsolution.

### Algorithm3REX-UCL

Require: ProblemP,next-actionaˆ,actionspaceA,statespaceS,actionspaceforstatesdenoted
asA(s)wheres∈S,numberofpassesN,rewardfunctionQ: S xA→R,upperconfidence
bound function U: S x A → R, UCL function L: S x A → R, expected-answer X, state at
depthddenotedasS ,andletDbethemaximumpermissibledepth,constantB,constantK
d
1: AGENT←Agent() /*initializeanagent*/
2: fori←1toN do
3: S d ←P
4: trajectory←[]
5: forj←1toDdo
6: U(S d ,a)←CALCULATEUCB(S d ,a), ∀a∈A(S d )
7: L(S d ,a)←B∗lnU(S K d,a)
8: forainA(S d )do
9: T ←GETTOKENIDS(a) /*gettokenidsforeachtokenintheaction*/
10: logit bias(t)←L(S d ,a), ∀t∈T
11: endfor
12: aˆ←AGENT.SOLVE(P,logit bias)
13: ifX isavailablethen
14: ifaˆ==X thenR←+1 elseR←0
15: else
16: valid answer←AGENT.VALIDATEACTION(P,S d ,aˆ)
17: ifvalid answerisTRUE thenR←+1 elseR←0
18: endif
19: trajectory←trajectory+(S d ,aˆ) /*appendstate&actiontotrajectory*/
20: Q(s,a)←Q(s,a)+R, ∀(s,a)∈trajectory
21: Update: S d ←S d +aˆ /*appendactiontostate*/
22: endfor
23: endfor

## 5 Experiments & Discussion


## 1 Baseline


## CoT:TheMulti-passCoT(Weietal.,2022)buildsupontheideathatinvolvesdesigning

prompts that include a series of step-by-step examples, ultimately leading to the final solution. A successful outcome is achieved if at least one of these queries yields a correct
answer.

## Reflexion: Reflexion(Shinnetal.,2023)frameworkleveragesLLMstostrategicallyplan

andexecuteatask. Reflectsonit’sdecisionmakingandthenimprovises.
7

<!-- Page 8 -->


### PublishedasaconferencepaperatICLR2024


## RAP:RAP(Haoetal.,2023)employsaconventionalMonteCarloTreeSearchapproach

toaddresstheassignedtask,reutilizingtheLLMinthedualrolesofaworldmodelanda
reasoningagent. Ourprimaryfocushereisthecomparativeanalysisoftimecomplexity.
ForourexperimentswehaveusedBlocksworlddatasetandGSM8Kdataset. Moredetailsonthese
twodatasetsarepresentedinthefollowingsections.

## 2 Blocksworld

TheBlocksworlddataset(Valmeekametal.,2023)representsaplanningproblemthatinvolvesthe
arrangement of blocks with varying colors in a predetermined configuration. Each instance of the
datasetprovidesaninitialblockconfigurationandthedesiredfinalconfiguration. Theterm‘block
configuration’referstothespecificarrangementoftheblocks,whereeachblockcanbepositioned
eitherontopofanotherblock, onatablesurface, orheldinhand, butnotalloptionsareavailable
simultaneously. Theblocksareuniquelyidentifiedbytheirrespectivecolors,suchasred,blue,and
soon. TheBlocksworlddatasetisdividedinto3subcategories(2,4,6steps)basedonthenumber
ofstepsrequiredtotransformtheblockconfigurationfrominitialtofinal.
To transform the block configuration from initial to final, the model is expected to propose sequence of steps/actions. In blocksworld, there are only four major actions - STACK, UNSTACK,
PICK and PUT. Table 1 illustrates the performance of the methodologies proposed in this study.
Eachexperimentisrunfor10iterations/passesandchatGPT(model: gpt-3.5-turbo)isusedforall
theexperiments. Thebestscoreisinboldandthesecondbestscoreisunderlined. Inordertoassess
theefficacyoftheproposedalgorithmmoreaccurately,wecomparetheperformanceofthedifferentalgorithmswithtemperature(T)ofLLMsissetto0.0. Settingahighertemperaturevaluecan
obscuretheimpactoftheproposedapproaches,makingitdifficulttoevaluatetheireffectiveness.

## 3 Gsm8K

The GSM8K dataset (Cobbe et al., 2021) comprises a collection of 8.5k grade school math word
problems of exceptional quality. For our experiment we have used GSM8K test set which has
roughly 1.3k samples. Each problem within this dataset typically requires a solution involving a
sequence of elementary calculations, utilizing fundamental arithmetic operations such as addition,
subtraction, multiplication, and division (+, -, ×, ÷). The number of steps required to solve each
problemfallswithintherangeof2to8steps. Theperformanceoftheproposedmethodologiesare
presentedinTable1. Temperaturesissetto0.0forallthemethods.
Table1: Accuracy&Timecomplexityofvariousmodels
Blocksworld GSM8K-test

### Time

Architecture 2-step 4-step 6-step 2-to-8-steps
complexity
(size=30) (size=56) (size=114) (size=1319)

### CoT 40% 17.85% 8.77% 80.81% n

Reflexion(+CoT) 41.67% 41.96% 29.82% 88.85% 3*n

### RAP - - - - n*m*d

REX-R 53.33% 37.5% 14.91% 81.34% n

### REX-UCB 80% 39.28% 25.43% 82.03% n


### REX-UCL 60% 44.64% 20.17% 90.44% n*d


## 4 Rex:Accuracy,Speed,Andlimitations

Accuracy: The accuracy of various methods presented in Table 1 unequivocally demonstrates
REX’s superior performance in comparison to alternative techniques such as CoT and Reflexion,
particularly on the Blocksworld (2 & 4 step) and GSM8K datasets. While it is noteworthy that
no single REX variation consistently outperforms every algorithm across all datasets, the average
performance of REX consistently exhibits highly promising results. RAP is not compatible with
OpenAI APIs; therefore, we have exclusively focused on evaluating the time complexity of RAP,
8

<!-- Page 9 -->

PublishedasaconferencepaperatICLR2024
withoutdelvingintoitsaccuracy.
Time complexity: In this context, we define time complexity as the count of queries made
to the LLM in order to accomplish a given task. We introduce three pivotal variables: ‘n,’ which
signifiesthenumberofiterationsorpasses;‘d,’denotingthedepthlimitorthemaximumcountof
intermediatesteps;and‘m,’indicatingthequantityofpotentialactionsgeneratedateachstate. The
last column in Table 1 presents the time complexities of various methods. Our approach not only
excelsincomputationalefficiencybutalsooffersheightenedflexibility. Notably,theintegrationof
REX enhancements is viable within any LLM, including widely adopted APIs such as OpenAI,
evenincaseswhereaccesstotheunderlyinglogitsisrestricted. ThisadaptabilityunderscoresREX
asanexceptionallyversatileandpragmaticchoicesuitableforawidespectrumofapplications. In
starkcontrast,RAPrequiresaccesstologitvaluesfortokensassociatedwithallactionstoestimate
rewards,renderingitlesspracticalforreal-worldscenarios.
Limitations: Numerous uncharted territories in this field await exploration, including but
not limited to context length limitations, in-context learning intricacies, and the scope of generalization capabilities. Herein, we elucidate key facets of REX’s performance and potential
limitations: (1)REXimpressivelydemonstratesformidableaccuracyandefficienttimecomplexity.
However, its efficacy is tethered to context length, owing to its reliance on In-Context Learning
(ICL);(2)EffectiveutilizationofREXnecessitatesaskillfulreconfigurationoftheprompt,tailored
meticulouslyforeachspecifictaskdomain;(3)TheC,B&KparametersinUCBandUCLrequire
domain-specificadjustments. Furthermore,harnessingthepowerofREX-UCLrequiresanuanced
manipulationofthelogitvalueswithintheLLMs.

## 5 Rex:Ucbforeffectiveexplorationandexploitation

From Table 1 it is clear that UCB based feedback outperforms simple reward R (UCB with C=0)
based feedback in Planning (Blocksworld) and Mathematical Reasoning (GSM8K) datasets. An
exampleproblemsolvedbyREX-RandREX-UCBispresentedintheAppendixC.
The examination detailed in Table 2, which delineates the mean count of distinct actions at each
intermediate solution stage, elucidates that the REX-UCB strategy fosters a more pronounced inclination for action exploration at each juncture when compared with the REX-R approach. This
strategicprioritizationofexplorationwithintheactionspaceispositedtoenhancetheagent’sability
touncoverinnovativeproblem-solvingpathways,consequentlyaugmentingthesuccessrate. REX,
throughitsincorporationofUCB-basedfeedback,strikesadelicateequilibriumbetweenexploration
andexploitation,ultimatelyyieldingenhancedperformance.
Table2: Avg. numberofuniqueactionsforeachintermediatestepafter10iterations

### Blocksworld

Architecture 2-step 4-step 6-step
(size=30) (size=56) (size=114)

## Rex-R 1.63 2.32 2.34


## Rex-Ucb 3.61 3.89 4.24


## 6 Conclusion

In conclusion, this paper presented REX and it’s variations, aimed at enhancing the performance
of AI agents. These techniques effectively strike a balance between exploration and exploitation,
which is crucial for successful problem-solving. Through extensive evaluations on Planning and
MathematicalReasoningdatasets, wehavedemonstratedthesuperiorityofREX.Thestrengthsof
REX lie in its simplicity, efficiency, flexibility, and speed, making it a compelling choice for realworld applications. By incorporating UCB-based formula and strategically allowing the model to
predictmultiplestepsatonce,ourtechniquesprovidearobustframeworkforoptimizingAIagents,
openingdoorsforadvancementsinvariousdomainsthatrequirelarge-scaleactionmodeling.
9

<!-- Page 10 -->

PublishedasaconferencepaperatICLR2024

## References

Agentgpt. https://github.com/reworkd/AgentGPT,2023.
Ekin Akyu¨rek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning
algorithmisin-contextlearning? investigationswithlinearmodels,2023.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,
ArielHerbert-Voss,GretchenKrueger,TomHenighan,RewonChild,AdityaRamesh,DanielM.
Ziegler,JeffreyWu,ClemensWinter,ChristopherHesse,MarkChen,EricSigler,MateuszLitwin,
Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
IlyaSutskever,andDarioAmodei. Languagemodelsarefew-shotlearners,2020.
HarrisonChase. Langchain. https://github.com/hwchase17/langchain,2023.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John
Schulman. Trainingverifierstosolvemathwordproblems,2021.
DamaiDai,YutaoSun,LiDong,YaruHao,ShumingMa,ZhifangSui,andFuruWei. Whycangpt
learnin-context? languagemodelsimplicitlyperformgradientdescentasmeta-optimizers,2023.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectionaltransformersforlanguageunderstanding,2019.
QingxiuDong, LeiLi, DamaiDai, CeZheng, ZhiyongWu, BaobaoChang, XuSun, JingjingXu,
LeiLi,andZhifangSui. Asurveyonin-contextlearning,2023.
Yingqiang Ge, Wenyue Hua, Kai Mei, Jianchao Ji, Juntao Tan, Shuyuan Xu, Zelong Li, and
YongfengZhang. Openagi: Whenllmmeetsdomainexperts. arXiv,2023.
Significant Gravitas. Autogpt. https://github.com/Significant-Gravitas/
Auto-GPT,2023.
ShiboHao,YiGu,HaodiMa,JoshuaJiahuaHong,ZhenWang,DaisyZheWang,andZhitingHu.
Reasoningwithlanguagemodelisplanningwithworldmodel,2023.
SiruiHong, XiawuZheng, JonathanChen, YuhengCheng, JinlinWang, CeyaoZhang, ZiliWang,
StevenKaShingYau,ZijuanLin,LiyangZhou,ChenyuRan,LingfengXiao,andChenglinWu.
Metagpt: Metaprogrammingformulti-agentcollaborativeframework,2023.
Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert
McHardy. Challengesandapplicationsoflargelanguagemodels,2023.
VolodymyrMnih,Adria`Puigdome`nechBadia,MehdiMirza,AlexGraves,TimothyP.Lillicrap,Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. CoRR,abs/1602.01783,2016.
YoheiNakajima. Babyagi. https://github.com/yoheinakajima/babyagi,2023.
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, ChristopherHesse,ShantanuJain,VineetKosaraju,WilliamSaunders,etal. Webgpt: Browser-assisted
question-answeringwithhumanfeedback. arXivpreprintarXiv:2112.09332,2021.
ReiichiroNakano,JacobHilton,SuchirBalaji,JeffWu,LongOuyang,ChristinaKim,Christopher
Hesse,ShantanuJain,VineetKosaraju,WilliamSaunders,XuJiang,KarlCobbe,TynaEloundou,
GretchenKrueger,KevinButton,MatthewKnight,BenjaminChess,andJohnSchulman.Webgpt:
Browser-assistedquestion-answeringwithhumanfeedback,2022.
OpenAI. Gpt-4technicalreport,2023.
JanePan,TianyuGao,HowardChen,andDanqiChen.Whatin-contextlearning”learns”in-context:
Disentanglingtaskrecognitionandtasklearning,2023.
10

<!-- Page 11 -->


### PublishedasaconferencepaperatICLR2024

Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and
MichaelSBernstein. Generativeagents: Interactivesimulacraofhumanbehavior. arXivpreprint
arXiv:2304.03442,2023.
JohnSchulman,SergeyLevine,PhilippMoritz,MichaelI.Jordan,andPieterAbbeel. Trustregion
policyoptimization. CoRR,abs/1502.05477,2015.
JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov. Proximalpolicy
optimizationalgorithms. CoRR,abs/1707.06347,2017.
Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang.
Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint
arXiv:2303.17580,2023.
Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and
Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. arXiv preprint
arXiv:2303.11366,2023.
R.S.Sutton, D.Mcallester, S.Singh, andY.Mansour. Policygradientmethodsforreinforcement
learningwithfunctionapproximation.InAdvancesinNeuralInformationProcessingSystems12,
volume12,pp.1057–1063.MITPress,2000.
Karthik Valmeekam, Sarath Sreedharan, Matthew Marquez, Alberto Olmo, and Subbarao Kambhampati. Ontheplanningabilitiesoflargelanguagemodels(acriticalinvestigationwithaproposedbenchmark),2023.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
LukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed,2017.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi,
Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language
models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), AdvancesinNeuralInformationProcessingSystems,2022.URLhttps://openreview.net/
forum?id=_VjQlMeSB_J.
Hui Yang, Sifu Yue, and Yunzhong He. Auto-gpt for online decision making: Benchmarks and
additionalopinions,2023.
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.
React: Synergizingreasoningandactinginlanguagemodels,2023.
DawenZhang, PamelaFinckenberg-Broman, ThongHoang, ShidongPan, ZhenchangXing, Mark
Staples,andXiweiXu. Righttobeforgottenintheeraoflargelanguagemodels: Implications,
challenges,andsolutions,2023.
11

<!-- Page 12 -->

PublishedasaconferencepaperatICLR2024

### Appendix


## A Synthesizing Large Language Models With Reinforcement


## Learning For Enhanced Ai Agents

Ideally, an AI agent is anticipated to autonomously execute designated tasks. To achieve this, the
AIagentnecessitatesunfetteredaccesstoanassemblageofinformation, factualdetails, rules, and
pertinent data. Furthermore, the agent must possess the faculty to engage in rational decisionmakingprocessesandsubsequentlyeffectuatetaskcompletion. IntherealmofadvancedAIagents,
an additional facet is the imperative capacity to engage with the surroundings, assimilate insights
from errors, and progressively refine their acumen in the sphere of decision making. To facilitate
the realization of these functionalities, the amalgamation of Large Language Models (LLMs) and
ReinforcementLearning(RL)emergesasahighlyauspicioustrajectory.

## A.1 Whyllmsareagoodchoice?

LLMs are deep neural networks that have been trained on vast amounts of text data to predict the
next word in a sequence (Vaswani et al., 2017; Brown et al., 2020; Devlin et al., 2019; OpenAI,
2023). Theylearnthestatisticalpatternsandsemanticrelationshipsbetweenwords,allowingthem
togeneratecontextuallyrelevanttext. LLMsaregoodatlogicalreasoningandDecisionmakingto
someextent.

## A.2 Somemajorlimitations

Large Language Models (LLMs) possess several limitations that stem from their lack of physical
embodiment and sensory perception. LLMs heavily rely on precise instructions, grappling with
ambiguity and misinterpreting vague commands, potentially yielding erroneous outcomes. Their
knowledge, drawnfromvasttextualdata, oftenlackscommonsensereasoningandcontextualunderstandingbeyondexplicitmentions.Thisresultsintechnicallyaccurateyetunexpectedresponses.
Moreover,theystruggletoprovidecausalreasoning,hinderingthecapacitytoofferjustificationsor
detailed explanations for decisions. Lastly, LLMs struggle to generalize to novel situations, functioningwellinfamiliartasksbutproducingunreliableoutcomeswhenexposedtounchartedinputs.

## A.3 Reinforcementlearning(Toovercometheselimitations)

ReinforcementLearning(RL)empowersLanguageModelagents(LLMs)tolearnoptimaldecisionmakingbyinteractingwithanenvironment,receivinginputtext,andgeneratingresponsesoractions.
Thisprocessinvolvesbalancingexplorationandexploitation,astheLLMagentgraduallydiscovers
effectivestrategiesthroughexperimentationandthenleveragesthatknowledgetomaximizerewards.
Key to RL’s success is defining a reward signal that guides learning; for LLMs, reward shaping
provides feedback on response quality, encouraging desired behavior while penalizing errors. RL
also facilitates training in simulated environments, enabling LLMs to acquire skills applicable to
real-worldscenarios. AtthecoreofRListheoptimizationoftheagent’spolicy—typicallyaneural
network—dictatingactionsinresponsetoinputtext. OnesuchprominentalgorithmisMonteCarlo
TreeSearch(MCTS).
An example instance from blocksworld dataset(2-step) is presented in Appendix. Figure 4 shows
thesolutionprovidedbyREX-RandFigure5showsthesolutionprovidedbyREX-UCB.Thisis
an example which shows how REX-UCB was able to solve a problem which REX-R wasn’t able
to solve. Clearly, encouraging the model to pick a HIGH reward action based on UCB score has
helpedthemodeltosolvethegivenproblem.
The REX-R approach involves providing the model with specific information and instructions to
solve a problem during each pass. The prompt includes details about the problem, instructions,
action space, and three examples of solved problems (3-shot examples). Additionally, feedback
from previous passes is given to the model in each pass. In the Simple-Reward(R) setting, if the
actionsequenceleadstothecorrectanswer,a‘HIGH’rewardisassignedtoeachactionorstepinthe
solution. Conversely,iftheactionsequencedoesnotleadtothecorrectanswer,a‘LOW’rewardis
12

<!-- Page 13 -->


### PublishedasaconferencepaperatICLR2024

assigned. Thisrewardinformationisincludedinthepromptforthesubsequentpass. It’simportant
tonotethatunlesstheactionsequenceresultsinacorrectanswer,noneoftheactionsorstepswill
receivea‘HIGH’reward. Consequently,themodelisencouragedtoproposenewactionsinorderto
improveitsperformance.
The REX-UCB solution is depicted in Figure 5. Similar to the REX-R approach, the REX-UCB
prompt incorporates comprehensive information about the problem, including problem details, instructions, action space, and three examples of solved problems (referred to as 3-shot examples).
Moreover,feedbackfrompreviouspassesisincorporatedintothemodelateachpass. Wecompute
theUpperConfidenceBound(UCB)scoreforeachactionwithinthesolutionduringeachpass.Subsequently, we assign the action associated with the highest UCB score as a ‘HIGH’ reward, while
the remaining actions are designated as ‘LOW’ reward. This UCB scoring mechanism ensures an
effective selection process, by striking a good balance between exploration and exploitation, for
identifying the most promising action to be executed, optimizing the model’s performance within
theREX-UCBframework.

## B Prompt Structure


## B.1 Rex-Ucb

The prompt for LLM has three major parts: (1) Task Instruction The set of specific directions,
guidelines, orinformationprovidedtoLLMtocompleteaparticulartaskorachieveaspecificobjective. Theseinstructionstypicallyoutlinewhatneedstobedone,howitshouldbedone,andany
relevantdetailsorrequirementstoensurethesuccessfulexecutionofthetask. Taskinstructionscan
varywidelydependingonthecontextandnatureofthetask,andtheyplayacrucialroleinguiding
andfacilitatingtasksorassignments;(2)Few-ShotexamplesThemodelisexpectedtogeneralize
from this limited set of examples to perform tasks or make predictions on new, unseen data; (3)
State-Action-FeedbackExpectedreward,basedonUCBscores,foreachactionineverystate.

## Llm


### Prompt


### Task Instruction Few-Shot Examples State-Action-Feedback

You are a helpful agent capable of Example 1: Actions and Expected Rewards:
solving logical problems. You are Initial Block Configuration: [...] Step 1:
required to take a sequence of action Final Block Configuration [....] - Stack blue on orange has HIGH
to transform Initial BLock Actions: [...] reward
Configuration to Final Block - Untack Green from Yellow has

### Configuration [....] Example 2: LOW reward

Possible action you can take: [....] Initial Block Configuration: [...]
Rules: [...] Final Block Configuration [....] Step 2:

### Actions: [...] [...]

Figure4: PrmptforREX-UCB

## B.2 Rex-Ucl

TheTaskInstructionandFew-ShotExamplescomponentsarethesameasinREX-UCB.State-
Action-FeedbackcomponentisremovedandinsteadUCLscoresareprovidedtoLLMtoadjustthe
logitvalues.
13

<!-- Page 14 -->

PublishedasaconferencepaperatICLR2024
UCL Scores LLM

### Prompt


### Task Instruction Few-Shot Examples


### You are a helpful agent capable of Example 1:

solving logical problems. You are Initial Block Configuration: [...]
required to take a sequence of action Final Block Configuration [....]
to transform Initial BLock Actions: [...]

### Configuration to Final Block


### Configuration [....] Example 2:

Possible action you can take: [....] Initial Block Configuration: [...]
Rules: [...] Final Block Configuration [....]

### Actions: [...]


### Figure5: REX-UCL


### Figure6: SampleproblemfromBlocksworld


## C Sample Solution Provided By Rex-R And Rex-Ucb

Anexamplefromthe2-stepblockconfigurationisshowninFigure6. InitialBlockConfiguration
represents how the colored block are are arranged initially and Final Block Configuration shows
theexpectedblockarrangement. GroundTruthisthesequenceofactionsthatneedstobetakento
transformBlockConfigurationfromInitialtoFinal. Figure7showsthesolutionprovidedbyREX-
RandFigure8showsthesolutionprovidedbyREX-UCB.After10passes,REX-Rwasunableto
solvethetask. Ontheotherhand,REX-UCLsolvedtheproblemin7thpass.
14

<!-- Page 15 -->

PublishedasaconferencepaperatICLR2024

### Figure7: REX-RbasedSolution

Figure8: REX-UCBbasedSolution
15

<!-- Page 16 -->


### PublishedasaconferencepaperatICLR2024

D MORE DETAILS ON THE ‘AGENT.XXXX()’ METHODS USED IN THE

## Algorithms


## CALCULATE UCB(): This method simply calculates the UCB score for every (state,

action)usingtheformulapresentedineq. 1
• Input: states,actiona
• Output: ascalarvalue

## AGENT.SOLVE(): This method represents the inference call made to the LLM. In our

case, this is exactly same as OAI’s chat.completion() API. It takes in the prompt and the
expected-reward-valuesforeach(state,action).
• Input: prompt/problemwithinstructionsP,expectedrewardfunctionE
• Output: Itproducestwovaluesinatuple: H,whichrepresentsasequenceofintermediatesteps,andZ,whichrepresentsthefinalanswer.

## AGENT.VALIDATE ACTION(): Thismethodrepresentstheinferencecallmadetothe

LLM. In our case, this is exactly same as OAI’s chat.completion() API. It takes in the
prompt, the sequence-of-intermediate-steps, and the final answer proposed by the model
i.e. outputofAGENT.SOLVE()
• Input: prompt/problemwithinstructionsP,sequenceofintermediatestepsH,final
answerZ
• Output: ItreturnsaBooleanvaluesi.eTrueorFalse
16

## Tables

**Table (Page 4):**

|  | Repeat N times Selection Expansion Backp Simulation Terminal Node Ter |  | Selection ropagation Expansion Repeat d times minal Node Terminal Node | Repeat N times Backpropagation Terminal Node |  |
|---|---|---|---|---|---|
| (a) REX-UC lationstepsi stepinREX. LLM Prom Memo (a) REX-UC ducedbythe the context w determination ward within tingent upon this, the UC LOWvalues | (a) REX-UC | B The Selection, Expans | ion, and Simu- (b)REX-UCLTheS | electionandExp | ansionstepsare |
|  |  | nMCTSiscombinedtoformonesingle amalgamated into a single step; howe adjustment of logits, all the steps are ouslygenerated; instead, eachstepis vidually. Figure2: REX Action LLM pt Environment UCL Prompt En scores ry Reward Memory B: Action denotes the solution pro- (b) REX-UCL: Action denotes th LLM,whileEnvironmentrepresents ducedbytheLLM,whileEnviron ithin which the task is defined. The the context within which the task i and subsequent updating of the Re- determination and subsequent upda the Memory (UCB table) are con- ward within the Memory (UCL ta the task’s characteristics. Following gentuponthetask’scharacteristics. B scores are translated into HIGH or theUCLscoresaredirectlyprovided andaresubsequentlyintegratedintothe fluence the tokens associated with |  |  |  |


**Table (Page 4):**

| Selection |
|---|
| Expansion |
|  |
