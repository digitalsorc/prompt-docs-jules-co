---
title: "Model Versioning Management"
original_file: "./Model_Versioning_Management.pdf"
document_type: "research"
conversion_date: "2025-11-29"
topics: ["llm", "rag", "fine-tuning", "evaluation", "multimodal"]
keywords: ["images", "fake", "real", "cid", "ave", "generated", "training", "wang", "nerf", "neural"]
summary: "<!-- Page 1 -->


### Towards More Accurate Fake Detection on Images

Generated from Advanced Generative and Neural Rendering Models
ChengdongDong1,2 VijayakumarBhagavatula1 ZhenyuZhou2 AjayKumar2
1DepartmentofElectricalandComputerEngineering,CarnegieMellonUniversity
2DepartmentofDataScienceandArtificialIntelligence,TheHongKongPolytechnicUniversity
{chengdong.dong,zhenyucs.zhou}@connect.polyu.hk,kumar@ece.cmu.edu,ajay.kumar@polyu.edu.hk
Abstract nologies such as Neural Radiance Fields (NeRF) [2,"
related_documents: []
---

# Model Versioning Management

<!-- Page 1 -->


### Towards More Accurate Fake Detection on Images

Generated from Advanced Generative and Neural Rendering Models
ChengdongDong1,2 VijayakumarBhagavatula1 ZhenyuZhou2 AjayKumar2
1DepartmentofElectricalandComputerEngineering,CarnegieMellonUniversity
2DepartmentofDataScienceandArtificialIntelligence,TheHongKongPolytechnicUniversity
{chengdong.dong,zhenyucs.zhou}@connect.polyu.hk,kumar@ece.cmu.edu,ajay.kumar@polyu.edu.hk
Abstract nologies such as Neural Radiance Fields (NeRF) [2,8â€“
11,14,17] and 3D Gaussian Splatting (3DGS) [7,12,20]
The remarkable progress in neural-network-driven vi- offers a novel approach to generating highly realistic imsualdatageneration,especiallywithneuralrenderingtech- agery such as scenes and digital humans/avatars, by the
niqueslikeNeuralRadianceFieldsand3DGaussiansplat- acquisition of two-dimensional projections from lifelike
ting, offers a powerful alternative to GANs and diffusion three-dimensionalspatialrepresentations. Thereevenexist
models. These methods can produce high-fidelity images methodologies [6,22] capable of directly editing the conandlifelikeavatars,highlightingtheneedforrobustdetec- tent within 3D representations. Unlike generative models,
tion methods. In response, an unsupervised training tech- neural rendering technologies produce more realistic synnique is proposed that enables the model to extract com- theticimagesbyreconstructingscenesfromactualimages,
prehensivefeaturesfromtheFourierspectrumâ€™smagnitude, therebyavoidinglogicalandsemanticinconsistencies. This
thereby overcoming the challenges of reconstructing the processallowsforsubtle3Dmodificationsthat,whenprospectrumduetoitscentrosymmetricproperties.Byleverag- jected to 2D, are nearly imperceptible. Notably, current
ingthespectraldomainanddynamicallycombiningitwith fake image detection systems have not addressed whether
spatialdomaininformation,wecreatearobustmultimodal neural-rendered images can also be identified as non-real.
detectorthatdemonstratessuperiorgeneralizationcapabil- This limitation prompts the question of whether current
ities in identifying challenging synthetic images generated fakedetectorspossesssufficientgeneralizationcapabilities
bythelatestimagesynthesistechniques. Toaddresstheab- todetectneural-renderedimagesasfake.Specifically,when
senceofa3Dneuralrendering-basedfakeimagedatabase, such detectors are trained exclusively on synthetic images
wedevelopacomprehensivedatabasethatincludesimages produced by generative models and then tested on neuralgenerated by diverse neural rendering techniques, provid- renderedimages,theirefficacyremainsuncertain.
ing a robust foundation for evaluating and advancing de- Toenhancegeneralizationability,[25]useslinearprobtectionmethods. ing to fine-tune the linear layer after a pre-trained large
vision model. [30,41] introduce side blocks to the main
branch of fixed parameters inspired by LoRA [62] to im-

## Introduction

prove representing ability. [41] further enhances cross-
Imagessynthesizedbygenerativemodels,suchasGen- domain performance by incorporating a frequency module
erative Adversarial Networks (GANs) [55â€“57] and Diffu- (FAA) within the LoRA-like side blocks, while maintainsionModels(DM)[58â€“61],haveraisedsignificantethical, ingaspatial-basedmainbranch. However,therepresenting
privacyandsecurityrelatedconcernsinoursociety. Acriti- power of these frequency modules is limited by the concalaspectoverlookedbypriorresearchisthepotentialfora strainedscaleofparameters.
largevolumeofgeneratedimages,ifundetected,tocontam- Inthiswork,weintroduceFourierFrequency-basediminatetheimagepoolsavailableonline,whichareoftenused ageTransformer(FFiT),anarchitectureleveraginglargevito train large-scale models. As noted in [51], recursively sionmodelstoextractspectraldomaininformationforfake
generateddatacanleadtomodelcollapseduetosuchcon- detection. The FFiT backbone is pre-trained using magnitamination. To address such concerns emerging from the tudes of spectra from real images in an unsupervised way.
generativemodels,numerousneuralsyntheticimagedetec- Traditional Masked Autoencoders [53] struggle with spectionmodels[1,24,25,30,40,41]havebeendeveloped. tralmagnitudesduetotheircentrosymmetricproperties(to
However, the advancement of neural rendering tech- be detailed later), leading to superficial reconstructions by
4202
voN
31
]VC.sc[
1v24680.1142:viXra

<!-- Page 2 -->

copying patterns from unmasked symmetric patches and â€¢Incontrasttoexistingdatabasesthatfocusexclusivelyon
failingtorecoverthepatchofmagnitudefromitsneighbor- fakeimagesfromgenerativemodelslikeGANandDM,our
ing unmasked patches. To address this, a novel loss func- databaseisthefirst initsinclusionofimagesderivedfrom
tion is introduced, enabling unsupervised learning of deep neural rendering-based synthesized and edited 3D scenes,
representationsfromspectralmagnitudes. Themultimodal aswellasrealisticscenesgeneratedbySora.
architecture combines the pre-trained FFiT backbone with â€¢Toanalyzethegeneralizationabilityofdetectors, arealaspatial-basedlargevisionmodel. Asabinaryclassifier,it fakeseparationindexisdevelopedtoquantitativelyuncover
isfine-tunedondatasetscontainingbothrealandsynthetic thecorrelationbetweenthecross-domainperformanceand
images. Both branches exhibit strong independent perfor- representing the generalization ability of deep models for
mance, attributable to their design and fine-tuning strate- fakedetection.
gies. Specifically, our multimodal detector achieves av-

## RelatedWorks

erage precision (AP) of 92.81% and AUROC of 91.19%
across11typesof3Dscenegenerators,trainingonrealand

### Realistic 3D Scene Generation: NeRF methods are de-


### GAN-generatedimages. Itoutperformsthestate-of-the-art

veloped to implicitly learn the 3D representation of spe-
(SOTA)[41],provingthatneural-renderedfakeimagescan
cific scenes, which can be reconstructed from a series of
alsobeaccuratelydetected.
inputimages[8â€“11]orfromapromptsuchasatextualde-
Reference[25]findsthatdetectorsbasedonsmallerneuscription[16]orasingle-viewimage[68]. Apotentialmaralnetworks,suchasResNet,struggletoidentifychallenglicious application involves integrating NeRF with editing
ingfakesamples(e.g.,DM-generatedimages)whentrained
techniques to alter the representation of 3D scenes using
on easily distinguishable fakes (e.g., GAN-generated imtextual instructions [6,35], images [13,14], or both [34].
ages). Incontrast,detectorsusinglargernetworkslikeViT

### Additionally,NeRF-basedmethodsareusedforgenerating

generalizewelltodifficultfakesevenwhentrainedoneasier
realistic digital humans (avatars), such as speech-to-video
ones. Reference[45]notesthattrainingwithdifficultfake
talkingheads[18,19,26,27]andbodysynthesis[36,68].
samples improves generalization for detectors, even those
The 3DGS methods [7,12] enable the learning of exbased on smaller networks. However, neither study offers
plicit 3D representations of scenes, which can be seama systematic explanation for these observations. Thus, an
lessly integrated into existing rendering pipelines. Conindexisintroducedtoquantitativelyanalyzethehyperplane
ditional editing techniques, such as Instruct-GS2GS [22],
differentiating clustered features while mitigating the imhavebeendevelopedtomodify3DGSscenes. Avatarsynpact of sparse outliers, unveiling the relationship between
thesismethods[20,28]basedon3DGShavealsobeenprocross-domain performance and the representing ability of
posed. Moreover, 3DGS can be combined with diffusion
deepmodelsforfakedetection.
modelstogenerate3Dscenesfromscratch,asexemplified
To advance the research on the detection of neuralbyGSGEN[17]andDreamGaussian[29].Sora[21],which
rendered images, we have collected a large-scale database
isrepresentativeamongaseriesoftext-to-videogeneration
comprising 296,504 images generated using NeRF and
methods[21,37,38],demonstratesanimpressiveabilityto
3DGS, supplemented by 330,073 images produced via 3D
generatevideoswithaccurate3Drelationships. ThiscapasceneeditingtechniquesleveragingNeRFand3DGS.Addibilitysuggeststhatitpossessesthecapacitytorepresentthe
tionally,theadventofsophisticatedvideogenerationmeth-
3Dworldeffectively. Inthiswork,thefocusisondetecting
ods, such as Sora [21], which demonstrate exceptional cafake images synthesized by the aforementioned 3D scene
pabilityindisplayingconsistent3Dscenes,posesnewchalgenerationmethods.
lengesforthedetectionofsyntheticvisualcontent. There-
FakeImageDetection:Todetectimagesgeneratedbygenfore,wealsoacquired60,531framesgeneratedbySora.
erative models, traditional detectors based on the spatial
Insummary,themaincontributionsare: domain [1,32,33,69â€“71] and those based on the spectral
â€¢ Development of FFiT, the first architecture using a large domain [24,39,40,73] are trained on real and fake imvision model to extract spectral domain imprints for accu- ages to identify the latent fingerprints of GANs and Deeprately detecting fake images. This success is achieved by fakes. Methods that require learning [31,48,49], and a
introducinganovellossfunctiontoaddressthechallenges learning-free method [46], exploit inherent properties of
posed by the spectrumâ€™s centrosymmetric property during DMarchitecturefordetectingDM-generatedfakes. Several
unsupervisedtraining. approaches [25,30,41â€“44,47] are effective in identifying
â€¢ The dynamic consolidation of FFiT features with those both GAN- and DM-generated images. Methods [44,47]
from the spatial branch can achieve SOTA performance. enhance detection through an attention mechanism in the
Each network branch exhibits robust performance capabil- spectraldomain. [43]utilizes captionsfrom realimages to
ities, which are attributed to its well-designed architecture generatefakesandthentrainsanSVMusingdeepfeatures
andrefinedfine-tuningstrategy. frombothrealandgeneratedfakes. NPR[42]developsan

<!-- Page 3 -->

operatortorevealneighboringpixelrelationshipswithinthe in the modelâ€™s ability to infer information from the neighspatialdomainforimproveddetection. Ojhaetal.[25]in- boringpatches.
troduce a large vision model with a fixed backbone to im- case(ii): Inthecaseofmaskedpatchesforwhichthecorprovegeneralization,while[30,41]fine-tunethebackbone respondingcentrosymmetricpatchesremainunmasked,the
of large vision models like LoRA [62] to enhance repre- pre-trained model demonstrates a capability to reconstruct
sentationwhilemaintaininggeneralization. FatFormer[41] these patches with high accuracy. This suggests that the
furtherintegratesinformationfromfrequencyandlanguage modeleffectivelycapturesandutilizesthecentrosymmetric
domainstoboostcross-domainperformance. propertyofthespectrumduringtraining.
However, the aforementioned fake image detectors fail case (iii): For the unmasked regions, it is evident that the
toaddressthechallengethatthelatentpatternsofsynthetic pre-trainedmodelfailstoreconstructthemaccurately. This
images generated by newer methods, such as NeRF and finding is contrary to the expected behavior in the spatial
3DGS,maysignificantlydifferfromthoseproducedbytra- domain, where an MAE-trained model typically succeeds
ditional generative models due to the domain gap between inreconstructingunmaskedareas.
thesegenerationprocesses. Consequently,weintroducesa
novelarchitecturetoaddresstheseemergingtechniquesand
theirassociateddetectionchallenges.

## ProposedMethod

In this section, we present FFiT, a framework that uti- (a) (b) (c)
lizes large vision models to extract spectral domain infor- case (i)

### Masked, and

mation for detecting fake images. The FFiT backbone is centr m os a y s m ke m d etric
case (ii)
initiallypre-trainedinanunsupervisedmannerusingmag- cen M t a ro sk sy e m d, m b e u t t ric
unmasked
nitudespectrafromrealimages. Thispre-trainedbackbone case (iii)

### Unmasked

isthencombinedwithaspatial-basedlargevisionmodelto centrosymmetric
relationship
create a multimodal architecture, which is fine-tuned on a (d)
datasetcontainingbothrealandsyntheticimages. Thefol- Figure1.Failureinspectralinformationextractionwiththeorigilowingsubsectionsdetailthearchitectureandtrainingstrat- nal MAE pre-training. (a) Input spectrum magnitude. (b) Patch
egyoftheproposedmethod. embedding mask for inference. (c) Poor-quality reconstruction
from(a)and(b).(d)Explanationofreconstructedpatchesin(c).

### MotivationandDesignofFFiT

MAE[53]isaclassicalmethodtotrainlargeneuralmod- 3.1.1 BalancingtheWeightsofVariousMaskingTypes
els in an unsupervised way. However, the centrosymmet-
IntheoriginalMAEtrainingprocess,theblock-wisereconric characteristic of the spectrum, wherein the amplitudes
struction loss L , which represents the reconstruction
atpositivefrequenciesareequivalenttothoseatthecorre- B(i,j)
errorfortheithrowandjthcolumnblock(0â‰¤i,j â‰¤Nâˆ’1)
sponding negative frequencies, thereby exhibiting symmebetween the original input magnitude of spectrum X and
try aboutthe zerofrequency, can introduceadverse effects
thereconstructedXâ€²,iscalculatedasfollows:
tothetrainingifweusethesamewayasMAEtotrainthe
Transformer on the spectral domain. In Fig. 1a, a sample

## Wâˆ’1Wâˆ’1

oftheoriginalmagnitudeofthespectrumispresented. Fig- L = (cid:88) (cid:88) ||X(Wi+m,Wj+n)âˆ’Xâ€²(Wi+m,Wj+n)||2 (1)

### B(i,j)

ure1bdisplaysthemaskutilizedforpatchmaskingduring m=0n=0
theinferencephase,withthemodelthatistrainedusingthe where X is divided into N Ã— N patches (N is an even
original MAE-based training strategy. In this mask, white number)inaTransformer-basedarchitecture. GiventhatX
blocksindicatethepatchesthataretobemaskedduringthe isofsize224Ã—224pixelsandeachpatchisofsizeWÃ—W
patch embedding process, whereas black blocks represent pixels with W = 16, we have N = 224/W = 14. Durtheregionsthatshouldremainunmasked. Fig.1cillustrates ingthetrainingprocess,masksareappliedtothesepatches,
thereconstructedmagnitudeofthespectrum, basedonthe compellingthemodeltoreconstructthepatternswithinthe
inputfromFig.1aandthemaskshowninFig.1b,demon- maskedregions,therebyfacilitatingunsupervisedlearning.
strating a poor quality of reconstruction. In Fig. 1d, three The total loss function in the original MAE training is
representative types of regions of Fig. 1c are highlighted, computed by summing the reconstruction losses over the
andthefollowingobservationscanbemade: masked blocks, i.e., those B(i,j) that are masked. This
case(i): Whenbothamaskedpatchanditscentrosymmet- approachhastwokeylimitationsthatcontributetothefailriccounterpartaremasked,thepre-trainedmodelisunable uretoreconstructthemagnitudeofthespectrum: 1. Ignotoaccuratelyreconstructeither. Thisindicatesalimitation rance of unmasked blocks in the loss function: the model

<!-- Page 4 -->

is not penalized for any inaccuracies in the unmasked re- 3.1.2 DynamicMaskingRatioforGlobalExtraction
gions, which can lead to a lack of refinement in the over-

### Although the reconstructed sample in Fig. 2 demonstrates

all reconstruction quality. 2. Overlooking centrosymmethigh-quality reconstruction with a masking ratio of 0.25
ric information: a masked block may have an unmasked
duringinference,itcanbeobservedthattheglobalmagnicentrosymmetric counterpart from which information can
tudeofthespectrumisnotperfectlyrecoveredasshownin
be easily copied. These limitations highlight the need for

### Fig.3: whenthemodelistrainedwiththesamesettingsbut

a more sophisticated loss function or training strategy that
evaluated with a mask ratio of 0, the reconstructed magnitakesintoaccounttheunmaskedregionsandleveragesthe
tude of the spectrum (Fig. 3b) exhibits inconsistencies beinherentsymmetrieswithinthespectraldatatoimprovethe
tween blocks. Additionally, we find that if the mask ratio
reconstructionperformance.
forinferencesignificantlyvariesfromthemaskratioduring
To address the limitations of the original MAE training
training, theperformanceofspectralreconstructioncanbe
process,weadoptamodifiedlossfunctionthatincorporates
negativelyinfluenced.
the focal loss mechanism. This approach aims to balance

### This observation inspires us to introduce a dynamic

the influence of different masking cases, considering the
masking mechanism during training, where the mask ratio
specialpropertiesofthespectralmagnitude. Thelossfuncis randomly varied across different batches. Specifically,
tion,denotedasLrÌ¸=0 (X,Xâ€²|r),isdefinedasfollows:
wedefinethreelevelsofmasking: heavilymasked,slightly
r L Ì¸=0 (X,Xâ€²|r)=âˆ’ N 1 2 N (cid:88) âˆ’1N (cid:88) âˆ’1 Î± t (1âˆ’L B(i,j) )Î³logL B(i,j) , (2) m r 1 a , s r k 2 e , d a , n a d n r d 3 n s o e t tt m o a 0 s . k 3 e , d 0 , .1 w 5 i , th an c d o 0 rr . e 0 s , p r o e n sp d e in c g tiv m el a y s . k W ra it t h io in s
i=0 j=0
each mini-batch, the mask ratio is consistent (i.e., it is eiforB(i,j)âˆˆmaskingcaset(t âˆˆ {1,2,3}),Î± isusedto
t
ther r , r , or r ), but the specific mask ratio used varies
balanceitsweightaccordingtotheoccurringfrequencyof 1 2 3
betweendifferentbatches.
thespecificcase.
(a)input (b)reconstructed (c)difference
(a)input (b)mask (c)reconstructed (d)difference

### Figure3. Duringtraining, adoptourlossfunctionbutsetr asa

Figure 2. During training, we adopt the loss function in Eq. (2)
fixedvalue0.3.Duringinference,usingmaskwithratioof0.
andfixrto0.3.Forinference,themaskratioof(b)issetto0.25.
ThefocusingparameterÎ³isdesignedtoimposeagreater
DifferentfromLrÌ¸=0 (X,Xâ€²|r)

## N

fo
âˆ’
r
1
r

## N 1 âˆ’

,
1
r
2
,whenr
3
=0:
penaltyonhardexamples. ByincreasingthevalueofÎ³,the L(X,Xâ€²|r =0)= 1 (cid:88) (cid:88) L (6)
3 N2 B(i,j)
modelplacesmoreemphasisonlessfrequentsamples,thus i=0 j=0
Intheexperiments,itwasobservedthattheorderofmagimproving their representation. The balancing factor Î± is
t
used to adjust the contribution of each class based on its nitude for the loss function varies with different mask raeffectivenumberofsamples. Itiscalculatedasfollows: tios. To mitigate potential instability in training caused by
1/P significant fluctuations in gradient updates across batches
Î± = t , t=1,2,3 (3)
t 1/P +1/P +1/P forvaryingr values,weintroduceascalingfactor. Specif-
1 2 3
whereP ,P ,andP refertotheexpectedprobabilityfor ically, we compute the expected loss E[L(X,Xâ€²|r)] for
1 2 3
maskingcases1,2,and3. each r, and then scale the individual losses L(X,Xâ€²|r 1 ),
The probability of a patch being masked is assumed to L(X,Xâ€²|r 2 ),andL(X,Xâ€²|r 3 )bythereciprocaloftheirrebe r. The expected number of pairs of masked blocks for spective expectations. This normalization ensures a more
thethreedifferentcases,whicharedenotedasE ,E ,and consistent gradient descent process, thereby enhancing the
1 2
E ,canbecomputedas: stability of the neural networkâ€™s training across different
3
maskratioconfigurations.

## N2 N2 N2

E = Ã—r2,E = Ã—2rÃ—(1âˆ’r),E = Ã—(1âˆ’r)2 (4) To compute the expectation of the reconstruction loss,
1 2 2 2 3 2
weassumethatL followsaÏ‡distributionandisinde-
ThustheprobabilitiesP ,P ,andP forthreecasesare: B(i,j)
1 2 3
pendentofthescenariotypet.Adetailedderivationproving
P 1 =r2, P 2 =2rÃ—(1âˆ’r), P 3 =(1âˆ’r)2 (5) thatL B(i,j) conformstoaÏ‡distributionisprovidedinthe
A reconstructed sample is presented in Fig. 2, with the Appendix A. Our goal is to determine E[L(X,Xâ€²|r )] for
k
masking ratio during inference set to 0.25. The results in- k = 1,2,3, where E[Â·] represents the expectation over the
dicatethatregionscorrespondingtoallthreemaskingcases specifieddistribution.
arereconstructedwithhighquality. Fork =1andk =2,ther Ì¸=0,weacquire:
k

<!-- Page 5 -->

Figure4.Spatial-frequencyarchitectureshowingthedifferentblockswithdistinctfine-tuningstrategiesacrossvariousnetworkstages.

### DesignofArchitecture

3
E(L(X,Xâ€²|r ))= (cid:88) P E[L(X,Xâ€²|r )|t] (7) WeemployaTransformer-basedbackboneconsistingof
k t k
t=1 ViT-L14 blocks for the spatial branch and ViT-B16 blocks
Foreacht,theE[L(X,Xâ€²|r )|t]isequalto: forthefrequencybranch.Thisconfigurationisbasedonexk
perimentalfindingsthatindicateincreasingthecomplexity

## Nâˆ’1Nâˆ’1

âˆ’ 1 (cid:88) (cid:88) Î± E[(1âˆ’L )Î³logL ] ofthefrequencybranchdoesnotsubstantiallyenhanceper-
N2 i=0 j=0 t B(i,j) B(i,j) (8) formance but instead introduces greater instability during
=âˆ’Î± E[(1âˆ’L )Î³logL ] training and reduces inference speed. The AdaLoRA [54]
t B B
blocksareintroducedinbothspatialandspectralbranches

### Therefore,fork =1andk =2,

to dynamically fine-tune the pre-trained parameters. We
E(L(X,Xâ€²|r ))=âˆ’ (cid:32) (cid:88) 3 P Î± (cid:33) E[(1âˆ’L )Î³logL ] utilizeGated-Multimodal-Unit(GMU)[50]forinformation
k t t B B fusion. ThesequenceofblocksarevisualizedinFig.4.
t=1 (9)
3r2(1âˆ’r )2 4. Quantitative Analysis of Generalization
=âˆ’ k k E[(1âˆ’L )Î³logL ]
3r k 2âˆ’3r k +2 B B AbilityforFakeDetectors
whereE[(1âˆ’L )Î³logL ]isequalto:
B B Inthenumericalexperimentation,itisobservedthatthe
performanceoffakedetectorstrainedonfakeimagesgener-
(cid:90) âˆ (1âˆ’x)Î³logxÂ· 1 2 e âˆ’(x 2 +Î») (cid:16) Î» x(cid:17)k 4 âˆ’1 2 I k/2âˆ’1 ( âˆš Î»x)dx (10) atedbyDM,NeRF,and3DGSisgenerallybetterthanthat
0 ofdetectorstrainedonGANs. Thisobservationalignswith
whereI (z)isthemodifiedBesselfunctionofthefirstkind the findings in [25,45], which suggest that fake detectors
Î½
oforderÎ½. DetailedstepsareprovidedintheAppendixA. trained on difficult fake samples with less domain-specific
Fork =3,wecaneasilyget: information typically perform better. Additionally, we ob-
(cid:34) Nâˆ’1Nâˆ’1 (cid:35) servethatthisperformancegapdiminishesastherepresen-
E[L(X,Xâ€²|r )]=E 1 (cid:88) (cid:88) L =E[L ] (11) tationabilityofthebackbonearchitectureincreases.Topro-
3 N2 B(i,j) B
i=0 j=0 videaquantitativeanalysisofthisphenomenon,weexam-
Therefore, for mask ratio of to 0.3, 0.15, we have the inethedistributionofclusteringfeatures. Theexperimental
scaledlossfunctionL as: observations indicate that as the representation capability
k
ofthebackbonearchitectureimproves, thedistancesalong
1
E(L(X,Xâ€²|r )) Â·L(X,Xâ€²|r k ) (12) the direction perpendicular to the optimal hyperplane for
k
real/fake classification between fake-fake clusters become

### Weshowfromexperimentalresultsthatitisnecessaryto

relatively small compared to the distances between realintroducedynamicmaskingratiostocapturetheglobalrefake clusters. This explains why the generalization ability
constructionduringpre-trainingofFFiT.Afterdynamically
oflargermodelsislesssensitivetowhetherthedetectoris
settingrtor ,r ,r =0.3,0.15,0respectively,theglobal
1 2 3 trainedoneasyordifficultfakesamples. Anillustrationof
reconstructionisperfectwhichcanbeobservedfromFig.5.
this phenomenon is provided in Fig. 6. The figure shows
that as the modelâ€™s generalization capability to distinguish
between the real and fake images improves, there is a notableincreaseintheinter-clusterdistancebetweenthefake
and real feature spaces, while the subspace of all fakes remains relatively stable. This observation explains why the
numberofincorrectlyclassifiedsamplesdecreasesformod-
(a)input (b)maskfor(c) (c)from(a),(b) (d)from(a)only
elstrainedonrealandeasilydistinguishablefakedata.
Figure5. Dynamicallysetr = 0.3,0.15,0. (a)originalmagni-

### It is assumed that the parameters of the backbone are

tude,(b)maskwithratioof0.25,(c)thereconstructedmagnitude
fixed to extract the deep features of an image as f âˆˆ Rd,
using(b)mask,(d)thereconstructedmagnitudewithoutmask. i

<!-- Page 6 -->

weassignthelabelsofrealdataas0andthelabelsoffake AfterselectingtheproperÎ»toresolvetheÎ˜âˆ—usingPowdataas1,thus,thelabell âˆˆ {0,1}. Thefeaturesaremul- elloptimizationalgorithm[72], weselectthoserowsofF
i
tipliedbyU âˆˆRdÃ—1 togivetheprojectedscoresinonedi- andLcorrespondingtothetop80%lowestvaluesofZâŠ—Î˜
mension.Weconsiderthechallengingcasethatunseenfake toacquireUâˆ—.
distribution does not fit well enough to the detector due to Uâˆ— =(FâŠ¤ F )â€ FâŠ¤ L (17)
low low low low
outlierdatapointsalwaysoccurringbeyondtheclusteren- Itisassumedthatthebestthresholdofscoresin1Dfor
velope, while the predicted results for real data are stable. separationist 0 , whichissetto0.5. Consequently, theoptimal high-dimensional hyperplane in feature space is ex-
We denote the error e of each data point projected by the
i
pressedas:
hyperplaneduringtraining: Uâˆ—f âˆ’t =0 (18)
i 0
e =l âˆ’z Î¸ âˆ’fâŠ¤U (13) For a point x i in high dimension, the distance d to the
i i i i i
where z = 0 if real and z = 1 if fake. An explicit hyperplaneisrepresentedas:
i i
data-dependentvariableÎ¸ i isintroducedtomodelthissmall d=|Uâˆ—x i âˆ’t 0 |/||Uâˆ—|| (19)
amountoflargeerror. We develop the index Ï to estimate the ratio of the distances between the most difficult and easiest fake clusters
ğ‘“ğ‘ğ‘˜ğ‘’!,ğ‘“ğ‘ğ‘˜ğ‘’",â€¦,ğ‘“ğ‘ğ‘˜ğ‘’# model!s to the distances between real and fake clusters, measured
falsely classi1ied generalization ability
alongthedirectionperpendiculartotheoptimalhyperplane.
subspace of all
clusters for fakes easy (cid:32) (cid:33)
ğ‘¡$ hard â„“ğ¢ğ§ğ­ğ«ğš easy ğ‘¡$ hard â„“ğ¢ğ§ğ­ğ«ğš
Ï=
m mÌ¸= ax n xiâˆˆ (cid:80) Gm |U | âˆ— |U xi âˆ— âˆ’ || t0| âˆ’ xi (cid:80) âˆˆGn |U | âˆ— |U xi âˆ— âˆ’ || t0|
(20)
real
â„“ğ¢ğ§ğ­ğğ«
hype o r p p t l i a m n a e l ğ‘¼ < âˆ—ğ’™ r âˆ’ e h a y ğ‘¡ l ( p ,e e a rp sy la > ne real
â„“ğ¢ğ§ğ­ğğ«
hype o r p p t l i a m n a e l ğ‘¼âˆ—ğ’™ < âˆ’ r ğ‘¡ e ( h a y l p ,e e a rp sy la > ne whe |{ re i|l G i
1
=0 d } e | n i o |l
(cid:80)
i te = s 0 t
|
h

## U

e |
âˆ—
|U c
x
l
i
âˆ— u
âˆ’
| s |
t
t
0
e
|
r
+
of |{ f i a | k li
1
e =1 im }| a i| g l
(cid:80)
i e = s 1 d
|
u

## U

r |
âˆ—
i | n U
x
g
i
âˆ—
âˆ’
| t |
t
r
0
a
|
ining.

### ThequantitativeresultsofÏfordifferentfakefeatureex-

Figure6. Asthemodelâ€™sgeneralizationcapacitystrengthens,the
tractionmethodsareprovidedinSec.6.4.
inter-clusterdistancebetweenrealandfakedataincreases,leading
tofewermisclassifications. 5.DatasetsandProtocols
Theiteml i âˆ’z i Î¸ i canberegardedassoftlabelofunseen We now present the experimental setup employed for
fakedatatocomputetheoptimalhyperplaneboundwithout evaluatingthedevelopedmethodforfakeimagedetection.
thenegativeinfluencefromsparseoutlierdatapoints.Based Table1.Collecteddata Table2.Protocoloftestdata
onthismodel,weaimtoacquiretheoptimalU forthebest

### I[9] II[10] Testgroup real/fake Type

separationonthetestingdatasetas: i-ngp tensorf 1Iâˆ¼V 3,726/13,942 unseenNeRF
115/52,927109/49,475 2VIâˆ¼VIII 3,726/10,496 unseen3DGS
n (cid:20) (cid:21) III[2] IV[8] 3Pix2NeRF[13] 70,000/96,000 GAN+NeRF
Uâˆ—,Î˜âˆ— =argmin (cid:88) 1 (l âˆ’z Î¸ âˆ’fâŠ¤U)2+Î»|z Î¸ | (14) nerfacto seaThru 4SketchFaceNeRF[14]70,000/90,000 GAN+NeRF
2 i i i i i i 122/70,073 97/34,557 5DreamFusion[16] 10,000/10,600 DM+NeRF

### U,Î˜ i=0 V[11] VI[7] 6GSGEN[17] 10,000/9,540 DM+3DGS

where l , f , z , Î¸ are stacked as L, F, Z, Î˜, respec- pynerf 3DGS 7instruct-N2N[6] 3,174/40,559 editableNeRF
i i i i 84/27,457 28/3,661 8instruct-GS2GS[22] 3,174/40,559 editable3DGS
tively. Giventheâ„“ 1 penaltyimposedonÎ˜toencouragethe VII[2] VIII[12] 9GeneFace++[19] 9,100/9,100 AvatarNeRF
sparsesolution,wecandirectlycomputetheUâˆ—afterÎ˜âˆ—is splatfacto C-3DGS 10SplattingAvatar[20] 33,728/33,715 Avatar3DGS
115/43,908 36/14,446 11Sora[21] 60,000/60,5313D-realisticvideo
resolvedasfollows:

### IntroductiontoOurDatabase


## Uâˆ— =(FâŠ¤F)â€ FâŠ¤(Lâˆ’ZâŠ—Î˜âˆ—) (15)

where â€  denotes Moore-Penrose inverse, âŠ— denotes Kro- We have compiled 139 groups of consecutive 2D phonecker product. Following [52], we define F(cid:101) = I âˆ’ tos,eachaccompaniedbythecorrespondingcameraposes.
F(FâŠ¤F)â€ FâŠ¤andL(cid:101) =F(cid:101)L,wesimplifytheobjectiveas: Thesecameraposeswereeitherrecordedduringthephoto
capture process or calibrated using structure-from-motion
1
Î˜âˆ— =argmin
2
||L(cid:101)âˆ’F(cid:101)(ZâŠ—Î˜)||2
2
+Î»||ZâŠ—Î˜||
1
(16) techniques. To generate fake images, we employed a vari-
Î˜ etyofmethodscapableofproducingrealistic3Drepresentationsfromtheaforementioneddata. Specifically, weutilized different NeRF-based and 3DGS-based methods, labeledfromItoVIII,togenerate3Dscenerepresentations.
In Tab. 1, we highlight the number of successfully reconstructed3Dscenesgeneratedbyeachmethodinblue. Once
the3Dscenesarereconstructed,theyareprojectedbackto
(a)CNNSpot[1] (b)UniFD[25] (c)multimodal(Ours) 2D images using the same projection parameters as those
estimatedfromtheoriginal2Dphotos. Theserendered2D
Figure7.SamplesofDistributionsprojectedtotwodimensions.

<!-- Page 7 -->

images are considered fake, and the numbers of the fake Wefollowthesametrainingprotocolasin[41], whichinprojectedimagesarehighlightedinred. Inexperiments,the volvestrainingonlyonProGAN-generatedfakeimages,exoriginal 2D photos used for reconstruction are regarded as cept for the method from [24], which is pre-trained using
realimages. CycleGAN images following [30]. Notably, the methods
from [1,25,30â€“33] are trained on 20 classes of ProGAN-

### ProtocolsforTrainingandEvaluation

generatedimages,whileFatFormer[41]andourmethodare
Weusefourtypesoftrainingdatasets(A,B,C,D),con- trained on 4 classes. These differences are highlighted in
tainingrealimagesandfakeimagesrespectivelygenerated Tab.4withdifferentbackgroundcolors.
byGAN,DM,NeRF,and3DGStotraindifferentfakede- Ourmethodexhibitssuperiorperformancewhentrained
tectorsintheexperiments. Forthedetailsofthosetraining only on ProGAN images and tested on unseen GAN/DM-
datasets,pleaserefertoAppendixC. generated images, demonstrating its advantage in general-
For evaluation, we assess the performance on the fake izationtotraditionalgenerativemodels.
imagesacrossseveralcategories: thosegeneratedbyNeRF

### AblationStudyontheFFiTTrainingStrategy

or 3DGS, combinations of traditional generative methods
with neural rendering, editable neural rendering, digital The results of the ablation study for different training
avatars (both heads and full bodies), and synthetic video strategiesofFFiTareprovidedinFig.8, quantifiedbyAP
frames with realistic 3D representations. These categories andAUROC,respectively. Generally,theFFiTbranchprearesystematicallyevaluatedfromgroup1togroup11. trained with the developed loss function outperforms the
We provide the details of the protocol for training the samearchitecturewithoutthedevelopedlossfunction. AdarchitectureinAppendixC. ditionally, the adopted architecture demonstrates superior

## ExperimentalResults performance compared to the previous architecture used

in[24]forextractinginformationfromthemagnitudespec-

### Weprovidetheresultstocomparethedevelopedmethod

trumofforgeryimages.Wefurtheranalyzethecontribution
toexistingfakeimagedetectors.
oftheinformationextractedfromthespectralbranchtothe

### Cross-DomainEvaluationonGroup1âˆ¼11

overallperformanceintheAppendixB.
InTab.3,thecross-domaintestingperformancefordif- 6.4.QuantitativeAnalysisforGeneralization
ferentdetectorstrainedongroupsA,B,C,andDandtested

### From Tab. 3, it is found that the cross-domain results

on groups 1 to 11 is provided. The table also includes the
of the detectors trained on group A always perform worse
averageAPandaverageAUROCforeachofthefourtrainthan those trained on groups B, C, and D, which supports
inggroupsonthe11testinggroups,aswellastheaverage
theclaiminSec.4.

### AP and average AUROC for each of the 11 testing groups

Fig. 7 presents three scenarios of the distribution proon the four training groups. From Tab. 3, it can be conjectedontoa2Dspaceusingt-SNE.ThefeaturesinFigs.7a
cludedthatthemethodperformswellwhenthespatialand
to 7c are extracted by three representative fake detectors:
spectral branches operate independently. Additionally, the
[1], [25], and the proposed method, with their representadesign of the multimodal backbone demonstrates superior
tion capabilities increasing in that order. Observations reperformanceindetectingfakeimagesgeneratedbynew3D
vealthattheintra-clusterdistances(betweenfake-fakeclusrealisticmethodsacrosstestinggroups1to11,comparedto
ters)remainrelativelyconstant, whiletheinter-clusterdisotherpopularfakedetectorsinthespatialdomain,spectral
tances (between real-fake clusters) increase progressively.
domain,andmultimodaldomain.

### InTab.5,wepresenttheÏvaluesfordifferentmethods,and


### EvaluationonTraditionalFakeImageDataset the values in the table support our assumption. To compute the Ï in Tab. 5, we sample 2000 unseen real images


### We compare the performance of the proposed method

from ImageNet database, and randomly sample 2000 imwithpreviousmethodsandprovidethecomparativeresults
agesfromtraininggroupA, B, C, D respectivelytorepreinTab.4,quantifiedbytheaverageprecisionmetric.Theresent the clusters of these four types of fake images during
sultsdemonstratethesuperiorperformanceofourmethod.
training. To simulate the unseen fake subspace, we randomlysample200imagesfromtestinggroup1âˆ¼11.
Resnet-50 FFiT w/o developed loss FFiT w/ developed loss Resnet-50 FFiT w/o developed loss FFiT w/ developed loss
100 100 7.ConclusionsandFutureWork
80 80
60 60
40 40 This study introduces a multimodal architecture that
1 2 3 4 5 6 7 8 9 10 11 1 2 3 4 5 6 7 8 9 10 11
(a) (b) comprehensively leverages information from both spatial
andspectraldomainsforthedetectionofsophisticatedfake
Figure 8. Ablation study of the developed loss for pre-training
images. To pre-train the frequency branch, we address the
frequencybranch,evaluatedby(a)APmetric,and(b)AUROC.

<!-- Page 8 -->

Table3.Comparativeresultsofdifferentmethodsin%.
AveragePrecision AUROC
group
1 2 3 4 5 6 7 8 9 10 11 ave 1 2 3 4 5 6 7 8 9 10 11 ave
desab-laitaps
A 83.20 76.19 89.75 95.66 67.04 56.14 98.11 98.73 65.71 68.33 43.45 76.57 56.02 52.16 91.47 95.04 69.66 64.34 78.60 84.76 65.29 74.25 39.21 70.07
B 88.03 75.21 88.49 97.29 84.20 93.21 98.05 98.49 55.36 49.08 60.45 80.71 66.04 52.75 88.39 96.94 86.78 94.68 81.07 85.27 60.69 43.59 60.62 74.26

### CNNSpot

C / 87.06 72.89 80.47 70.36 82.55 99.97 99.79 58.34 84.62 67.21 80.33 / 66.67 73.15 79.92 64.83 79.33 99.65 97.61 59.31 92.05 68.19 78.07
[1]
D 94.31 / 92.80 65.68 61.84 96.44 99.36 99.95 65.43 73.96 59.06 80.88 80.77 / 92.53 64.92 59.24 94.46 92.74 99.46 62.12 84.99 55.47 78.67
ave 88.51 79.49 85.98 84.77 70.86 82.08 98.87 99.24 61.21 68.99 57.54 67.61 57.19 86.38 84.20 70.12 83.20 88.01 91.77 61.85 73.72 55.87
A 93.82 77.02 92.78 92.46 61.63 73.15 96.09 97.72 60.05 61.19 42.40 77.12 80.56 55.90 91.39 91.44 61.55 77.40 72.49 84.41 62.76 57.79 37.01 70.25
B 94.19 88.26 76.57 99.31 93.66 93.07 99.39 99.79 64.62 71.77 48.84 84.50 82.21 73.77 67.68 99.03 92.94 93.55 94.00 97.97 67.69 70.60 43.21 80.24

### UniFD

C / 83.65 75.23 82.14 76.50 66.72 99.89 99.91 60.41 72.55 71.51 78.85 / 65.56 71.17 81.80 71.98 73.78 98.83 99.02 57.84 70.60 72.38 76.30 [25]
D 95.67 / 92.09 86.97 73.83 62.63 99.82 99.97 64.46 83.61 82.42 84.15 86.02 / 90.08 85.73 71.04 67.37 97.92 99.72 64.04 82.91 82.89 82.77
ave 94.56 82.97 84.16 90.22 76.40 73.89 98.79 99.34 62.38 72.28 61.29 82.93 65.07 80.08 89.50 74.37 78.02 90.81 95.28 63.08 70.47 58.87
A 86.08 82.50 91.43 98.72 73.71 85.68 96.32 99.78 64.22 60.42 62.28 81.92 60.41 63.85 90.86 98.42 74.50 86.10 64.49 97.36 58.06 60.16 59.42 73.97
B 87.93 85.03 88.65 99.85 98.94 94.94 98.22 99.26 66.02 65.32 64.73 86.26 65.73 69.70 88.80 99.80 98.82 95.05 81.87 91.62 60.29 63.93 62.25 79.81

### MoeFD

C / 87.32 91.20 88.96 82.23 78.09 96.77 99.82 61.19 67.69 69.58 82.29 / 74.94 90.66 89.34 82.87 78.76 68.28 97.86 54.36 65.77 67.71 77.06
[30]
D 89.68 / 90.13 90.46 78.20 88.69 98.13 99.49 69.00 71.25 74.23 84.93 70.94 / 89.86 90.63 78.94 89.01 80.97 94.11 63.99 68.62 72.81 79.99
ave 87.89 84.95 90.35 94.49 83.27 86.85 97.36 99.58 65.10 66.17 67.70 65.69 69.49 90.04 94.54 83.78 87.23 73.90 95.23 59.17 64.62 65.54
A 93.38 92.02 78.57 91.25 98.95 98.12 97.76 97.98 63.08 49.43 62.18 83.88 85.38 85.15 76.32 93.47 99.20 98.52 83.25 83.53 63.80 54.81 64.16 80.69
B 87.67 83.77 90.95 98.51 99.54 99.14 98.35 99.81 66.78 59.04 56.05 85.42 68.88 68.17 89.52 98.37 99.60 99.27 84.12 97.76 65.19 57.85 53.09 80.17

### RGBbranch

C / 99.25 86.01 94.57 99.33 98.16 99.83 99.87 65.18 53.31 54.88 85.04 / 98.79 84.82 95.95 99.49 98.62 98.70 98.99 66.91 58.71 54.19 85.52
(ours)
D 99.45 / 92.99 99.76 99.94 99.62 99.98 99.99 77.79 77.88 57.42 90.48 98.05 / 90.76 99.71 99.94 99.68 99.84 99.96 74.64 76.06 56.12 89.48
ave 93.50 91.68 87.13 96.02 99.44 98.76 98.98 99.41 68.21 59.92 57.63 84.10 84.04 85.36 96.88 99.56 99.02 91.48 95.06 67.64 61.86 56.89
desab-lartceps
A 81.83 74.59 94.94 79.52 79.19 77.15 96.64 97.69 53.44 67.29 46.69 77.18 53.15 49.77 96.87 81.84 82.70 84.86 70.39 78.00 57.58 68.45 47.27 70.08
B 83.15 76.44 84.47 79.26 80.74 76.85 96.58 96.88 56.93 70.78 57.35 78.13 58.51 54.48 87.94 80.60 81.49 85.83 71.68 73.74 58.55 74.74 59.35 71.54

### Freq-spec

C / 87.48 97.07 90.64 59.34 92.39 99.06 99.05 57.92 64.83 76.10 82.39 / 70.42 96.18 84.73 46.72 94.21 93.10 93.33 57.94 76.46 73.26 78.64 [24]
D 96.60 / 89.56 81.52 54.01 90.47 99.71 99.68 60.43 86.79 72.76 83.15 89.70 / 91.27 68.74 65.54 93.68 97.17 96.86 63.02 86.37 70.29 82.26
ave 87.19 79.50 91.51 82.73 68.32 84.22 98.00 98.33 57.18 72.42 63.23 67.12 58.22 93.06 78.98 69.11 89.64 83.08 85.48 59.27 76.51 62.54
A 89.55 82.41 99.96 96.19 97.75 97.19 97.78 98.08 63.52 81.10 67.64 88.29 69.86 62.46 99.95 95.17 97.67 97.98 78.67 81.82 62.33 81.76 67.67 81.39
B 97.67 90.98 95.54 98.73 90.30 93.95 99.00 99.12 64.16 55.15 84.97 88.14 92.30 80.30 92.94 98.94 90.04 95.21 88.95 90.53 67.99 50.42 82.58 84.56
FFiT C / 82.66 97.90 99.91 78.53 72.24 97.67 97.59 51.10 75.82 87.31 84.07 / 64.04 98.06 99.87 78.51 79.04 81.11 80.52 50.89 73.27 84.65 79.00
D 92.75 / 99.11 94.38 64.26 96.77 99.47 99.66 50.43 96.48 80.78 87.41 79.06 / 98.84 92.77 54.91 95.81 94.62 96.95 54.00 96.76 78.53 84.23
ave 93.32 85.35 98.13 97.30 82.71 90.04 98.48 98.61 57.30 77.14 80.18 80.41 68.93 97.45 96.69 80.28 92.01 85.84 87.46 58.80 75.55 78.36
ladomitlum
A 95.76 94.93 99.37 99.65 99.35 99.08 99.38 99.50 67.19 80.16 81.58 92.36 86.89 87.93 99.30 99.55 99.40 99.23 93.22 94.54 64.76 82.07 82.07 89.91
B 93.28 87.40 99.70 99.99 99.83 99.87 98.85 99.92 72.18 76.35 86.02 92.13 81.56 75.09 99.60 99.99 99.84 99.89 88.35 99.02 67.40 75.13 87.01 88.44

### FatFormer

C / 94.25 99.88 99.99 98.87 97.36 99.86 99.90 67.82 82.72 87.38 92.80 / 85.59 99.85 99.99 98.43 97.30 98.32 98.93 63.71 83.13 88.25 91.35
[41] D 96.16 / 99.76 99.78 98.32 99.74 99.89 99.66 65.85 97.44 80.96 93.76 86.90 / 99.66 99.70 97.66 99.72 98.56 96.30 65.40 97.59 83.77 92.53
ave 95.07 92.47 99.68 99.85 99.09 99.01 99.50 99.75 68.26 84.17 83.99 85.12 82.87 99.60 99.81 98.83 99.04 94.61 97.20 65.32 84.48 85.28
A 97.56 96.21 99.15 99.86 99.79 99.56 99.68 99.83 71.72 83.38 74.13 92.81 91.76 90.67 99.08 99.82 99.80 99.62 96.28 98.00 68.74 84.89 74.42 91.19
B 91.82 87.86 99.78 99.99 99.96 99.98 98.99 99.93 73.73 73.15 89.67 92.26 77.52 76.09 99.70 99.99 99.96 99.98 89.73 99.21 69.17 71.83 90.71 88.54
Ours C / 93.79 99.75 99.99 99.27 98.40 99.76 99.83 66.15 84.71 87.94 92.96 / 84.49 99.69 99.99 98.98 98.39 97.33 98.18 62.02 84.94 85.95 91.00
D 97.98 / 99.89 99.91 97.03 99.94 99.92 99.97 66.70 99.18 85.76 94.63 92.96 / 99.84 99.88 95.59 99.94 99.10 99.74 66.29 99.09 84.36 93.68
ave 95.79 92.62 99.64 99.94 99.01 99.47 99.59 99.89 69.58 85.11 84.38 87.41 83.75 99.58 99.92 98.58 99.48 95.61 98.78 66.56 85.19 83.86

### Table4.GeneralizationresultsonUniFDdataset[25]

Generativeadversarialnetworks Diffusionmodels Total

### Method LDM GLIDE

ProGAN CycleGAN BigGAN StyleGAN GauGAN StarGAN Deepfakes Guided DALL-E mAP
200s 200sw/CFG 100s 100-27 50-27 100-10
FreqSpec[24] 55.39 100.0 75.08 55.11 66.08 100.0 45.18 57.72 77.72 77.25 76.47 68.58 64.58 61.92 67.77 69.92
CNNSpot[1] 100.0 93.47 84.50 99.54 89.49 98.15 89.02 73.72 70.62 71.00 70.54 80.65 84.91 82.07 70.59 83.88
PatchForensics[32] 80.88 72.84 71.66 85.75 65.99 69.25 76.55 75.03 87.10 86.72 86.40 85.37 83.73 78.38 75.67 78.75
CoOccurrence[33] 99.74 80.95 50.61 98.63 53.11 67.99 59.14 70.20 91.21 89.02 92.39 89.32 88.35 82.79 80.96 79.63
DIRE[31] 100.0 76.73 72.80 97.06 68.44 100.0 98.55 94.29 95.17 95.43 95.77 96.18 97.30 97.53 68.73 90.27
UniFD[25] 100.0 99.46 99.59 97.24 99.98 99.60 82.45 87.77 99.14 92.15 99.17 94.74 95.34 94.57 97.15 95.89
MoE(ViT-L14)[30] 100.0 99.97 99.93 99.81 100.0 99.81 91.45 94.40 99.87 97.85 99.93 99.12 99.48 99.23 99.22 98.67
FatFormer [41] 100.0 100.0 99.90 97.40 100.0 100.0 97.30 92.00 99.80 99.10 99.90 99.10 99.40 99.20 99.80 98.86
FreqNet [73] 99.92 99.63 96.05 99.89 99.71 98.63 99.92 96.27 96.06 100.0 62.34 99.80 99.78 96.39 77.78 94.81
RGBbranch(Ours) 100.0 99.98 99.94 99.97 99.99 99.96 97.89 95.40 99.81 98.94 99.86 98.04 98.33 97.75 99.34 99.01
Freqbranch(Ours) 99.60 98.77 97.10 98.87 96.61 99.94 98.67 92.51 99.21 98.22 99.24 97.04 97.61 96.67 94.62 97.64
multimodal(Ours) 100.0 99.99 99.99 99.99 99.99 100.0 98.58 97.00 99.91 99.51 99.94 98.33 98.81 98.11 99.60 99.31
Table5.ThecomparativeÏfordifferentmethods evenwhenfine-tunedonalimitednumberoftrainingsam-
Method CNNSpot[1] UniFD[25] FatFormer[41] multimodal(ours) ples. Theseresultsconfirmtheapproachâ€™sefficacyinaccu-
Ï 0.121 0.095 0.059 0.043 rately detecting fake images generated by advanced methodsforrealistic3Dscenes. Moreover,thisworkdevelopsa
large-scaledatabaseencompassingdiverseneural-rendered
limitationsoftraditionalMaskedAutoencodersinhandling
images,complementingexistingdatasetsthatprimarilyfothe centrosymmetric property of spectral magnitudes by
cus on GAN and DM-generated content. Additionally, we
proposinganovellossfunctionthatfacilitatesunsupervised
provideaquantitativeframeworkthatcanquantifytherelalearning of deep representations. In the future, the Transtionshipbetweencross-domainperformanceandtherepreformerarchitecturetrainedusingthedevelopedmethodcan
sentational capabilities of deep models, thereby enhancing
be extended to other tasks that require extracting informatheunderstandingofdetectorgeneralization.
tion from the spectral domain, which underscores the po-

### Thisstudypavesthewayforfurtheradvancementsinthe

tential of our approach to contribute to a broader range of
accuratedetectionofneural-renderedimagerywhileunderapplications beyond synthetic content detection. The mullining the importance of robust, generalized detection for
timodal architecture, which integrates spectral and spatial
useinreal-worldapplications.
features, demonstrates superior cross-domain performance

<!-- Page 9 -->

References radiancefieldstranslation,â€inProceedingsoftheIEEE/CVF
conference on computer vision and pattern recognition,
[1] S.-Y.Wang,O.Wang,R.Zhang,A.Owens,andA.A.Efros,
2022,pp.3981â€“3990. 2,6,13
â€œCnn-generated images are surprisingly easy to spot... for
now,â€inProceedingsoftheIEEE/CVFconferenceoncom- [14] G. Lin, L. Feng-Lin, C. Shu-Yu, J. Kaiwen, L. Chunpeng,
putervisionandpatternrecognition,2020,pp.8695â€“8704. Y. Lai, and F. Hongbo, â€œSketchfacenerf: Sketch-based fa-
1,2,6,7,8,13,16 cial generation and editing in neural radiance fields,â€ ACM

### TransactionsonGraphics,2023. 1,2,6,14

[2] M. Tancik, E. Weber, E. Ng, R. Li, B. Yi, T. Wang,
A.Kristoffersen,J.Austin,K.Salahi,A.Ahujaetal.,â€œNerf- [15] M.A.Rahman, B.Paul, N.H.Sarker, Z.I.A.Hakim, and
studio: A modular framework for neural radiance field de- S. A. Fattah, â€œArtifact: A large-scale dataset with artificial
velopment,â€inACMSIGGRAPH2023ConferenceProceed- andfactualimagesforgeneralizableandrobustsyntheticimings,2023,pp.1â€“12. 1,6,14,15 age detection,â€ in 2023 IEEE International Conference on

### ImageProcessing(ICIP). IEEE,2023,pp.2200â€“2204. 12,

[3] J.T.Barron,B.Mildenhall,D.Verbin,P.P.Srinivasan,and
13,14,15

### P. Hedman, â€œMip-nerf 360: Unbounded anti-aliased neural

radiance fields,â€ in Proceedings of the IEEE/CVF Confer- [16] B.Poole,A.Jain,J.T.Barron,andB.Mildenhall,â€œDreamenceonComputerVisionandPatternRecognition,2022,pp. fusion: Text-to-3d using 2d diffusion,â€ arXiv preprint
5470â€“5479. 14 arXiv:2209.14988,2022. 2,6,14
[4] B.Mildenhall,P.P.Srinivasan,R.Ortiz-Cayon,N.K.Kalan- [17] Z. Chen, F. Wang, and H. Liu, â€œText-to-3d using gaussian
tari,R.Ramamoorthi,R.Ng,andA.Kar,â€œLocallightfield splatting,â€arXivpreprintarXiv:2309.16585,2023. 1,2,6,
fusion: Practicalviewsynthesiswithprescriptivesampling 14
guidelines,â€ACMTransactionsonGraphics(TOG),vol.38,
[18] Z.Ye,Z.Jiang,Y.Ren,J.Liu,J.He,andZ.Zhao,â€œGeneno.4,pp.1â€“14,2019. 14 face: Generalizedandhigh-fidelityaudio-driven3dtalking
[5] C.Wang, R.Jiang, M.Chai, M.He, D.Chen, andJ.Liao, facesynthesis,â€arXivpreprintarXiv:2301.13430,2023. 2
â€œNerf-art: Text-driven neural radiance fields stylization,â€
[19] Z.Ye, J.He, Z.Jiang, R.Huang, J.Huang, J.Liu, Y.Ren,
IEEE Transactions on Visualization and Computer Graph-
X. Yin, Z. Ma, and Z. Zhao, â€œGeneface++: Generalized
ics,2023. 14
andstablereal-timeaudio-driven3dtalkingfacegeneration,â€
[6] A. Haque, M. Tancik, A. A. Efros, A. Holynski, and arXivpreprintarXiv:2305.00787,2023. 2,6,15
A. Kanazawa, â€œInstruct-nerf2nerf: Editing 3d scenes with
[20] Z. Shao, Z. Wang, Z. Li, D. Wang, X. Lin, Y. Zhang,
instructions,â€inProceedingsoftheIEEE/CVFInternational
M. Fan, and Z. Wang, â€œSplattingavatar: Realistic real-time
Conference on Computer Vision, 2023, pp. 19740â€“19750.
human avatars with mesh-embedded gaussian splatting,â€
1,2,6,14
arXivpreprintarXiv:2403.05087,2024. 1,2,6,15
[7] B.Kerbl,G.Kopanas,T.LeimkuÂ¨hler,andG.Drettakis,â€œ3d
[21] OpenAI, â€œCreating video from text,â€ 2024,
gaussian splatting for real-time radiance field rendering,â€
https://openai.com/index/sora/. 2,6,15

### ACM Transactions on Graphics, vol. 42, no. 4, pp. 1â€“14,

2023. 1,2,6,14,15 [22] J. Wu, J.-W. Bian, X. Li, G. Wang, I. Reid, P. Torr,
and V. A. Prisacariu, â€œGaussctrl: Multi-view consistent
[8] D.Levy, A.Peleg, N.Pearl, D.Rosenbaum, D.Akkaynak,
text-driven 3d gaussian splatting editing,â€ arXiv preprint
S.Korman, andT.Treibitz, â€œSeathru-nerf: Neuralradiance
arXiv:2403.08733,2024. 1,2,6,15
fieldsinscatteringmedia,â€inProceedingsoftheIEEE/CVF
Conference on Computer Vision and Pattern Recognition, [23] D. C. Epstein, I. Jain, O. Wang, and R. Zhang, â€œOnline
2023,pp.56â€“65. 1,2,6,14,15 detection of ai-generated images,â€ in Proceedings of the

### IEEE/CVF International Conference on Computer Vision,

[9] T.MuÂ¨ller,A.Evans,C.Schied,andA.Keller,â€œInstantneural
2023,pp.382â€“392. 13,15
graphics primitives with a multiresolution hash encoding,â€
ACMtransactionsongraphics(TOG),vol.41,no.4,pp.1â€“ [24] X. Zhang, S. Karaman, and S.-F. Chang, â€œDetecting and
15,2022. 1,2,6,14,15 simulating artifacts in gan fake images,â€ in 2019 IEEE internationalworkshoponinformationforensicsandsecurity
[10] A.Chen,Z.Xu,A.Geiger,J.Yu,andH.Su,â€œTensorf: Ten-
(WIFS). IEEE,2019,pp.1â€“6. 1,2,7,8,16
sorialradiancefields,â€inEuropeanConferenceonComputer
Vision. Springer,2022,pp.333â€“350. 1,2,6,14,15 [25] U.Ojha,Y.Li,andY.J.Lee,â€œTowardsuniversalfakeimage
detectorsthatgeneralizeacrossgenerativemodels,â€inPro-
[11] H.Turki,M.ZollhoÂ¨fer,C.Richardt,andD.Ramanan,â€œPynceedingsoftheIEEE/CVFConferenceonComputerVision
erf: Pyramidal neural radiance fields,â€ Advances in Neural
andPatternRecognition,2023,pp.24480â€“24489. 1,2,3,
InformationProcessingSystems,vol.36,2024. 1,2,6,14,
5,6,7,8,13,16
15
[26] Y.Guo,K.Chen,S.Liang,Y.-J.Liu,H.Bao,andJ.Zhang,
[12] J.C.Lee,D.Rho,X.Sun,J.H.Ko,andE.Park,â€œCompact
â€œAd-nerf: Audio driven neural radiance fields for talking
3dgaussianrepresentationforradiancefield,â€arXivpreprint
head synthesis,â€ in Proceedings of the IEEE/CVF internaarXiv:2311.13681,2023. 1,2,6,14,15
tionalconferenceoncomputervision,2021,pp.5784â€“5794.
[13] S. Cai, A. Obukhov, D. Dai, and L. Van Gool, â€œPix2nerf:
2
Unsupervised conditional p-gan for single image to neural

<!-- Page 10 -->

[27] S. Shen, W. Li, Z. Zhu, Y. Duan, J. Zhou, and J. Lu, [40] R. Durall, M. Keuper, and J. Keuper, â€œWatch your up-
â€œLearningdynamicfacialradiancefieldsforfew-shottalking convolution:Cnnbasedgenerativedeepneuralnetworksare
headsynthesis,â€inEuropeanconferenceoncomputervision. failing to reproduce spectral distributions,â€ in Proceedings
Springer,2022,pp.666â€“682. 2 oftheIEEE/CVFconferenceoncomputervisionandpattern
recognition,2020,pp.7890â€“7899. 1,2
[28] J. Wang, J.-C. Xie, X. Li, F. Xu, C.-M. Pun, and H. Gao,
â€œGaussianhead: High-fidelity head avatars with learn- [41] H. Liu, Z. Tan, C. Tan, Y. Wei, J. Wang, and Y. Zhao,
ablegaussianderivation,â€arXivpreprintarXiv:2312.01632, â€œForgery-awareadaptivetransformerforgeneralizablesyn-
2023. 2 thetic image detection,â€ in Proceedings of the IEEE/CVF
[29] J.Tang,J.Ren,H.Zhou,Z.Liu,andG.Zeng,â€œDreamgaus-

### Conference on Computer Vision and Pattern Recognition,

sian: Generative gaussian splatting for efficient 3d content 2024,pp.10770â€“10780. 1,2,3,7,8,16
creation,â€arXivpreprintarXiv:2309.16653,2023. 2 [42] C. Tan, Y. Zhao, S. Wei, G. Gu, P. Liu, and Y. Wei, â€œRe-
[30] Z.Liu, H.Wang, Y.Kang, andS.Wang, â€œMixtureoflow- thinkingtheup-samplingoperationsincnn-basedgenerative
rankexpertsfortransferableai-generatedimagedetection,â€ network for generalizable deepfake detection,â€ in ProceedarXivpreprintarXiv:2404.04883,2024. 1,2,3,7,8,16 ingsoftheIEEE/CVFConferenceonComputerVisionand

### PatternRecognition,2024,pp.28130â€“28139. 2,16

[31] Z.Wang,J.Bao,W.Zhou,W.Wang,H.Hu,H.Chen,and
H. Li, â€œDire for diffusion-generated image detection,â€ in [43] D.Cozzolino,G.Poggi,R.Corvi,M.NieÃŸner,andL.Ver-
ProceedingsoftheIEEE/CVFInternationalConferenceon doliva,â€œRaisingthebarofai-generatedimagedetectionwith
ComputerVision,2023,pp.22445â€“22455. 2,7,8 clip,â€inProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,2024,pp.4356â€“4366.
[32] L.Chai,D.Bau,S.-N.Lim,andP.Isola,â€œWhatmakesfake
2,16
images detectable? understanding properties that generalize,â€inComputerVisionâ€“ECCV2020: 16thEuropeanCon- [44] C.T.DolorielandN.-M.Cheung, â€œFrequencymaskingfor
ference, Glasgow, UK, August 23â€“28, 2020, Proceedings, universal deepfake detection,â€ in ICASSP 2024-2024 IEEE
PartXXVI16. Springer,2020,pp.103â€“120. 2,7,8 International Conference on Acoustics, Speech and Signal

### Processing(ICASSP). IEEE,2024,pp.13466â€“13470. 2

[33] L.Nataraj,T.M.Mohammed,S.Chandrasekaran,A.Flenner,J.H.Bappy,A.K.Roy-Chowdhury,andB.Manjunath, [45] J. Ricker, S. Damm, T. Holz, and A. Fischer, â€œTowards
â€œDetecting gan generated fake images using co-occurrence thedetectionofdiffusionmodeldeepfakes,â€ arXivpreprint
matrices,â€arXivpreprintarXiv:1903.06836,2019. 2,7,8 arXiv:2210.14571,2022. 2,5
[34] R.He,S.Huang,X.Nie,T.Hui,L.Liu,J.Dai,J.Han,G.Li, [46] J. Ricker, D. Lukovnikov, and A. Fischer, â€œAeroblade:
andS.Liu,â€œCustomizeyournerf:Adaptivesourcedriven3d Training-free detection of latent diffusion images using
sceneeditingvialocal-globaliterativetraining,â€inProceed- autoencoder reconstruction error,â€ in Proceedings of the
ingsoftheIEEE/CVFConferenceonComputerVisionand IEEE/CVF Conference on Computer Vision and Pattern
PatternRecognition,2024,pp.6966â€“6975. 2 Recognition,2024,pp.9130â€“9140. 2,16
[35] H. Jung, S. Nam, N. Sarafianos, S. Yoo, A. Sorkine- [47] Y.Li,Q.Bammey,M.Gardella,T.Nikoukhah,J.-M.Morel,
Hornung, and R. Ranjan, â€œGeometry transfer for stylizing M. Colom, and R. G. Von Gioi, â€œMasksim: Detection of
radiance fields,â€ in Proceedings of the IEEE/CVF Confer- synthetic images by masked spectrum similarity analysis,â€
enceonComputerVisionandPatternRecognition,2024,pp. in Proceedings of the IEEE/CVF Conference on Computer
8565â€“8575. 2 VisionandPatternRecognition,2024,pp.3855â€“3865. 2
[36] C. Ma, Y.-L. Liu, Z. Wang, W. Liu, X. Liu, and Z. Wang, [48] G.Cazenavette,A.Sud,T.Leung,andB.Usman,â€œFakein-
â€œHumannerf-se: A simple yet effective approach to an- version: Learning to detect images from unseen text-toimate humannerf with diverse poses,â€ in Proceedings of imagemodelsbyinvertingstablediffusion,â€inProceedings
theIEEE/CVFConferenceonComputerVisionandPattern oftheIEEE/CVFConferenceonComputerVisionandPat-
Recognition,2024,pp.1460â€“1470. 2 ternRecognition,2024,pp.10759â€“10769. 2,16
[37] W. Hong, M. Ding, W. Zheng, X. Liu, and J. Tang, [49] Z. Zhou, K. Sun, Z. Chen, H. Kuang, X. Sun, and R. Ji,
â€œCogvideo: Large-scale pretraining for text-to-video gen- â€œStealthdiffusion:Towardsevadingdiffusionforensicdetecerationviatransformers,â€arXivpreprintarXiv:2205.15868, tionthroughdiffusionmodel,â€inACMMultimedia,2024. 2,
2022. 2 16
[38] O. Bar-Tal, H. Chefer, O. Tov, C. Herrmann, R. Paiss, [50] J. Arevalo, T. Solorio, M. Montes-y GoÂ´mez, and F. A.
S. Zada, A. Ephrat, J. Hur, Y. Li, T. Michaeli et al., â€œLu- GonzaÂ´lez,â€œGatedmultimodalunitsforinformationfusion,â€
miere: Aspace-timediffusionmodelforvideogeneration,â€ arXivpreprintarXiv:1702.01992,2017. 5
arXivpreprintarXiv:2401.12945,2024. 2
[51] I.Shumailov, Z.Shumaylov, Y.Zhao, N.Papernot, R.An-
[39] T.Dzanic,K.Shah,andF.Witherden,â€œFourierspectrumdis- derson,andY.Gal,â€œAimodelscollapsewhentrainedonrecrepanciesindeepnetworkgeneratedimages,â€Advancesin cursively generated data,â€ Nature, vol. 631, no. 8022, pp.
neural information processing systems, vol. 33, pp. 3022â€“ 755â€“759,2024. 1
3032,2020. 2

<!-- Page 11 -->

[52] K. Fan, T. Liu, X. Qiu, Y. Wang, L. Huai, Z. Shangguan, [63] H.Dang,F.Liu,J.Stehouwer,X.Liu,andA.K.Jain,â€œOn
S. Gou, F. Liu, Y. Fu, Y. Fu et al., â€œTest-time linear out- thedetectionofdigitalfacemanipulation,â€inProceedingsof
of-distributiondetection,â€ inProceedingsoftheIEEE/CVF theIEEE/CVFConferenceonComputerVisionandPattern
Conference on Computer Vision and Pattern Recognition, recognition,2020,pp.5781â€“5790. 13
2024,pp.23752â€“23761. 6
[64] Y. He, B. Gan, S. Chen, Y. Zhou, G. Yin, L. Song,
[53] K. He, X. Chen, S. Xie, Y. Li, P. DollaÂ´r, and R. Girshick, L. Sheng, J. Shao, and Z. Liu, â€œForgerynet: A versatile
â€œMaskedautoencodersarescalablevisionlearners,â€inPro- benchmarkforcomprehensiveforgeryanalysis,â€inProceedceedings of the IEEE/CVF conference on computer vision ings of the IEEE/CVF conference on computer vision and
andpatternrecognition,2022,pp.16000â€“16009. 1,3 patternrecognition,2021,pp.4360â€“4369. 13
[54] Q.Zhang,M.Chen,A.Bukharin,N.Karampatziakis,P.He, [65] Y. Wang, Z. Huang, and X. Hong, â€œBenchmarking deepart
Y.Cheng,W.Chen,andT.Zhao,â€œAdalora:Adaptivebudget detection,â€arXivpreprintarXiv:2302.14475,2023. 13
allocationforparameter-efficientfine-tuning,â€arXivpreprint
[66] J. J. Bird and A. Lotfi, â€œCifake: Image classification and
arXiv:2303.10512,2023. 5
explainableidentificationofai-generatedsyntheticimages,â€
[55] T. Karras, T. Aila, S. Laine, and J. Lehtinen, â€œProgressive IEEEAccess,2024. 13
growing of gans for improved quality, stability, and varia-
[67] M.Zhu,H.Chen,Q.Yan,X.Huang,G.Lin,W.Li,Z.Tu,
tion,â€inICLR,2018. 1

### H. Hu, J. Hu, and Y. Wang, â€œGenimage: A million-scale

[56] G.KwonandJ.C.Ye,â€œDiagonalattentionandstyle-based benchmark for detecting ai-generated image,â€ Advances in
gan for content-style disentanglement in image generation NeuralInformationProcessingSystems,vol.36,2024. 12,
and translation,â€ in Proceedings of the IEEE/CVF Interna- 13
tional Conference on Computer Vision, 2021, pp. 13980â€“
[68] X. Gao, X. Li, C. Zhang, Q. Zhang, Y. Cao, Y. Shan, and
13989. 1

### L. Quan, â€œContex-human: Free-view rendering of human

[57] T. Karras, M. Aittala, S. Laine, E. HaÂ¨rkoÂ¨nen, J. Hellsten, from a single image with texture-consistent synthesis,â€ in
J. Lehtinen, and T. Aila, â€œAlias-free generative adversarial ProceedingsoftheIEEE/CVFConferenceonComputerVinetworks,â€ Advances in neural information processing sys- sionandPatternRecognition,2024,pp.10084â€“10094. 2
tems,vol.34,pp.852â€“863,2021. 1
[69] C. Tan, Y. Zhao, S. Wei, G. Gu, and Y. Wei, â€œLearn-
[58] P.DhariwalandA.Nichol,â€œDiffusionmodelsbeatganson
ing on gradients: Generalized artifacts representation for
imagesynthesis,â€Advancesinneuralinformationprocessing
gan-generated images detection,â€ in Proceedings of the
systems,vol.34,pp.8780â€“8794,2021. 1

### IEEE/CVF Conference on Computer Vision and Pattern

[59] R.Rombach,A.Blattmann,D.Lorenz,P.Esser,andB.Om- Recognition,2023,pp.12105â€“12114. 2
mer,â€œHigh-resolutionimagesynthesiswithlatentdiffusion
[70] F.Marra,D.Gragnaniello,L.Verdoliva,andG.Poggi,â€œDo
models,â€ in Proceedings of the IEEE/CVF conference on
gansleaveartificialfingerprints?â€ in2019IEEEconference
computervisionandpatternrecognition,2022,pp.10684â€“
onmultimediainformationprocessingandretrieval(MIPR).
10695. 1

### IEEE,2019,pp.506â€“511. 2

[60] A.Q.Nichol,P.Dhariwal,A.Ramesh,P.Shyam,P.Mishkin,
[71] T. Zhao, X. Xu, M. Xu, H. Ding, Y. Xiong, and W. Xia,

### B. Mcgrew, I. Sutskever, and M. Chen, â€œGlide: Towards

â€œLearning self-consistency for deepfake detection,â€ in Prophotorealisticimagegenerationandeditingwithtext-guided
ceedingsoftheIEEE/CVFinternationalconferenceoncomdiffusionmodels,â€inInternationalConferenceonMachine
putervision,2021,pp.15023â€“15033. 2

### Learning. PMLR,2022,pp.16784â€“16804. 1

[72] M.J.Powell,â€œAnefficientmethodforfindingtheminimum
[61] M. Li, T. Cai, J. Cao, Q. Zhang, H. Cai, J. Bai, Y. Jia,
ofafunctionofseveralvariableswithoutcalculatingderiva-
K.Li,andS.Han,â€œDistrifusion: Distributedparallelinfertives,â€ The computer journal, vol. 7, no. 2, pp. 155â€“162,
enceforhigh-resolutiondiffusionmodels,â€inProceedingsof
1964. 6
theIEEE/CVFConferenceonComputerVisionandPattern
Recognition,2024,pp.7183â€“7193. 1 [73] C. Tan, Y. Zhao, S. Wei, G. Gu, P. Liu, and Y. Wei,
â€œFrequency-aware deepfake detection: Improving general-
[62] E.J.Hu, Y.Shen, P.Wallis, Z.Allen-Zhu, Y.Li, S.Wang,
izabilitythroughfrequencyspacedomainlearning,â€inPro-

### L. Wang, and W. Chen, â€œLoRA: Low-rank adaptation

ceedings of the AAAI Conference on Artificial Intelligence,
of large language models,â€ in International Conference
vol.38,2024,pp.5052â€“5060. 2,8
on Learning Representations, 2022. [Online]. Available:
https://openreview.net/forum?id=nZeVKeeFYf9 1,3

<!-- Page 12 -->


## Supplementary Material

is computed. For each testing group from 1 to 11, 1000
featuresoftheimagesfromthatgroup,extractedusingthe

### A. Squared Euclidean Distance between Two

spatial branch fine-tuned on groups A, B, C, and D, are
NormallyDistributedVectors randomly sampled, resulting in a total of 4000 features.

### Thisprocessisrepeatedusingthespectralbranchtoacquire

In the main text section detailing the developed loss
another 4000 features. The Maximum Mean Discrepancy
functionforthefrequencybranch,weassumethatthepatch
(MMD) score is then computed to estimate the correlation
X representing the predicted magnitude of the frequency
betweenthefeaturesextractedfromthetwodomains.
follows a normal distribution X âˆ¼ N(Âµ ,Î£ ), while the
1 1 For each testing group, the introduction of the spectral
patchYrepresentingthegroundtruthmagnitudeofthefrebranch brings varying improvements in performance. To
quencyfollowsY âˆ¼ N(Âµ ,Î£ ). Here,Âµ andÂµ denote
2 2 1 2 quantify the significance of these improvements, we comthemeanvectors,andÎ£ andÎ£ representthecorrespond-
1 2 putethePearsoncorrelationcoefficientandthecorrespondingcovariancematrices.
ing p-value between the MMD scores and the increase in
When computing the squared Euclidean distance beperformance. Our analysis reveals a strong relationship
tween these vectors, we are essentially calculating L =
B between these two variables, indicating that the spectral
(X âˆ’ Y)âŠ¤(X âˆ’ Y). Letting Z = X âˆ’ Y, then Z is
branch provides independent and complementary informaalsoamultivariatenormalrandomvectorwithmeanÂµ =
Z tiontothespatialbranch. Thegreaterindependenceofthe
Âµ âˆ’Âµ andcovariancematrixÎ£ = Î£ +Î£ (assuming
1 2 Z 1 2 features extracted by the spectral branch contributes to a
independence).
significantimprovementinmultimodalperformance.
The distribution of ZâŠ¤Z follows a generalized chisquared distribution. Specifically, if Âµ = Âµ and Î£ =
1 2 1 C.DetailsoftheDatasetandProtocol
Î£ = I, where I is the identity matrix, then ZâŠ¤Z would
2
followastandardchi-squareddistributionwithddegreesof C.1.ComparisonofOurDatabasewithOthers
freedom (d being the dimension of X and Y). However,

### Welistthecomparisonofourdatabasewiththeprevious

when Âµ Ì¸= Âµ or Î£ Ì¸= Î£ , the distribution of ZâŠ¤Z is a
1 2 1 2 popularfakedetectiondatabaseinTab.II.
noncentralchi-squareddistribution.

### Notably, due to the generation of 3D scenes and then

NoncentralChi-SquaredDistribution: ForZâŠ¤Z,thedeprojectto2Dcostalotandneededtobebuiltfromscratch,
greesoffreedomkequalsthedimensionofZ,andthenonour generation cost evaluated by GPU hours is four times
centralityparameterÎ»isgivenby:
longerthanGenImage[67].
Î»=ÂµâŠ¤Î£âˆ’1Âµ
Z Z Z C.2.DetailsofOurCollectedDataset

### Thus,thedistributioncanbewrittenas:


### We describe the details of our collected dataset in

ZâŠ¤Zâˆ¼Ï‡2(k,Î») Tab.IV.FromAtoO,welistthesourcesofthe2Dimage
groupsusedtogeneratethe3Dscenes.

### The probability density function (PDF) of a noncentral

chi-squareddistributionwithkdegreesoffreedomandnon- C.3.SplitProtocoloftheTrainingDataset
centralityparameterÎ»isgivenby:

### A:Fortherealimages,werandomlysample20,000images

1 (cid:16)x(cid:17)(k/4âˆ’1/2) âˆš fromeachfolderof{afhq,celebahq,lsun}ofArtiFact[15]
f(x;k,Î»)= eâˆ’(x+Î»)/2 I ( Î»x)
2 Î» k/2âˆ’1 database,respectively,andcollectallthe4,318imagesfrom
landscape folder and all the 1,336 images from metfaces
whereI (z)isthemodifiedBesselfunctionofthefirstkind
Î½ folder. Thereforeweacquireatotalof65,654realimages.
oforderÎ½.
For GAN-generated fake images, we collect 10k, 10k, 7k,

### Let L be a variable that follows the noncentral chi-

B 10k, 15k, and 15k images from the folders of BigGAN,
squared distribution defined above, therefore, E[(1 âˆ’
Gans-former, GauGAN, ProjectedGAN, StyleGAN3, and

### L )Î³logL ]canbecomputedas:

B B TamingTransformer,respectively.
(cid:90) âˆ B:TherealimagesarethesameasAwhileatotalof66,896
(1âˆ’x)Î³log(x)f(x;k,Î»)dx
fakeimagesgeneratedby6DMsareselected. Exactly,we
0
collect 10k, 896, 20k, 6k, 20k, and 10k images from the
B. Contribution of Spectral Branch to the folders of Glide, DDPM, Latent Diffusion, Palette, Stable
WholeMultimodalArhictecture Diffusion,andVQDiffusion,respectively.

### C: For the real class, we use all the 69,377 real images in

In Tab. I, the proportional growth in performance af- Aâˆ¼J. For the rendered class, we sample 12,000 images in
ter introducing the spectral branch into the spatial branch

<!-- Page 13 -->

TableI.SignificanceoftheSpectralBranch
1 2 3 4 5 6 7 8 9 10 11 Pearson
AveragePrecision â‘ spatialonly 93.50 91.68 87.13 96.02 99.44 98.76 98.98 99.41 68.21 59.92 57.63 Correlation:
(in%) â‘¡multimodal 95.79 92.62 99.64 99.94 99.01 99.47 99.59 99.89 69.58 85.11 84.38 0.8816
(â‘¡âˆ’â‘ )/â‘ (in%) 2.45 1.03 14.36 4.08 -0.43 0.72 0.62 0.48 2.01 42.04 46.42 p-value:
MMD(spatial,spectral) 4.74 3.05 6.38 5.66 2.27 3.51 3.62 2.97 4.13 7.54 7.89 3.32Ã—10âˆ’4
TableII.Comparisonofdifferentfakedatabases.

### Method

Method TaskType real:fake releasingyear
generativemodel neuralrendering

### DFFD[63] deepfakeface GAN âœ— 58,703:240,336 2020

ForgeryNet[64] deepfakeface GAN âœ— 1,438,201:1,457,861 2021
DeepArt[65] deepfakeart DM âœ— 64,479:73,411 2023
CNNSpot[1] general GAN1 âœ— 362,000:362,000 2020
CIFAKE[66] general DM2 âœ— 60,000:60,000 2023

### UniFD[25] general DM3 âœ— 1000:8000 2023

GenImage[67] general GAN,DM4 âœ— 1,331,167:1,350,000 2023
ours general Sora NeRF,3DGS 259,105:687,108 2024
1ProGAN,CycleGAN,BigGAN,StyleGAN,GauGAN,StarGAN,Deepfakes,SITD,SAN,CRN,IMLE
2StableDiffusionModel
3Glide,LDM,Dalle-mini
4GAN(BigGAN)&DM(MidjourneySD1.4,MidjourneySD1.5,ADM,Glide,Wukong,VQDM)
Aâˆ¼J for method I, II, III, IV, V, respectively. Therefore, scribedin[23],tobalancethedistributionofgeneratedand
weacquireatotalof60,000renderedimages. realimagesoveranepoch.
D: For the real class, we use all the 69,377 real images

### C.5.SplitprotocolofTestingDataset

in Aâˆ¼J. For the rendered class, we use all the 40,734
splatfacto-rendered images in Aâˆ¼J and all the 10,785 Fortestinggroups1and2,weselectthescenesandthe
C3dgs-rendered images in G. Therefore, we acquire a to- corresponding 2D images that never occur in the training
talof51,519renderedimages. dataset. Thedetailsareprovidedasfollows:
Group1(Iâˆ¼V):Fortherealclass, wesampleallthereal

### C.4.DetailsofTrainingtheArchitecture

imagesfromK,L,M,N,O.Forthefakeclass,wesample
Wepre-traintheFFiTusingtheImageNetdatasetbyre- alltheimagesfromK,L,M,N,Orenderedbythemethod
sizingtheimagesto256Ã—256andthencroppingthemto I,II,III,IV,V,respectively.
224Ã—224. Duringthepre-training,wesetthefocusingpa- Group 2 (V âˆ¼ VIII): For the real class, we sample all the
rameterÎ³ inthedevelopedlossfunctionto2. Thelearning real images from K, L, M, N, O. For the fake class, we
rate is set to 1Ã—10âˆ’4 with a batch size of 256 on a sin- samplealltheimagesfromK,L,M,N,Orenderedbythe
gleH100GPU.Weemployearlystopping,terminatingthe methodVI,VII,VIII,respectively.
trainingwhenthereconstructionlossstagnatesanddoesnot Besides evaluating the performance of unseen fake imimprovefor5consecutiveepochs. ages generated by NeRF or 3DGS, further consideration
The spatial branch is initialized with ViT weights pre- isgiventoscenarioswheretraditionalgenerativemethods,
trainedontheImageNetdataset,exceptfortheAda-LoRA suchasGANsandDMs,arecombinedwithneuralrendermodules, which are randomly initialized. The spectral ingtechniquesingroups3,4,5,and6.Additionally,theuse
branchisinitializedwithViTweightspre-trainedbyFFiT, ofeditableneuralrenderingmethodsisexplored. Ingroups
which were acquired before, with its Ada-LoRA modules 7and8, tworepresentativemethodscapableofediting3D
alsorandomlyinitialized. Bothbranchesarefine-tunedus- scenes within their 3D representations are selected. A seingalearningrateof1Ã—10âˆ’4andabatchsizeof256ona ries of prompts for 3D editing are used, and these edited
singleH100GPUfor20epochs,employingtheBCEWith- 3D scenes are then projected into 2D to acquire the fake
Logits loss. The fine-tuning follows the training protocols images. Anotherimportantapplicationofneuralrendering
of groups A, B, C, and D. After acquiring the fine-tuned technologies,digitalhuman(avatar)generation,isalsoconspatial and spectral branches, we fix their parameters and sidered. Ingroups9and10,thesetechnologiesareusedto
fine-tuneonlythelastGMUlayerwiththeFClayertoob- generate images of avatars, including both heads and full
tain the optimal parameters for fake image classification. bodies. In group 11, frames sampled from Sora-generated
Duringeachtrainingstage,includingthefine-tuningofthe videos,whichexhibitrealistic3Drepresentationswithinthe
spatialbranch,spectralbranch,andGMUmodule,weusea video,arecollected.
class-balancedrandomsampler,followingtheapproachde- Pix2NeRF [13]: For real class, We use all the 70,000 imagesinffhqfolderofArtiFact[15]database. Forfakeclass,

<!-- Page 14 -->

TableIII.TheDetailedInformationofOurInstruct-N2NandInstruct-GS2GSDataset.(generatedscenes/renderedimages)
instruct-N2N instruct-GS2GS

### Prompts

K[3] L[4] M[5,6] N[7] O[8] K[3] L[4] M[5,6] N[7] O[8]
namuhrofstpmorp
Indianattire âœ— âœ— 4/353 âœ— âœ— âœ— âœ— 4/353 âœ— âœ—

### Mustache âœ— âœ— 4/353 âœ— âœ— âœ— âœ— 4/353 âœ— âœ—

Bronzestatue âœ— âœ— 4/353 âœ— âœ— âœ— âœ— 4/353 âœ— âœ—

### Jokermakeup âœ— âœ— 4/353 âœ— âœ— âœ— âœ— 4/353 âœ— âœ—

Gothicmakeup âœ— âœ— 4/353 âœ— âœ— âœ— âœ— 4/353 âœ— âœ—

### Animeeyes âœ— âœ— 4/353 âœ— âœ— âœ— âœ— 4/353 âœ— âœ—

Vintagesepiatone âœ— âœ— 4/353 âœ— âœ— âœ— âœ— 4/353 âœ— âœ—

### Neonlights âœ— âœ— 4/353 âœ— âœ— âœ— âœ— 4/353 âœ— âœ—


### Cyberpunkstyle âœ— âœ— 4/353 âœ— âœ— âœ— âœ— 4/353 âœ— âœ—

Renaissancepainting âœ— âœ— 4/353 âœ— âœ— âœ— âœ— 4/353 âœ— âœ—

### Popart âœ— âœ— 4/353 âœ— âœ— âœ— âœ— 4/353 âœ— âœ—

Tribalfacepaint âœ— âœ— 4/353 âœ— âœ— âœ— âœ— 4/353 âœ— âœ—
Alienfeatures âœ— âœ— 4/353 âœ— âœ— âœ— âœ— 4/353 âœ— âœ—

### Pixelart âœ— âœ— 4/353 âœ— âœ— âœ— âœ— 4/353 âœ— âœ—

Watercoloreffect âœ— âœ— 4/353 âœ— âœ— âœ— âœ— 4/353 âœ— âœ—

### Sketchdrawing âœ— âœ— 4/353 âœ— âœ— âœ— âœ— 4/353 âœ— âœ—

Surrealdistortion âœ— âœ— 4/353 âœ— âœ— âœ— âœ— 4/353 âœ— âœ—

### Filmnoir âœ— âœ— 4/353 âœ— âœ— âœ— âœ— 4/353 âœ— âœ—

Glitchart âœ— âœ— 4/353 âœ— âœ— âœ— âœ— 4/353 âœ— âœ—
erutanrofstpmorp
Snowylandscape 9/1,940 8/305 âœ— 2/488 4/88 9/1,940 8/305 âœ— 2/488 4/88
summerstyle 9/1,940 8/305 âœ— 2/488 4/88 9/1,940 8/305 âœ— 2/488 4/88
Autumnfoliage 9/1,940 8/305 âœ— 2/488 4/88 9/1,940 8/305 âœ— 2/488 4/88
springstyle 9/1,940 8/305 âœ— 2/488 4/88 9/1,940 8/305 âœ— 2/488 4/88
Tropicalparadise 9/1,940 8/305 âœ— 2/488 4/88 9/1,940 8/305 âœ— 2/488 4/88
Ancientstyle 9/1,940 8/305 âœ— 2/488 4/88 9/1,940 8/305 âœ— 2/488 4/88
Highbrightness 9/1,940 8/305 âœ— 2/488 4/88 9/1,940 8/305 âœ— 2/488 4/88
Halloweentheme 9/1,940 8/305 âœ— 2/488 4/88 9/1,940 8/305 âœ— 2/488 4/88
Cosmicstyle 9/1,940 8/305 âœ— 2/488 4/88 9/1,940 8/305 âœ— 2/488 4/88
Industrialchic 9/1,940 8/305 âœ— 2/488 4/88 9/1,940 8/305 âœ— 2/488 4/88
cyberpunk 9/1,940 8/305 âœ— 2/488 4/88 9/1,940 8/305 âœ— 2/488 4/88
Baroqueinspiration 9/1,940 8/305 âœ— 2/488 4/88 9/1,940 8/305 âœ— 2/488 4/88
TableIV.TheNumbers(x/y)ofGeneratedScenesandtheCorresponding2DImagesofOurDatabase:x,ydenotethenumberofgenerated
3Dscenesandrendered2Dimages,respectively.Only3Dscenesthataresuccessfullyreconstructedareadopted.
Sourcesof I[9] II[10] III[2] IV[8] V[11] VI[7] VII[2] VIII[12]
theScenes real i-ngp tensorf nerfacto seaThru pynerf 3dgs splatfacto C3dgs

### Ablender[2] 8/800 8/800 8/800 6/600 8/800 8/800 âœ— 2/200 âœ—

BD-NeRF[2] 8/1,100 7/1,000 8/1,100 5/550 8/1,100 8/1,100 âœ— âœ— âœ—
Ceyeful-T[2] 11/28,572 11/28,572 8/25,107 11/28,572 8/15,465 5/10,668 âœ— 7/9,654 âœ—
Dmill19[2] 2/3,618 2/3,618 2/3,618 2/3,618 2/3,618 2/3,618 âœ— 2/3,618 âœ—

### Enerfosr[2] 9/4,703 9/4,426 7/3,634 9/4,426 âœ— âœ— âœ— 7/3,634 âœ—

Fnerfstudio[2] 17/6,321 17/6,321 17/6,321 17/6,321 17/6,321 11/4,605 âœ— 17/6,321 âœ—
Gphototour[2] 10/17,741 âœ— âœ— 10/17,741 âœ— âœ— âœ— 8/10,785 8/10,785

### Hrecord3d[2] 1/300 1/300 1/300 1/300 1/300 1/300 âœ— 1/300 âœ—

IsdfStudio[2] 34/4,771 34/4,771 34/4,771 34/4,771 32/4,172 31/3,871 âœ— 34/4,771 âœ—

### Jsitcoms3d[2] 10/1,451 âœ— 10/1,451 âœ— âœ— âœ— âœ— 10/1,451 âœ—

Kmip360[3] 9/1,940 9/1,940 8/1,755 9/1,940 9/1,940 8/1,755 9/1,940 9/1,940 9/1,940
Lllff[4] 8/305 7/250 2/45 8/305 4/105 5/146 8/305 8/305 8/305
M1head[5,6] 4/353 4/353 1/65 4/353 2/160 1/65 3/288 4/353 3/288
Ninria[7] 4/1,040 2/488 2/488 2/488 2/488 2/488 4/1,040 2/488 4/1,040

### Ounderwater[8] 4/88 4/88 1/20 4/88 4/88 2/41 4/88 4/88 4/88

sumscenes/images 139/73,103 115/52,927 109/49,475 122/70,073 97/34,557 84/27,457 28/3,661 115/43,908 36/14,446
1Thehead/faceisfrom[6]andhead/{fangzhou,yanan,shuquan}arefrom[5].
we render 96,000 (1,000Ã—96) images, where we recon- where we use 106 prompts for generation and render 100
struct 1000 identities and render 96 images from different imagesfromdifferentviewsforeachgenerated3Dscene.
viewsforeachidentity. GSGEN [17]: For real class, we randomly sample 10,000
SketchFaceNeRF [14]: For real class, We use all the images in imagenet folder of ArtiFact [15] database. For
70,000imagesinffhqfolderofArtiFact[15]database. For the fake class, we render 9,540 (106Ã—90) images, where
fakeclass,werender90,000(60Ã—60Ã—25)images,where we use 106 prompts for generation and render 90 images
weuse60sketchestostyle-transfer60identitiesandrender fromdifferentviewsforeachgenerated3Dscene.
25 images from different views for each style-transferred Instruct-N2N [6]: For real class, we collect all the 3,174
head. real images which are used to successfully train the ner-
DreamFusion [16]: For real class, we randomly sample facto(III)ofdatasetK,L,M,N,O.Forthefakeclass,we
10,000imagesinimagenetfolderofArtiFact[15]database. generate40,559editedimagesfromthenerfacto-generated
For the fake class, we render 10,600 (106Ã—100) images,

<!-- Page 15 -->

3Dscenes. ThedetailsofthisInstruct-N2Ndatasetcanbe som tree, Indian curry, Petra Jordan, Venus flytrap, ChimfoundinTab.III. panzee, Indian samosas, Pine forest, Water lily pond, Chi-
Instruct-GS2GS [22]: For real class, we collect all the nesedumplings,Irishstew,Polarbear,WestminsterAbbey,
3,174 real images which are used to successfully train the Colosseum,Italiangelato,Redfox,Coralreef,Italianpasta,
splatfacto (VII) of dataset K, L, M, N, O. For the fake Redkangaroo.
class,wegenerate40,559editedimagesfromthesplatfactogenerated 3D scenes. The details of this Instruct-GS2GS D.Traininthepastandtestinthefuture
datasetcanbefoundinTab.III.
GeneFace++ [19]: We have 14 videos of different iden-

### TableV.DatasetSplitofPT-NNandPT-GG

tities for training the 3D representation of speaking head.
For fake speech video generation, we enable each idenrelease training evaluation
tity to speak the contents from the other 13 identities by method -date scenes images scenes images
inputting the extracted audio, and therefore generate 182
(14Ã—13)fakevideos. Fortherealclass,Weevenlysample
650framesfromeachrealvideoandgenerate9,100images
(650Ã—14). Forthefakeclass,weevenlysample50frames
fromeachfakevideoandgenerate9,100images(50Ã—182).

### SplattingAvatar [20]: For the real class, we use all the

33,728 images of 14 identities (10 identities are head and
4 identities are full body) provided by [20]. For the fake
class,wegenerate33,715renderedimagesfor14identities.
SORA [21] frames: For real class, we randomly sample
60,000imagesincocofolderofArtiFact[15]database. For
thefakeclass,wecollected94publiclyreleasedvideosgenerated by SORA and randomly cropped a total of 60,531
images in the size of 512Ã—512 pixels from the frames of
theseSORA-generatedvideos.
106 Prompts for Stable-DreamFusion and GSGEN:

### AcropolisofAthens,Desertcactus,Jamaicanjerkchicken,


### Red panda, African elephant, Dolphin, Japanese ramen,

Redwood forest, African lion, Dutch pancakes, Japanese
sushi,Rhinoceros,Alpinemeadow,Egyptiankoshari,King
cobra, Rose garden, Amazon jungle, Eiffel Tower, Koala
bear,Russianborscht,Americanburger,Emperorpenguin,

### Koreanbarbecue,SagradaFamilia,AngkorWat,Ethiopian

injera,Koreanbibimbap,SaintBasilCathedral,Arcticwolf,

### French bakery, Lavender fields, Siberian tiger, Argentine

steak,Frenchcrepes,Lebanesefalafel,Snowleopard,Australian steak, German sausages, Machu Picchu, Spanish
tapas, Bald eagle, Giant panda, Malaysian satay, Statue
of Liberty, Bamboo forest, Giraffe, Maple tree, Sunflower
field, Belgianwaffles, GoldenGateBridge, Mexicanchurros, Swedish meatballs, Bengal tiger, Gray wolf, Mexican
tacos, Swiss chocolate, Blue whale, Great Barrier Reef,

### Moroccan couscous, Sydney Opera House, Bonsai tree,


### Great Wall of China, Neuschwanstein Castle, Taj Mahal,

Brandenburg Gate, Great white shark, Notre Dame Cathedral,Thaicurry,Brazilianbarbecue,Greeksalad,Oaktree,

### Thaimangostickyrice,Britishfishandchips,Grizzlybear,

Orcawhale,TowerBridge,BurjKhalifa,HagiaSophia,Orchidgarden,Tropicalrainforest,Canadianpoutine,Hawaiian poke bowl, Palm tree, Tulip garden, Cheetah, Hippopotamus,Peruvianceviche,Turkishkebab,Cherryblos-

## Nn-Tp

real / 115 62,286 24 10,817

### I[9] 2022Jan 91 42,110 24 10,817

II[10] 2022Mar 85 38,658 24 10,817
III[2] 2023Feb 98 59,256 24 10,817
IV[8] 2023Apr 73 23,740 24 10,817
V[11] 2023Nov 60 16,640 24 10,817

## Gg-Tp

real / 20 1,786 9 1,940

### VI[7] 2023Aug 19 1,721 9 1,940


### VII[2] 2023Sep 18 1,234 9 1,940


### VIII[12] 2024Feb 19 1,721 9 1,940

We especially define two self-evaluation protocols (PT-

### NN and PT-GG) to observe the performance of detectors

which is trained on the past and tested on the future: i)
train on NeRF-rendered images and test on unseen NeRF-
rendered images, ii) train on 3DGS-rendered images and
testonunseen3DGS-renderedimages. ForPT-NNandPT-
GG,thelistofevaluationsisprovidedintheTab.V.

### PT-NN: To train the detectors, we select the images from

the methods of real, I, II, III, IV, and V, excluding those
from folders designated for evaluation, which are summarized in the list of evaluation. To evaluate the performanceofthedetectors,weutilizeallimagesfromthemethods of real, I, II, III, IV, and V located within the folders in the following list that is specified for evaluation:
nerfstudio/{bww-entrance, campanile, desolation, Egypt,
kitchen,library,person,redwoods2,storefront,stump,vegetation}, record3d/bear, head/face, eyefultower/{office1b,
office-view2,riverview},mip360/{bicycle,bonsai,counter,
flowers,kitchen,room,stump,treehill}.
PT-GG To train the detectors, we select the 1,786, 1,721,
1,234, 1,721 images from the L, M, N, O datasets for the
methods of real, VI, VII, and VIII. To evaluate the performance of the detectors, we use the images from the K
datasetforthemethodsofreal,VI,VII,andVIII.

### We provide the evaluation performance on PT-NN and

PT-GG of different detectors in Tab. VI and Tab. VII, respectively. Wesorttheneuralrenderingmethodsaccording
to the release date, for example, â€â‰¤ nerfactoâ€ means we
use the fake images generated by i-ngp, tensorf and nerfactofortraining. Theresultsin[23]revealthatthetesting
performanceontherecentlyreleasedgenerationmethodsof
thedetectorcanbebenefitfrommoreexposuretofakeim-

<!-- Page 16 -->

ages generated by newly developed methods during train- gives a fair comparison of UniFD [25] with NPR [42] and
ing. However, observed from Tab. VI and Tab. VII, such gives the conclusion that UniFD [25] is better than NPR
trenddoesnotshowinNeRFand3DGS-generatedfakeim- [42]. Wedonâ€™tcomparewith[43]sincethismethodutilizes
agedetection. the text extractor to extract the text description from real
images, and then input the text to a diffusion model-based

### TableVI.ComparativePerformanceofPT-NN(AP/AUROC)

generator to synthesize fake images. Then SVM classifier
istrainedbasedonrealandsuchsynthesizedimages. Howtest
i-ngp tensorf nerfacto seaThru pyNerf everwecannotusethesamewaytosynthesizetrainingsamtrain
â‰¤i-ngp 96.29/95.76 98.98/98.97 95.52/94.93 97.18/96.82 97.60/97.39 plesbyNerf/3DGSmethodsforgroupC andD. Wedonâ€™t
â‰¤tensorf 94.63/93.55 98.49/98.32 93.35/92.27 95.94/95.01 97.00/96.46 comparewith[46,48,49]sincetheyutilizetheinherentfea-
â‰¤nerfacto [1] 95.01/94.00 98.33/98.02 93.96/93.00 95.89/94.91 96.42/95.48 turesofDMandtheyareDM-onlymethods.
â‰¤seaThru 95.09/94.35 98.84/98.73 94.07/93.34 96.33/95.79 97.01/96.36
â‰¤pyNerf 95.18/95.07 98.69/98.64 94.12/94.06 96.19/95.85 96.63/96.30

### E.1.EvaluationMetrics

â‰¤i-ngp 93.64/91.97 96.77/96.31 93.14/91.53 92.21/90.67 92.77/91.67
â‰¤tensorf 94.41/92.66 97.74/97.20 94.01/92.15 93.40/91.75 94.12/92.88

### The evaluation performance, quantified by average pre-

â‰¤nerfacto [24] 92.97/91.21 96.70/95.94 92.59/90.58 91.82/90.16 93.75/92.71
â‰¤seaThru 94.87/94.08 97.95/97.67 94.44/93.49 94.38/93.61 95.66/95.21 cision (AP) and AUROC, of the benchmark detectors and
â‰¤pyNerf 94.81/93.77 97.20/96.50 94.32/93.20 94.20/93.21 95.33/94.59 the proposed method is provided in Tab. 3. The per-
â‰¤i-ngp 94.09/94.45 98.42/98.52 91.69/92.60 94.39/94.92 96.43/96.34
formance of the proposed method is compared with re-
â‰¤tensorf 93.77/93.78 99.26/99.25 90.10/90.36 94.49/94.44 96.98/96.69
â‰¤nerfacto [25] 92.46/92.77 98.80/98.78 88.52/89.23 93.17/93.34 96.39/96.14 implemented detectors [1,25,30] for the spatial branch
â‰¤seaThru 92.38/92.83 98.78/98.77 88.27/89.11 93.38/93.64 96.42/96.20 only, and with the re-implemented detector [24] for the
â‰¤pyNerf 92.12/92.73 98.73/98.72 88.00/89.03 93.41/93.77 96.54/96.36
spectral branch only. The multimodal backbone is com-
â‰¤i-ngp 97.55/96.78 99.12/99.06 96.74/95.43 97.39/97.10 98.62/97.87
â‰¤tensorf 96.24/94.91 99.14/99.20 95.32/93.17 96.84/96.59 97.21/97.93 paredwithFatFormer[41]byre-implementingthespatial-
â‰¤nerfacto Ours 95.42/95.18 98.97/98.86 94.73/93.61 96.54/95.32 97.89/96.25 spectral multimodal branch of [41] while removing the
â‰¤seaThru 96.35/95.21 99.48/99.12 97.63/95.89 97.27/96.54 97.91/97.36
language-based branch for fair comparison. This adjust-
â‰¤pyNerf 96.45/96.23 99.38/99.34 96.56/95.12 96.89/96.71 97.32/96.45
ment not only streamlines the comparison but also mitigatesdiscrepanciesinthetext-guidedinteraction[41]mod-
TableVII.ComparativePerformanceofPT-GG(AP/AUROC) uleduringevaluation,giventhepartialandlow-levelscenes
prevalentintheNeRF/3DGS-generatedimagesofthetesttest
3dgs splatfacto c3dgs ing dataset, unlike the uniform rich contexts of GAN/DM
train
â‰¤3dgs 79.77/76.73 73.47/70.05 81.21/78.52 outputsusedfortestingin[41]. Whencomparingthetest-
â‰¤splatfacto [1] 90.11/87.62 86.86/84.21 91.03/88.75 ing results on GAN/DM-generated images with detectors
â‰¤c3dgs 95.34/94.00 95.18/94.22 96.09/94.97
trained on ProGAN images only, the proposed methods
â‰¤3dgs 65.83/67.52 64.55/66.09 69.78/70.89
â‰¤splatfacto [24] 63.82/66.34 61.37/65.23 68.61/70.62 arecomparedwithFatFormer[41]whilekeepingtheirlan-
â‰¤c3dgs 63.33/65.10 59.25/63.29 68.44/69.53 guagebranch.
â‰¤3dgs 91.71/89.18 84.13/79.48 93.75/91.75

### Forarigorouscross-domainevaluation,theperformance

â‰¤splatfacto [25] 91.13/88.21 87.00/82.95 93.40/91.21
â‰¤c3dgs 92.55/90.39 88.21/84.68 94.55/93.06 metricsofdetectorsthatarebothtrainedandtestedonim-
â‰¤3dgs 93.67/92.34 88.59/82.14 94.42/91.88 agesgeneratedbythesamemethod,suchasthoseproduced
â‰¤splatfacto Ours 92.76/91.45 89.32/85.67 94.58/93.12 byNeRFor3DGS,arenotincludedintheprimaryresults.
â‰¤c3dgs 95.32/94.78 95.67/95.45 96.19/95.82

### These cases are considered within-domain evaluations and

do not align with the cross-domain assessment objectives.
E.ExplainationonEvaluationProtocols Instead,thefindingsfromthesewithin-domaintestsaredetailedinSec.D,adheringtoaseparateevaluationprotocol.
We select several representative methods for comparison. Wedonâ€™tcomparewithNPR[42]sinceliterature[43]

## Tables

**Table (Page 6):**

|  | ğ‘¡$ hard |
|---|---|
|  |  |


**Table (Page 8):**

| AveragePrecision group 1 2 3 4 5 6 7 8 9 10 11 ave |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| A 83.20 76.19 89.75 95.66 67.04 56.14 98.11 98.73 65.71 68.33 43.45 B 88.03 75.21 88.49 97.29 84.20 93.21 98.05 98.49 55.36 49.08 60.45 CNNSpot C / 87.06 72.89 80.47 70.36 82.55 99.97 99.79 58.34 84.62 67.21 [1] D 94.31 / 92.80 65.68 61.84 96.44 99.36 99.95 65.43 73.96 59.06 ave 88.51 79.49 85.98 84.77 70.86 82.08 98.87 99.24 61.21 68.99 57.54 |  |  |  |  |  |  |  |  |  |  |  |  | 76.57 | 56.02 52.16 91.47 95.04 69.66 64.34 78.60 84.76 65.29 74.25 39.21 66.04 52.75 88.39 96.94 86.78 94.68 81.07 85.27 60.69 43.59 60.62 / 66.67 73.15 79.92 64.83 79.33 99.65 97.61 59.31 92.05 68.19 80.77 / 92.53 64.92 59.24 94.46 92.74 99.46 62.12 84.99 55.47 |  |  |  |  |  |  |  |  |  |  | 70.07 |
|  |  |  |  |  |  |  |  |  |  |  |  |  | 80.71 |  |  |  |  |  |  |  |  |  |  |  | 74.26 |
|  |  |  |  |  |  |  |  |  |  |  |  |  | 80.33 |  |  |  |  |  |  |  |  |  |  |  | 78.07 |
|  |  |  |  |  |  |  |  |  |  |  |  |  | 80.88 |  |  |  |  |  |  |  |  |  |  |  | 78.67 |
|  | ave | 88.51 | 79.49 | 85.98 | 84.77 | 70.86 | 82.08 | 98.87 | 99.24 | 61.21 | 68.99 | 57.54 |  | 67.61 | 57.19 | 86.38 | 84.20 | 70.12 | 83.20 | 88.01 | 91.77 | 61.85 | 73.72 | 55.87 |  |
| A 93.82 77.02 92.78 92.46 61.63 73.15 96.09 97.72 60.05 61.19 42.40 B 94.19 88.26 76.57 99.31 93.66 93.07 99.39 99.79 64.62 71.77 48.84 UniFD C / 83.65 75.23 82.14 76.50 66.72 99.89 99.91 60.41 72.55 71.51 [25] D 95.67 / 92.09 86.97 73.83 62.63 99.82 99.97 64.46 83.61 82.42 ave 94.56 82.97 84.16 90.22 76.40 73.89 98.79 99.34 62.38 72.28 61.29 |  |  |  |  |  |  |  |  |  |  |  |  | 77.12 | 80.56 55.90 91.39 91.44 61.55 77.40 72.49 84.41 62.76 57.79 37.01 82.21 73.77 67.68 99.03 92.94 93.55 94.00 97.97 67.69 70.60 43.21 / 65.56 71.17 81.80 71.98 73.78 98.83 99.02 57.84 70.60 72.38 86.02 / 90.08 85.73 71.04 67.37 97.92 99.72 64.04 82.91 82.89 |  |  |  |  |  |  |  |  |  |  | 70.25 |
|  |  |  |  |  |  |  |  |  |  |  |  |  | 84.50 |  |  |  |  |  |  |  |  |  |  |  | 80.24 |
|  |  |  |  |  |  |  |  |  |  |  |  |  | 78.85 |  |  |  |  |  |  |  |  |  |  |  | 76.30 |
|  |  |  |  |  |  |  |  |  |  |  |  |  | 84.15 |  |  |  |  |  |  |  |  |  |  |  | 82.77 |
|  | ave | 94.56 | 82.97 | 84.16 | 90.22 | 76.40 | 73.89 | 98.79 | 99.34 | 62.38 | 72.28 | 61.29 |  | 82.93 | 65.07 | 80.08 | 89.50 | 74.37 | 78.02 | 90.81 | 95.28 | 63.08 | 70.47 | 58.87 |  |
| A 86.08 82.50 91.43 98.72 73.71 85.68 96.32 99.78 64.22 60.42 62.28 B 87.93 85.03 88.65 99.85 98.94 94.94 98.22 99.26 66.02 65.32 64.73 MoeFD C / 87.32 91.20 88.96 82.23 78.09 96.77 99.82 61.19 67.69 69.58 [30] D 89.68 / 90.13 90.46 78.20 88.69 98.13 99.49 69.00 71.25 74.23 ave 87.89 84.95 90.35 94.49 83.27 86.85 97.36 99.58 65.10 66.17 67.70 |  |  |  |  |  |  |  |  |  |  |  |  | 81.92 | 60.41 63.85 90.86 98.42 74.50 86.10 64.49 97.36 58.06 60.16 59.42 65.73 69.70 88.80 99.80 98.82 95.05 81.87 91.62 60.29 63.93 62.25 / 74.94 90.66 89.34 82.87 78.76 68.28 97.86 54.36 65.77 67.71 70.94 / 89.86 90.63 78.94 89.01 80.97 94.11 63.99 68.62 72.81 |  |  |  |  |  |  |  |  |  |  | 73.97 |
|  |  |  |  |  |  |  |  |  |  |  |  |  | 86.26 |  |  |  |  |  |  |  |  |  |  |  | 79.81 |
|  |  |  |  |  |  |  |  |  |  |  |  |  | 82.29 |  |  |  |  |  |  |  |  |  |  |  | 77.06 |
|  |  |  |  |  |  |  |  |  |  |  |  |  | 84.93 |  |  |  |  |  |  |  |  |  |  |  | 79.99 |
|  | ave | 87.89 | 84.95 | 90.35 | 94.49 | 83.27 | 86.85 | 97.36 | 99.58 | 65.10 | 66.17 | 67.70 |  | 65.69 | 69.49 | 90.04 | 94.54 | 83.78 | 87.23 | 73.90 | 95.23 | 59.17 | 64.62 | 65.54 |  |
| A 93.38 92.02 78.57 91.25 98.95 98.12 97.76 97.98 63.08 49.43 62.18 B 87.67 83.77 90.95 98.51 99.54 99.14 98.35 99.81 66.78 59.04 56.05 RGBbranch C / 99.25 86.01 94.57 99.33 98.16 99.83 99.87 65.18 53.31 54.88 (ours) D 99.45 / 92.99 99.76 99.94 99.62 99.98 99.99 77.79 77.88 57.42 ave 93.50 91.68 87.13 96.02 99.44 98.76 98.98 99.41 68.21 59.92 57.63 |  |  |  |  |  |  |  |  |  |  |  |  | 83.88 | 85.38 85.15 76.32 93.47 99.20 98.52 83.25 83.53 63.80 54.81 64.16 68.88 68.17 89.52 98.37 99.60 99.27 84.12 97.76 65.19 57.85 53.09 / 98.79 84.82 95.95 99.49 98.62 98.70 98.99 66.91 58.71 54.19 98.05 / 90.76 99.71 99.94 99.68 99.84 99.96 74.64 76.06 56.12 |  |  |  |  |  |  |  |  |  |  | 80.69 |
|  |  |  |  |  |  |  |  |  |  |  |  |  | 85.42 |  |  |  |  |  |  |  |  |  |  |  | 80.17 |
|  |  |  |  |  |  |  |  |  |  |  |  |  | 85.04 |  |  |  |  |  |  |  |  |  |  |  | 85.52 |
|  |  |  |  |  |  |  |  |  |  |  |  |  | 90.48 |  |  |  |  |  |  |  |  |  |  |  | 89.48 |
|  | ave | 93.50 | 91.68 | 87.13 | 96.02 | 99.44 | 98.76 | 98.98 | 99.41 | 68.21 | 59.92 | 57.63 |  | 84.10 | 84.04 | 85.36 | 96.88 | 99.56 | 99.02 | 91.48 | 95.06 | 67.64 | 61.86 | 56.89 |  |
| A 81.83 74.59 94.94 79.52 79.19 77.15 96.64 97.69 53.44 67.29 46.69 B 83.15 76.44 84.47 79.26 80.74 76.85 96.58 96.88 56.93 70.78 57.35 Freq-spec C / 87.48 97.07 90.64 59.34 92.39 99.06 99.05 57.92 64.83 76.10 [24] D 96.60 / 89.56 81.52 54.01 90.47 99.71 99.68 60.43 86.79 72.76 ave 87.19 79.50 91.51 82.73 68.32 84.22 98.00 98.33 57.18 72.42 63.23 |  |  |  |  |  |  |  |  |  |  |  |  | 77.18 | 53.15 49.77 96.87 81.84 82.70 84.86 70.39 78.00 57.58 68.45 47.27 58.51 54.48 87.94 80.60 81.49 85.83 71.68 73.74 58.55 74.74 59.35 / 70.42 96.18 84.73 46.72 94.21 93.10 93.33 57.94 76.46 73.26 89.70 / 91.27 68.74 65.54 93.68 97.17 96.86 63.02 86.37 70.29 |  |  |  |  |  |  |  |  |  |  | 70.08 |
|  |  |  |  |  |  |  |  |  |  |  |  |  | 78.13 |  |  |  |  |  |  |  |  |  |  |  | 71.54 |
|  |  |  |  |  |  |  |  |  |  |  |  |  | 82.39 |  |  |  |  |  |  |  |  |  |  |  | 78.64 |
|  |  |  |  |  |  |  |  |  |  |  |  |  | 83.15 |  |  |  |  |  |  |  |  |  |  |  | 82.26 |
|  | ave | 87.19 | 79.50 | 91.51 | 82.73 | 68.32 | 84.22 | 98.00 | 98.33 | 57.18 | 72.42 | 63.23 |  | 67.12 | 58.22 | 93.06 | 78.98 | 69.11 | 89.64 | 83.08 | 85.48 | 59.27 | 76.51 | 62.54 |  |
| A 89.55 82.41 99.96 96.19 97.75 97.19 97.78 98.08 63.52 81.10 67.64 B 97.67 90.98 95.54 98.73 90.30 93.95 99.00 99.12 64.16 55.15 84.97 FFiT C / 82.66 97.90 99.91 78.53 72.24 97.67 97.59 51.10 75.82 87.31 D 92.75 / 99.11 94.38 64.26 96.77 99.47 99.66 50.43 96.48 80.78 ave 93.32 85.35 98.13 97.30 82.71 90.04 98.48 98.61 57.30 77.14 80.18 |  |  |  |  |  |  |  |  |  |  |  |  | 88.29 | 69.86 62.46 99.95 95.17 97.67 97.98 78.67 81.82 62.33 81.76 67.67 92.30 80.30 92.94 98.94 90.04 95.21 88.95 90.53 67.99 50.42 82.58 / 64.04 98.06 99.87 78.51 79.04 81.11 80.52 50.89 73.27 84.65 79.06 / 98.84 92.77 54.91 95.81 94.62 96.95 54.00 96.76 78.53 |  |  |  |  |  |  |  |  |  |  | 81.39 |
|  |  |  |  |  |  |  |  |  |  |  |  |  | 88.14 |  |  |  |  |  |  |  |  |  |  |  | 84.56 |
|  |  |  |  |  |  |  |  |  |  |  |  |  | 84.07 |  |  |  |  |  |  |  |  |  |  |  | 79.00 |
|  |  |  |  |  |  |  |  |  |  |  |  |  | 87.41 |  |  |  |  |  |  |  |  |  |  |  | 84.23 |
|  | ave | 93.32 | 85.35 | 98.13 | 97.30 | 82.71 | 90.04 | 98.48 | 98.61 | 57.30 | 77.14 | 80.18 |  | 80.41 | 68.93 | 97.45 | 96.69 | 80.28 | 92.01 | 85.84 | 87.46 | 58.80 | 75.55 | 78.36 |  |
| A 95.76 94.93 99.37 99.65 99.35 99.08 99.38 99.50 67.19 80.16 81.58 B 93.28 87.40 99.70 99.99 99.83 99.87 98.85 99.92 72.18 76.35 86.02 FatFormer C / 94.25 99.88 99.99 98.87 97.36 99.86 99.90 67.82 82.72 87.38 [41] D 96.16 / 99.76 99.78 98.32 99.74 99.89 99.66 65.85 97.44 80.96 ave 95.07 92.47 99.68 99.85 99.09 99.01 99.50 99.75 68.26 84.17 83.99 |  |  |  |  |  |  |  |  |  |  |  |  | 92.36 | 86.89 87.93 99.30 99.55 99.40 99.23 93.22 94.54 64.76 82.07 82.07 81.56 75.09 99.60 99.99 99.84 99.89 88.35 99.02 67.40 75.13 87.01 / 85.59 99.85 99.99 98.43 97.30 98.32 98.93 63.71 83.13 88.25 86.90 / 99.66 99.70 97.66 99.72 98.56 96.30 65.40 97.59 83.77 |  |  |  |  |  |  |  |  |  |  | 89.91 |
|  |  |  |  |  |  |  |  |  |  |  |  |  | 92.13 |  |  |  |  |  |  |  |  |  |  |  | 88.44 |
|  |  |  |  |  |  |  |  |  |  |  |  |  | 92.80 |  |  |  |  |  |  |  |  |  |  |  | 91.35 |
|  |  |  |  |  |  |  |  |  |  |  |  |  | 93.76 |  |  |  |  |  |  |  |  |  |  |  | 92.53 |
|  | ave | 95.07 | 92.47 | 99.68 | 99.85 | 99.09 | 99.01 | 99.50 | 99.75 | 68.26 | 84.17 | 83.99 |  | 85.12 | 82.87 | 99.60 | 99.81 | 98.83 | 99.04 | 94.61 | 97.20 | 65.32 | 84.48 | 85.28 |  |
| A 97.56 96.21 99.15 99.86 99.79 99.56 99.68 99.83 71.72 83.38 74.13 B 91.82 87.86 99.78 99.99 99.96 99.98 98.99 99.93 73.73 73.15 89.67 Ours C / 93.79 99.75 99.99 99.27 98.40 99.76 99.83 66.15 84.71 87.94 D 97.98 / 99.89 99.91 97.03 99.94 99.92 99.97 66.70 99.18 85.76 ave 95.79 92.62 99.64 99.94 99.01 99.47 99.59 99.89 69.58 85.11 84.38 |  |  |  |  |  |  |  |  |  |  |  |  | 92.81 | 91.76 90.67 99.08 99.82 99.80 99.62 96.28 98.00 68.74 84.89 74.42 77.52 76.09 99.70 99.99 99.96 99.98 89.73 99.21 69.17 71.83 90.71 / 84.49 99.69 99.99 98.98 98.39 97.33 98.18 62.02 84.94 85.95 92.96 / 99.84 99.88 95.59 99.94 99.10 99.74 66.29 99.09 84.36 |  |  |  |  |  |  |  |  |  |  | 91.19 |
|  |  |  |  |  |  |  |  |  |  |  |  |  | 92.26 |  |  |  |  |  |  |  |  |  |  |  | 88.54 |
|  |  |  |  |  |  |  |  |  |  |  |  |  | 92.96 |  |  |  |  |  |  |  |  |  |  |  | 91.00 |
|  |  |  |  |  |  |  |  |  |  |  |  |  | 94.63 |  |  |  |  |  |  |  |  |  |  |  | 93.68 |
|  | ave | 95.79 | 92.62 | 99.64 | 99.94 | 99.01 | 99.47 | 99.59 | 99.89 | 69.58 | 85.11 | 84.38 |  | 87.41 | 83.75 | 99.58 | 99.92 | 98.58 | 99.48 | 95.61 | 98.78 | 66.56 | 85.19 | 83.86 |  |


**Table (Page 8):**

| FreqSpec[24] | 55.39 | 100.0 | 75.08 | 55.11 | 66.08 | 100.0 | 45.18 | 57.72 | 77.72 | 77.25 | 76.47 | 68.58 | 64.58 | 61.92 | 67.77 | 69.92 |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| CNNSpot[1] | 100.0 | 93.47 | 84.50 | 99.54 | 89.49 | 98.15 | 89.02 | 73.72 | 70.62 | 71.00 | 70.54 | 80.65 | 84.91 | 82.07 | 70.59 | 83.88 |
| PatchForensics[32] | 80.88 | 72.84 | 71.66 | 85.75 | 65.99 | 69.25 | 76.55 | 75.03 | 87.10 | 86.72 | 86.40 | 85.37 | 83.73 | 78.38 | 75.67 | 78.75 |
| CoOccurrence[33] | 99.74 | 80.95 | 50.61 | 98.63 | 53.11 | 67.99 | 59.14 | 70.20 | 91.21 | 89.02 | 92.39 | 89.32 | 88.35 | 82.79 | 80.96 | 79.63 |
| DIRE[31] | 100.0 | 76.73 | 72.80 | 97.06 | 68.44 | 100.0 | 98.55 | 94.29 | 95.17 | 95.43 | 95.77 | 96.18 | 97.30 | 97.53 | 68.73 | 90.27 |
| UniFD[25] | 100.0 | 99.46 | 99.59 | 97.24 | 99.98 | 99.60 | 82.45 | 87.77 | 99.14 | 92.15 | 99.17 | 94.74 | 95.34 | 94.57 | 97.15 | 95.89 |
| MoE(ViT-L14)[30] | 100.0 | 99.97 | 99.93 | 99.81 | 100.0 | 99.81 | 91.45 | 94.40 | 99.87 | 97.85 | 99.93 | 99.12 | 99.48 | 99.23 | 99.22 | 98.67 |
| FatFormer [41] | 100.0 | 100.0 | 99.90 | 97.40 | 100.0 | 100.0 | 97.30 | 92.00 | 99.80 | 99.10 | 99.90 | 99.10 | 99.40 | 99.20 | 99.80 | 98.86 |
| FreqNet [73] | 99.92 | 99.63 | 96.05 | 99.89 | 99.71 | 98.63 | 99.92 | 96.27 | 96.06 | 100.0 | 62.34 | 99.80 | 99.78 | 96.39 | 77.78 | 94.81 |
| RGBbranch(Ours) | 100.0 | 99.98 | 99.94 | 99.97 | 99.99 | 99.96 | 97.89 | 95.40 | 99.81 | 98.94 | 99.86 | 98.04 | 98.33 | 97.75 | 99.34 | 99.01 |
| Freqbranch(Ours) | 99.60 | 98.77 | 97.10 | 98.87 | 96.61 | 99.94 | 98.67 | 92.51 | 99.21 | 98.22 | 99.24 | 97.04 | 97.61 | 96.67 | 94.62 | 97.64 |
| multimodal(Ours) | 100.0 | 99.99 | 99.99 | 99.99 | 99.99 | 100.0 | 98.58 | 97.00 | 99.91 | 99.51 | 99.94 | 98.33 | 98.81 | 98.11 | 99.60 | 99.31 |


**Table (Page 14):**

| instruct-N2N Prompts K[3] L[4] M[5,6] N[7] O[8] |
|---|
| Indianattire âœ— âœ— 4/353 âœ— âœ— Mustache âœ— âœ— 4/353 âœ— âœ— Bronzestatue âœ— âœ— 4/353 âœ— âœ— Jokermakeup âœ— âœ— 4/353 âœ— âœ— Gothicmakeup âœ— âœ— 4/353 âœ— âœ— Animeeyes âœ— âœ— 4/353 âœ— âœ— Vintagesepiatone âœ— âœ— 4/353 âœ— âœ— Neonlights âœ— âœ— 4/353 âœ— âœ— Cyberpunkstyle âœ— âœ— 4/353 âœ— âœ— Renaissancepainting âœ— âœ— 4/353 âœ— âœ— Popart âœ— âœ— 4/353 âœ— âœ— Tribalfacepaint âœ— âœ— 4/353 âœ— âœ— Alienfeatures âœ— âœ— 4/353 âœ— âœ— Pixelart âœ— âœ— 4/353 âœ— âœ— Watercoloreffect âœ— âœ— 4/353 âœ— âœ— Sketchdrawing âœ— âœ— 4/353 âœ— âœ— Surrealdistortion âœ— âœ— 4/353 âœ— âœ— Filmnoir âœ— âœ— 4/353 âœ— âœ— Glitchart âœ— âœ— 4/353 âœ— âœ— |
| Snowylandscape 9/1,940 8/305 âœ— 2/488 4/88 summerstyle 9/1,940 8/305 âœ— 2/488 4/88 Autumnfoliage 9/1,940 8/305 âœ— 2/488 4/88 springstyle 9/1,940 8/305 âœ— 2/488 4/88 Tropicalparadise 9/1,940 8/305 âœ— 2/488 4/88 Ancientstyle 9/1,940 8/305 âœ— 2/488 4/88 Highbrightness 9/1,940 8/305 âœ— 2/488 4/88 Halloweentheme 9/1,940 8/305 âœ— 2/488 4/88 Cosmicstyle 9/1,940 8/305 âœ— 2/488 4/88 Industrialchic 9/1,940 8/305 âœ— 2/488 4/88 cyberpunk 9/1,940 8/305 âœ— 2/488 4/88 Baroqueinspiration 9/1,940 8/305 âœ— 2/488 4/88 |
