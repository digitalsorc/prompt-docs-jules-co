---
title: "Towards System 2 Reasoning Meta CoT"
original_file: "./Towards_System_2_Reasoning_Meta_CoT.pdf"
document_type: "research"
conversion_date: "2025-11-29"
topics: ["prompt-engineering", "llm", "rag", "chain-of-thought", "agents"]
keywords: ["model", "search", "reasoning", "training", "models", "thought", "but", "roots", "page", "meta"]
summary: "<!-- Page 1 -->

Towards System 2 Reasoning in LLMs:
Learning How to Think With Meta

### Chain-of-Thought

VioletXiang2,CharlieSnell3,KanishkGandhi2,AlonAlbalak1,AnikaitSingh2,ChaseBlagden1,DuyPhung1,
RafaelRafailov2,1,NathanLile1,DakotaMahan1,LouisCastricato1,Jan-PhilippFrÃ¤nken2,NickHaber2 and

### ChelseaFinn2

1SynthLabs.ai,2StanfordUniversity,3UCBerkeley
We propose a novel framework, Meta Chain-of-Thought (Meta-CoT), which extends traditional Chain-of-
Thought (CoT) by explicitly modeling t"
related_documents: []
---

# Towards System 2 Reasoning Meta CoT

<!-- Page 1 -->

Towards System 2 Reasoning in LLMs:
Learning How to Think With Meta

### Chain-of-Thought

VioletXiang2,CharlieSnell3,KanishkGandhi2,AlonAlbalak1,AnikaitSingh2,ChaseBlagden1,DuyPhung1,
RafaelRafailov2,1,NathanLile1,DakotaMahan1,LouisCastricato1,Jan-PhilippFrÃ¤nken2,NickHaber2 and

### ChelseaFinn2

1SynthLabs.ai,2StanfordUniversity,3UCBerkeley
We propose a novel framework, Meta Chain-of-Thought (Meta-CoT), which extends traditional Chain-of-
Thought (CoT) by explicitly modeling the underlying reasoning required to arrive at a particular CoT. We
presentempiricalevidencefromstate-of-the-artmodelsexhibitingbehaviorsconsistentwithin-contextsearch,
andexploremethodsforproducingMeta-CoTviaprocesssupervision,syntheticdatageneration,andsearch
algorithms. Finally,weoutlineaconcretepipelinefortrainingamodeltoproduceMeta-CoTs,incorporating
instructiontuningwithlinearizedsearchtracesandreinforcementlearningpost-training. Finally,wediscuss
openresearchquestions,includingscalinglaws,verifierroles,andthepotentialfordiscoveringnovelreasoning
algorithms. ThisworkprovidesatheoreticalandpracticalroadmaptoenableMeta-CoTinLLMs,pavingthe
wayformorepowerfulandhuman-likereasoninginartificialintelligence.

### Give a man a fish and you feed him for a day;

teach a man to fish and you feed him for a lifetime.
-Proverb

### Contents

1 Introduction 4
1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.2 Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2 Meta Chain-Of-Thought 6
2.1 Deriving The Meta-CoT Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.2 Why Does (Classical) CoT Fail? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
3 Towards Deliberate Reasoning With Language Models - Search 9
3.1 Inference-Time Compute: Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
3.2 Inference-Time Compute: Verification . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
Authorsareorderedrandomly.Correspondencetoteam@synthlabs.ai.
5202
naJ
8
]IA.sc[
1v28640.1052:viXra

<!-- Page 2 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
3.3 From Best-of-N To General Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
3.4 Is Search (Inference Time Compute) A Fundamental Capability Shift? . . . . . . . . . 14
4 Towards Meta-CoT Reasoning 15
4.1 Bootstrapping Meta-CoT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
4.1.1 Self-Taught Reasoner . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
4.1.2 Meta-STaR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
4.2 Empirical Examples Of Internalizing Search . . . . . . . . . . . . . . . . . . . . . . . 16
4.2.1 Small-Scale Empirical Results on Internalizing Search . . . . . . . . . . . . . . 16
4.2.2 In-context Exploration For LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . 19
4.2.3 Using variable Compute . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
4.2.4 Backtracking in LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
4.3 Synthetic Meta-CoT Via Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
4.3.1 Monte-Carlo Tree Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
4.3.2 A* search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
4.4 Do Advanced Reasoning Systems Implement In-Context Search? . . . . . . . . . . . . 23
5 Process Supervision 26
5.1 Learning Process Reward Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
5.2 PRM Quality And Its Effect On Search . . . . . . . . . . . . . . . . . . . . . . . . . . 27
5.3 Verifiable Versus Open-Ended Problems . . . . . . . . . . . . . . . . . . . . . . . . . . 27
6 Meta Reinforcement Learning - Learning How To Think 28
6.1 Meta-RL In Small Domains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
6.2 Meta-RL In Language Model Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . 32
6.3 Efficiency Or Super-Intelligence? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
6.4 Can System 2 Reasoning Emerge From Pure RL? . . . . . . . . . . . . . . . . . . . . . 35
6.4.1 Inducing Meta-Reasoning In LLMs . . . . . . . . . . . . . . . . . . . . . . . . 35
7 Putting It All Together - A Pipeline for System 2 Reasoning 39
7.1 Instruction Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
7.2 Post-Training With RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
7.2.1 Q* or q-STaR? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
7.2.2 Discount Rates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
2

<!-- Page 3 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
8 Going Forward 43
8.1 The "Big MATH" Project . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
8.1.1 Data Sourcing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
8.1.2 Data Filtering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
8.2 Infrastructure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
8.3 Open Research Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
8.3.1 Open-Ended Verification And CoT Faithfulness . . . . . . . . . . . . . . . . . . 47
8.3.2 Process Guidance And The Verifier Gap . . . . . . . . . . . . . . . . . . . . . . 48
8.3.3 Scaling Laws For Reasoning And Search . . . . . . . . . . . . . . . . . . . . . 49
8.3.4 Meta-Search/Search 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
8.3.5 Reasoning with External Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
9 Conclusion 51
10 Acknowledgments 52

### A Prompting 62


### B Regret Analysis 63

C Different Instruction Tuning Objectives 63

### D MCTS Details 63

E Chains-Of-Thought 64
3

<!-- Page 4 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought

## Introduction


### Motivation

A key aspect of the current era of Large Language Models has been the foundational principle of
next-token prediction (Elman, 1990; Jordan, 1997). That is, tokenizing text (or other continuous
modalities) into a discrete sequence in the following way:
"The quick brown fox jumps over the lazy dog." â†’ y ,y ,...,y ,
1 2 ğ‘›
where y are elements of some finite vocabulary and, subsequently, train a large parameterized
ğ‘–
neural network ğ‘ (transformer) model with the following maximum likelihood objective:
ğœƒ
[ï¸ƒ ]ï¸ƒ
âˆ‘ï¸
â„’ = E âˆ’ logğ‘ (y |y ) .
ğœƒ ğ’Ÿtrain ğœƒ ğ‘¡+1 â‰¤ğ‘¡
ğ‘¡
Behind this approach is a simple principle often abbreviated as "compression is intelligence", or
the model must approximate the distribution of data and perform implicit reasoning in its activations
in order to predict the next token (see Solomonoff Induction; Solomonoff 1964). That is, language
models learn the implicit meaning in text, as opposed to the early belief some researchers held that
sequence-to-sequence models (including transformers) simply fit correlations between sequential
words.
A fundamental question, however, is: What is the relationship between the complexity of the
underlying data stream and the modelâ€™s capability to learn the data-generating algorithm? While
natural language has some irreducible entropy, this question holds even in deterministic scenarios (as
demonstrated below). Mathematical reasoning is a good domain to demonstrate and evaluate these
questions. One can ask an LLM to generate a response to questions like:
â€œWhat is 1+2?â€
to which most immediately answer â€œ3â€. We can increase the complexity of the question by asking, for
example:
â€œWhat is the value of
(ğ‘¥2âˆ’1)(ğ‘¥+1) 1
âˆ’
ğ‘¥3âˆ’ğ‘¥ ğ‘¥
evaluated at ğ‘¥ = ğœ‹?â€
This is actually a simple question since the expression is canonically equal to 1, yet even powerful
LLMs such as GPT-4o and Claude fail to answer it correctly even once. That is, the true conditional
generative process ğ‘(y |y ) could have arbitrarily high computational complexity even when it is
ğ‘¡+1 ğ‘¡
deterministic as in the example above. In contrast, if we instruct models to â€œthink step by stepâ€ and
produce a chain-of-thought we can significantly improve their capabilities producing traces like the
following:
4

<!-- Page 5 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
We are tasked with evaluating
(ğ‘¥2âˆ’1)(ğ‘¥+1) 1
âˆ’
ğ‘¥3âˆ’ğ‘¥ ğ‘¥
at ğ‘¥ = ğœ‹. Let us simplify the expression before substituting ğ‘¥ = ğœ‹.

### Step 1: Simplify (ğ‘¥2âˆ’1)(ğ‘¥+1)

The numerator of the first term can be rewritten:
ğ‘¥2âˆ’1 = (ğ‘¥âˆ’1)(ğ‘¥+1),
so
(ğ‘¥2âˆ’1)(ğ‘¥+1) = (ğ‘¥âˆ’1)(ğ‘¥+1)2.
Step 2: Simplify ğ‘¥3âˆ’ğ‘¥ in the denominator

### Factorize ğ‘¥3âˆ’ğ‘¥:

ğ‘¥3âˆ’ğ‘¥ = ğ‘¥(ğ‘¥2âˆ’1) = ğ‘¥(ğ‘¥âˆ’1)(ğ‘¥+1).
Thus, the first term becomes:
(ğ‘¥2âˆ’1)(ğ‘¥+1) (ğ‘¥âˆ’1)(ğ‘¥+1)2
= .
ğ‘¥3âˆ’ğ‘¥ ğ‘¥(ğ‘¥âˆ’1)(ğ‘¥+1)
Cancel (ğ‘¥âˆ’1) and one factor of (ğ‘¥+1):
(ğ‘¥2âˆ’1)(ğ‘¥+1) ğ‘¥+1
= .
ğ‘¥3âˆ’ğ‘¥ ğ‘¥
Step 3: Subtract 1
ğ‘¥
Now substitute this simplified result into the full expression:
ğ‘¥+1 1
âˆ’ .
ğ‘¥ ğ‘¥
Combine the terms over the common denominator ğ‘¥:
ğ‘¥+1 1 ğ‘¥+1âˆ’1 ğ‘¥
âˆ’ = = = 1.
ğ‘¥ ğ‘¥ ğ‘¥ ğ‘¥
Step 4: Evaluate at ğ‘¥ = ğœ‹ Theexpressionsimplifiesto1,whichisindependentofğ‘¥. Thus,the
value at ğ‘¥ = ğœ‹ is:
1 .
There have been multiple explanations for this phenomenon (Merrill & Sabharwal, 2023; Prystawski
et al., 2024; Ye et al., 2024a; Dutta et al., 2024), however we focus on the complexity hypothesis (Merrill & Sabharwal, 2023). Specifically, a CoT expansion allows for (in-theory) arbitrarily large
amounts of compute to be deployed towards the prediction of answer tokens. A large theoretical
literature has been developed arguing for the representational complexity of transformers with and
withoutCoT(Merrill&Sabharwal,2023;Nowaketal.,2024;Lietal.,2024). Theliteraturesaysthat
endowing LLMs with CoT allows them to represent new orders of complexity and (with assumptions,
such as infinite memory) may even make them Turing complete. However, in practice, todayâ€™s LLMs
can only reliably solve problems of limited complexity (Snell et al., 2024).
5

<!-- Page 6 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought

### Outline

In this paper, we investigate the limitations of current LLMs in handling complex reasoning tasks and
propose a novel framework, Meta Chain-of-Thought (Meta-CoT), to address these shortcomings. We
argue that traditional Chain-of-Thought (CoT) methods, while effective for simpler problems, fail
to capture the true data-generating process of complex reasoning which often involves a non-linear,
iterative, and latent process of exploration and verification. Meta-CoT extends CoT by explicitly
modeling this latent â€œthinkingâ€ process, which we hypothesize is essential for solving problems that
require advanced reasoning capabilities.
We draw inspiration from Cognitive Scienceâ€™s dual-process theory, framing Meta-CoT as a form of
System 2 reasoning. We establish the theoretical foundations of Meta-CoT, demonstrating how it can
be realized through systematic search processes, and how these processes can be internalized within
a single auto-regressive model. We then present empirical evidence supporting our claims, including
analyses on state-of-the-art models like OpenAIâ€™s o1 (OpenAI, 2024) and DeepSeek-R1 (DeepSeek,
2024), which exhibit behaviors consistent with internalized (in-context) search. We further explore
methodsfortrainingmodelsonMeta-CoTthroughprocesssupervision,andsyntheticdatageneration
via search algorithms like Monte Carlo Tree Search (MCTS) and A*.
Finally, we outline a concrete pipeline for achieving Meta-CoT in a single end-to-end system,
incorporating instruction tuning with linearized search traces and reinforcement learning (RL) posttraining. We discuss open research questions, including the scaling laws of reasoning and search, the
roleofverifiers,andthepotentialfordiscoveringnovelreasoningalgorithmsthroughmeta-RL.Wealso
present the â€œBig MATHâ€ project, an effort to aggregate over 1,000,000 high-quality, verifiable math
problems to facilitate further research in this area. Our work provides both theoretical insights and a
practical road map to enable Meta-CoT in LLMs, paving the way for more powerful and human-like
reasoning in artificial intelligence.

## Meta Chain-Of-Thought

In this section, we first formulate the meta chain-of-thought process and discuss how it can describe
the problem solving process for complex reasoning problems. Then, we describe and demonstrate
why classical chain-of-thought fails under certain circumstances.

### Deriving The Meta-CoT Process

A question to ask ourselves is: Should language models with Chain-Of-Thought prompting really be
able to express any function, and thus solve arbitrarily complex problems, which was the theoretical
pointoftheprevioussection? Wewillstickwiththemathematicalreasoningdomainforthepurposeof
the discussion. Today, the capabilities of frontier models are enough for a large class of mathematical
reasoning problems. Current state-of-the art systems such as GPT-4o and Claude largely solve the
Hendrycks MATH Levels 1-3 Benchmark (Hendrycks et al., 2021), however, they still struggle with
advanced problems such as those in Levels 4 and 5, HARP (Yue et al., 2024) and Omni-MATH (Gao
et al., 2024) (as well as other advanced reasoning tasks). We put forward the following theory to
explain these empirical observations.
Reasoning data present in pre-training corpuses does not represent the true data generation
process, especially for complex problems, which is a product of extensive latent reasoning.
Moreover, this process generally does not occur in a left-to-right, auto-regressive, fashion.
6

<!-- Page 7 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
In more details, the CoT reasoning data prevalent in the pre-training corpus and post-training
instruction tuning follows the true data-generating process for solutions of simple problems such as
algebraic computations, counting, basic geometry etc.. That is, for example, the textbook solutions
forsolvinghigh-school algebrapresent thegeneral processofgeneratingthose solutions. Ifwefollow
somesetofstepsorapproachespresentinexistingtextbooks,wecaneventuallyarriveatthesolution.
Hence,thesearelearnablewithaconstant-depthtransformersthatcanexpressthecomplexityofeach
individual step in the process. In contrast, complex reasoning problems do not follow that pattern.
We may have a set of triples (q,S,a) of questions q, solution steps S = (s ,...,s ) and (optionally)
1 ğ‘›
answers a, but the true data generation process is not auto-regressive:
q â†’ z â†’ ... â†’ z â†’ (s ,...,s ,a), (1)
1 ğ¾ 1 ğ‘›
where z are the latent "thoughts" left out of the solutions steps, which can be represented fully
ğ‘–
with left-to-right generation, while the dataset solution S = (s ,...,s ) is generated jointly. Let
1 ğ‘›
us illustrate this with an example from the International Mathematics Olympiad 2011. This is the
(in)famous "windmill" problem:
â€œLet ğ’® be a finite set of at least two points in the plane. Assume that no three points of ğ’® are
collinear. A windmill is a process that starts with a line â„“ going through a single point ğ‘ƒ âˆˆ ğ’®.
The line rotates clockwise about the pivot ğ‘ƒ until the first time that the line meets some other
point belonging to ğ’®. This point, ğ‘„, takes over as the new pivot, and the line now rotates
clockwise about ğ‘„, until it next meets a point of ğ’®. This process continues indefinitely. Can we
choose a point ğ‘ƒ in ğ’® and a line â„“ going through ğ‘ƒ such that the resulting windmill uses each
point of ğ’® as a pivot infinitely many times.â€
which has the following solution:
â€œLet|ğ‘†| = ğ‘›. Nowconsideranarbitrarypointğ‘ƒ inğ‘† andchoosealineğ‘™ throughğ‘ƒ whichsplits
the points in the plane into roughly equal chunks. Next notice that as the line rotates it will
sweep a full 2ğœ‹ angle against some fixed reference frame. Now take another random point
ğ‘ƒâ€² and similarly constructed stationary line ğ‘™â€². At some point in the windmill process we will
have ğ‘™||ğ‘™â€². However notice that ğ‘™ and ğ‘™â€² split the points into the same two sets and are parallel.
Therefore we must have that ğ‘™ â‰¡ ğ‘™â€² and thus ğ‘™ passes through ğ‘ƒâ€². This of course holds for any
ğ‘ƒâ€² âˆˆ ğ‘†. Applying the same argument recursively yields the final proof that it is in fact possible
to make such a construction for any set ğ‘† with these properties.â€
The solution above does not use any prior knowledge and fits within a few sentences. Yet, this
problem was considered among the most difficult in the competition (there were only a handful of
solutions among the 600+ participants). What makes the problem difficult is the highly non-linear
structure of the solution. Most participants would follow the standard "generative" solution process
andinvestigateapproachesbasedontheconvexhullconstructionorusetoolsfromHamiltoniangraph
theory, none of these leading to a solution. Instead, participants who solved the problem followed an
experimental approach with a lot of geometric exploration and inductive reasoning. Moreover, the
solution itself is not linear. Itâ€™s hard to see the utility of the proposed construction in the beginning
without the analysis of the dynamics of ğ‘™. Essentially, to start generating the solution requires
that we already know the full approach. The underlying generative process of the solution is not
auto-regressive from left-to-right.
We can formalize this argument through the interpretation of reasoning as a latent variable
process (Phan et al., 2023). In particular, classical CoT can be viewed as
7

<!-- Page 8 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Figure 1: Top: Performance of current frontier models by size on the HARP mathematics benchmark (Yue et al., 2024) by difficulty level and topic. The OpenAI O1 series significantly out-performs
prior generation models across the board. Source: Figure 3 in (Yue et al., 2024). Bottom Average
number of tokens generated by each model grouped by difficulty level, as well as average number of
tokens in human-generated solutions (using GPT4 tokenizer). Source: Figure 4 in (Yue et al., 2024).
âˆ«ï¸ ğ‘›
âˆï¸
ğ‘data(a|q) âˆ ğ‘data(a|s
1
,...,s
ğ‘›
,q) ğ‘data(s
ğ‘¡
|s
<ğ‘¡
,q)ğ‘‘S,
âŸ â
AnswerGeneration ğ‘¡=1
âŸ â

### CoT

i.e., the probability of the final answer being produced by a marginalization over latent reasoning
chains. Weclaimthatforcomplexproblems,thetruesolutiongeneratingprocessshouldbeviewedas

## âˆ«ï¸ ğ¾

âˆï¸
ğ‘data(a,s
1
,...,s
ğ‘›
|q) âˆ ğ‘data(a,s
1
,...,s
ğ‘›
|z
1
,...,z
ğ‘˜
,q) ğ‘data(z
ğ‘¡
|z
<ğ‘¡
,q)ğ‘‘Z,
âŸ â
JointAnswer+CoT ğ‘¡=1
âŸ â

### Meta-CoT

i.e., the joint probability distribution of the solution (a,s ,...,s ) is conditioned on the latent
1 ğ‘›
generative process. Notice that this argument is a meta-generalization of the prior CoT argument,
hence why we will refer to the process q â†’ z â†’ ... â†’ z as Meta-CoT.

## 1 ğ¾

8

<!-- Page 9 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought

### Why Does (Classical) CoT Fail?

Based on the previous discussion, a natural question follows: Why do LLMs fail at these advanced
reasoning tasks? Above we proposed that the pre-training and instruction-tuning corpora consist of
data of the type (q,s ,...,s ,a), which do not contain the true data generating process as shown in
1 n
Equation 1. Indeed, the solution to the windmill problem above is widely available on the internet,
but there is little to no discussion about the ways in which commonly used convex hull or planar
graph arguments fail. This is true in general - textbooks contain advanced proofs but not the full
thought process of deriving these proofs. We can then apply the same general meta-argument of why
CoT is necessary to the Meta-CoT case: simply because the conditional solution-level distribution
ğ‘data(a,s
1
,...,s
ğ‘›
|q) (without the intermediate Meta-CoT) on hard reasoning questions can have
arbitrarily high complexity in the same way that ğ‘data(a|q) can have arbitrarily high complexity in
the standard CoT setting. We will examine some empirical evidence for our stance in the following
sections.
We will argue in the following chapters that the OpenAI o1 model series performs full Meta-CoT
reasoning in an auto-regressive fashion at inference time. A useful analysis is presented in a new
mathematics benchmark with challenging high-school Olympiad-level problems (Yue et al., 2024).
Figure1sourcedfromthatworkshowstherelevantresults. First,weseethattheo1familyofmodels
significantlyoutperformsâ€œstandardâ€reasoningmodelsacrosstheboard. However,thegapbetweeno1
andothermodelsâ€™performanceincreasesonhigherdifficultyproblems(withtheinterestingexception
of the LLaMa 3.1 models), that is, problems which have higher solution complexity.
Furthermore, the bottom half of Figure 1 shows the average number of tokens generated grouped
byproblemdifficultylevel. First,weseethatoutsideoftheo1seriesofmodels,LLMsgeneratesolutions
of comparable lengths to humans. While this may initially appear quite intriguing, suggesting that
models are learning to approximate or replicate human reasoning, the simple explanation is that
models are learning solutions to match the training data - i.e. ğ‘data(a,s
1
,...,s
ğ‘›
|q). Much more
intriguingly, the o1 series of models exhibits significantly different token behavior. We see that:

## On level 1 problems the o1 series generates a comparable number of tokens to human-written

solutions. These are the types of problems where the training solutions likely match the true
datagenerationprocessandeachindividuallogicalstepcanbeinternalizedinaconstant-depth
transformer.

## Athigherdifficulty,theo1seriesofmodelsgeneratessignificantlymoretokensperproblemand

also widens the performance gap over the classical reasoning models. In fact the gap between
the inference compute used by the o1 model and prior series of models seems to scale with
the complexity of the problems. We hypothesize that in those more challenging problems the
solutions do NOT in fact represent the true data generative process, which is instead better
approximated by the more extensive Meta-CoT generated by the o1 family of models.
Ofcourse,inpracticethedistinctionbetweenthesetwoisnotsoclearcut,andinfacttheconstantdepth transformer can likely internalize part of the Meta-CoT generative process as evidenced by the
gradation of (Meta-)CoT lengths from Levels 2-6 in Figure 1. In the next chapter we will discuss in
greater detail what the Meta-CoT process actually represents.

## Towards Deliberate Reasoning With Language Models - Search

In the previous section we introduced the Meta-CoT process and argued that LLMs fail on advanced
reasoningtasksbecausethetrainingdatadoesnotadequatelyrepresentthetruedatageneration
9

<!-- Page 10 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
0.5
0.4
0.3
0.2
0 100000 200000 300000 400000 500000 600000 700000
Number of Examples
ycaruccA

## Htam

0.8
0.7
0.6
0.5
0.4
Majority@64
Majority@8
0.3

### Greedy


### Base Filter


### Original 0.2

0 100000 200000 300000 400000 500000 600000 700000
Number of Examples
ycaruccA

## Htam

Pass@2 Pass@16
Pass@4 Pass@32

### Pass@8 Pass@64

Figure 2: Train-time scaling curves for increasing quantities of training data during supervised
fine-tuning of Llama3.1 8B, when evaluating for accuracy on the MATH test set. Left: A comparison
of training on on the original data (x) and base filtered data (o) and evaluated using either greedy
or majority vote decoding. For all settings, the filtered dataset shows improved scaling, and has not
plateaued. Right: A comparison of pass@k for increasing ğ‘˜ with a model trained on the base filtered
dataset. The chart shows how increasing ğ‘˜ leads to a much greater probability of at least 1 correct
solution, even for a relatively small model. Additionally, the performance improvement from ğ‘˜ = 32
to64doesnotshowplateauing,suggestingthatğ‘˜ canfurtherbeincreasedforimprovedperformance.
process, i.e. text corpora do not include (or only include limited amounts of) Meta-CoT data. So the
remaining question is: what does the true data generating process look like?

## First, we argue that for many advanced reasoning or goal-oriented problems there exist meaningful gaps between the complexity of generation and verification. This is of course one of the

fundamental open problems of theoretical computer science and any attempt to prove this is
significantly beyond the scope of the current writing, but we will review what we believe to be
compelling empirical evidence from the literature.

## Second,assuminganon-trivialgenerator-verifiergap,wearguethatthesolutionstochallenging

problemspresentintextcorporaaretheoutcomesofanextended search process,whichitself
is not represented in the data.
We will dive deeper into these two points in the remainder of this section.

### Inference-Time Compute: Search

The first point above (generation-verification gap) has recently become a popular research and
discussion direction under the framework of â€œdeploying inference-time computeâ€ and we explore this
in our first experiment. We start with a LLaMa 3.1 8B base model (Dubey et al., 2024) and carry out
extensive supervised fine-tuning on the Numina MATH dataset (LI et al., 2024). Refer to Figure 2 for
resultsandSection8.1fordatasetdetails. Foreachintermediatecheckpointweevaluateperformance
on the Hendrycks MATH (Hendrycks et al., 2021) 500 problems evaluation dataset (Lightman et al.,
2023). Based on the results, we make a few observations here:

## We evaluate pass@ğ‘˜ (i.e. using an oracle verifier) on intermediate checkpoints and see a

significant jump in performance for increasing ğ‘˜. While zero-shot performance with greedy
10

<!-- Page 11 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Figure 3: Scaling trends for verifier models on algorithmic reasoning, grade-school math (GSM8k),
and transfer from GSM8k to MATH. The performance of all verifiers improves in the best-of-N setting,
as N increases. Figure sourced from (Zhang et al., 2024a).
decoding improves from about 20% to 40% (see the base filter on the left side of Figure 2),
even the first model checkpoint outperforms these results at pass@4 (right side of Figure 2).
Moreover, the pass@64 for the final checkpoint of an 8B model achieves accuracy close to 85%,
outperforming the zero-shot performance of many current frontier models.

## Wealsoevaluateperformanceundermajorityvotingwithğ‘˜ = 8andğ‘˜ = 64. Thereiscontinuous

improvement for both increased training and samples, with maj@64 outperforming the greedy
model performance with only 15% of the training compute without access to a ground-truth
verifier.
These results demonstrate that even as we directly optimize for answer generation ability by
finetuning on increasing amounts of SFT data, there remains a consistent verifier-generator gap, as
evidenced by the improved performance in botht eh pass@k and majority voting settings. Recent
literature has observed similar results on post-training sampling (Lightman et al., 2023; Brown
et al., 2024; Snell et al., 2024). However, most of these studies do not systematically evaluate the
effects of varying amounts of training data, compute, and model size which we believe is a fruitful
direction for additional empirical work. These questions are important as the observed gains from
additional inference might disappear at larger scales and training - i.e. the model may be able to
fully internalize the reasoning process. This definitely seems to be the case for advanced models
and simpler benchmarks like GSM8k (Cobbe et al., 2021). While we observe the opposite result in
our experiments, we admit that our results are the outcomes of preliminary study and additional
work is required, but we will argue from a theoretical point in Section 6 that a persistent search gap
remains in domains with high enough epistemic uncertainty. Besides this point, the question remains
whethertheimprovementfromincreasedinferencecanbeeffectivelyachievedwithoutoracleverifiers
or environment feedback. In theory, it is possible to generate correct solutions under an increased
inference budget, but we may not be able to verify them effectively, as verification complexity may be
just as high as, or even higher than, generation complexity. We will address this issue next.

### Inference-Time Compute: Verification

Severalworksfocusontrainingverifiermodels,whichexplicitlyevaluatethecorrectnessofreasoning
steps and solutions. Verifiers can be trained either using explicit binary classification (Cobbe et al.,
2021; Lightman et al., 2023; Snell et al., 2024; Anonymous, 2024; Setlur et al., 2024b) or modeling
11

<!-- Page 12 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Figure 4: Reasoning via Planning (RAP) demonstrates the search procedure described here. If we
haveaccesstoastateevaluator,wecantruncatebrancheswithlowvaluesandbacktracktopromising
nodes, without resampling the same steps again. Source: Figure 2 in (Hao et al., 2023).
evaluation directly in natural language, using the LLM-as-a-judge prior (Zhang et al., 2024a; Mahan
et al., 2024). The unifying formulation of these approaches is the model ğ‘£ which evaluates a
ğœƒ
reasoning process ğ‘£ (q,S) â†’ [0,1]. Under this framework, ğ¾ candidate solutions (S1,...,Sğ¾) can
ğœƒ
be generated from a fixed generator ğœ‹ (Â·|q) and ranked based on their evaluation score.
ğœƒ
S* = argmax{ğ‘£ (q,S1),...,ğ‘£ (q,Sğ¾)}
ğœƒ ğœƒ
For empirical results, we refer the reader to Figure 3 sourced from (Zhang et al., 2024a) which
evaluates a number of verifier models ğ‘£ . Regardless of the efficiency of the verifier, there is a
ğœƒ
significant improvement in performance with additional online sampling. Moreover using explicitly
trainedverifiermodelsoutperformsnaiveinference-computescalingstrategiessuchasself-consistency
or majority voting.
A question remains regarding the effect of using a fixed generation model (policy): Could this
modelbeunder-trained,andifitwerefurthertrained,coulditszero-shotperformanceimprovetothe
point where additional online search no longer provides meaningful improvement? We will address
this in Section 3.4.

### From Best-of-N To General Search

Sofar,weempiricallyexploredbest-of-Napproaches,generatingmultiplefullsolutionsindependently
andselectingthemostpromisingonebasedonscores. However,thisapproachisinefficientbecauseit
requires exploring full solution paths, even if a mistake occurs early on, and may repeatedly sample
thesamecorrectsteps. Instead,wecanmodelreasoningasaMarkovDecisionProcess(MDP),defined
by the tuple â„³ = (ğ’®,ğ’œ,ğ‘ƒ,ğ‘…,ğ›¾), where:
â€¢ ğ’®: the set of states, where each state S âˆˆ ğ’®, consists of the prompt and generations so far, i.e.
S = (q,s ,...,s ).
ğ‘¡ 1 ğ‘¡
â€¢ ğ’œ: the set of actions, where each action a âˆˆ ğ’œ will be represented as the next reasoning step
a = s .
ğ‘¡+1 ğ‘¡+1
â€¢ ğ‘ƒ(sâ€² | s,a): the transition probability function, representing the probability of transitioning
to state sâ€² when taking action a in state s. For simplicity, we will mostly consider the deter-
12

<!-- Page 13 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
ministictransitionfunctionğ‘ƒ(Â·|s ,(q,s ,...,s )) â†’ (q,s ,...,s ,s )thatappendsthenext
ğ‘¡+1 1 ğ‘¡ 1 ğ‘¡ ğ‘¡+1
reasoning step to the context. In general, the environment dynamics can be more complex.
For example, models with tool access have to call the actual tool and receive the environment
feedbackincontextorevenmodifytheirenvironmentsuchasthecasesofSWEandWebagents.
â€¢ ğ‘…(s,a): the reward function, which provides a scalar reward for taking action a in state s.
We will assume zero intermediate rewards and final reward of 1 for a correct solution and
zero otherwise, although this is not strictly necessary in the presence of a good process reward
model (Setlur et al., 2024c).
â€¢ ğ›¾ âˆˆ [0,1]: thediscountfactor,balancingthetrade-offbetweenfurthercomputationandrewards.
We refer to the LLM generating the reasoning steps as the policy s âˆ¼ ğœ‹ (Â·|S ). In addition we
ğ‘¡+1 ğœƒ ğ‘¡
refer to a solution starting from s = q as an episode or a trajectory. We will also use the notation z
0 ğ‘¡
to represent individual reasoning steps that are part of the Meta-CoT and correspondingly denote
Z = (q,z ,...,z ).
ğ‘¡ 1 ğ‘¡
Inthepriorsectionweconsideredgeneratingandrankingfullsolutions,whichmaybeinefficient.
We can extend the concept of a solution verifier from the prior section, to estimating the probability
that a particular intermediate state will lead to a solution: ğ‘£ (q,S ) â†’ [0,1]. These models have
ğœƒ ğ‘¡
become more widely known as Process Reward Models (PRMs) (Lightman et al., 2023). If we have
access to such a model, we can improve the efficiency of the search process with the following steps:

## Terminate a solution attempt that is not making progress, or is incorrect prior to reaching the

final answer.

## Reset the agent to any intermediate, previously visited, state that has a high likelihood of

success.
Notice that with these two operations, and the general structure of language, we can implement any tree search procedure. This is the premise of several approaches (Yao et al., 2023; Hao
et al., 2023; Zhou et al., 2024a) with the RAP method (Hao et al., 2023) illustrated in Figure 4.
Theseapproachesusedifferingsearchstrategies(DF-

### S/BFS vs. MCTS) and process guidance evaluation

(generative self-evaluation vs. Monte-Carlo rollouts),
but they all share the same core idea: formulate the
reasoning problem as tree search guided by an intermediate heuristic function. As noted above, in
theory, tree search does not induce a fundamental
capability shift over parallel sampling, however, it
may induce significant efficiency gains as demonstrated by Yao et al. (2023). In particular, Figure 5
(sourcedfromYaoetal.(2023))showsnearly4times
increased efficiency, in terms of inference budget,
on a toy reasoning problem (Game of 24) when using a tree-structured search approach compared to
parallel sampling. While these earlier works focus
on zero-shot (or close to zero-shot) performance on
simple reasoning tasks, it is important to note that
tree-search methods have been successfully scaled Figure 5: ToT efficiency on the game of 24
shownasaccuracy(y-axis)vs. #nodesvisited
and deployed to a number of realistic agentic appli-
(x-axis). Source: Figure3inYaoetal.(2023).
cations (Koh et al., 2024; Putta et al., 2024; Brown
et al., 2024; Yu et al., 2024).
13

<!-- Page 14 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Figure 6: Scaling trends for MCTS at training and inference-time on board games. Left: Elo
scores of models trained with different architectures (depth Ã— width) where each point represents
theEloscoreofthatmodelevaluatedwiththelabeledtreesize(between1to512nodes). Thecurves
demonstrate that the performance of each model snapshot follows a sigmoid pattern with respect
to the test-time compute budget. Source: Figure 8 in (Jones, 2021). Right: The trade-off between
train-time and test-time compute, with progressively improving Elo (from bottom-left to top-right).
Source: Figure 9 in (Jones, 2021).

### Is Search (Inference Time Compute) A Fundamental Capability Shift?

Aspointedoutearlier,thequestionremainswhetherinference-timesearchisafundamentalnewcapability or whether it is accessible with additional training. Results from classical RLHF tuning (Dubois
et al., 2024) suggest that this is a learnable capability, where zero-shot performance of post-trained
models matches or outperforms the best-of-N paradigm.
We stipulate that performance on complex reasoning tasks is governed by a scaling law, which
involves model size, training data (compute) and inference time compute.
ThisisindeedconsistentwiththetheoreticalresultsofLietal.(2024)andtheintuitionpresented
in Section 2. Larger models are more capable of internalizing the Meta-CoT process in their activations, and are also capable of using longer inference-time Meta-CoT to approximate solutions with
significantly higher computational complexity. Empirically, we have limited (but promising) evidence
towards this hypothesis. A major prior work to study these questions is Jones (2021) which carries
out studies using the AlphaZero algorithm (Silveretal.,2018)onboardgames. Thisapproach

### Method Policy Value Accuracy(%)

fits our desiderata very well as the underlying ğœ‹ - 41.4

### Greedy ğœƒ0

MCTS algorithm jointly scales the policy and ğœ‹ - 47.9
ğœƒ1
value (verifier) modelsâ€™ training in conjunction ğœ‹ {ğ‘£,ğ‘ŸË†} 51.9
with search. Moreover, this family of board ğœ‹
ğœƒ0
{ğ‘£,ğ‘ŸË†}
ğœ‘0
53.2
gameshaveacleargenerator-verifiergapasgen-
MCTS-ğ›¼
ğœ‹
ğœƒ0
{ğ‘£,ğ‘ŸË†}
ğœ‘1
54.1
erating optimal strategies at intermediate steps
ğœƒ1 ğœ‘0
ğœ‹ {ğ‘£,ğ‘ŸË†} 56.5
can be quite computationally complex, while
ğœƒ1 ğœ‘1
verifyingawinningconditionistrivial. Thema- Table 1: Iterative update results on GSM8k. ğœƒ ,ğœ‘
0 0
jor empirical results on scaling are shown in are the old parameters while ğœƒ ,ğœ‘ are the new
1 1
Figure 6. On the right side we see that perfor- ones. TS-LLM can boost performance by training
mance increases both with increased training LLMpolicy,value,orboth. Source: Table4in(Feng
compute and model size, as outlined earlier. In- et al., 2024).
terestingly, on the left we see the performance
14

<!-- Page 15 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
of using different quantities of compute (i.e., search with a value function) during inference. There
is also a clear scaling trend, showing improved performance with additional online search at each
intermediate checkpoint of training. In fact, the results in this domain indicate there is a clear
log-log scaling trade-off between train-time and test-time compute deployment. Currently, we have
limited evidence of similar scaling laws in LLMs because such a training pipeline requires significant
resources. One major work towards that goal is Feng et al. (2024) which carries out two iterations of
MCTSfine-tuningusingaLLaMa7BontheGSM8kdataset(Cobbeetal.,2021). Theyshowimproved
performance in zero-shot evaluations of the policy, as well as significant gains from using additional
inference-time search, at both iterations 1 and 2 (full results are shown in Table 1). However, their
work does not ablate the model size, data scaling, or inference-time search scaling, which remain
under-explored in the literature for LLM reasoning.

## Towards Meta-CoT Reasoning

Inpriorsectionswe: introducedtheconceptofMeta-CoTandarguedthatitisnecessaryforadvanced
reasoning, discussed the generator-verifier gap as a fundamental limitation, argued for search as
a fundamental building block of the Meta-CoT, and discussed the utility of approaches integrating
generator, verifier, and search components. However, the question remains on how to integrate these
into a model to perform Meta-CoT or â€œSystem 2â€ reasoning. The first question we need to answer is:
why do we actually need to internalize deliberate reasoning inside a single model? We propose two
main reasons:

## Efficiency: By incorporating search within the context of an auto-regressive model, exploration

can be done efficiently since the model has access to all previously visited nodes, in context.
Unique to the case of reasoning in natural language, many branches may contain semantically
similar content, unlike other domains (e.g., board games), motivating the need for improved
efficiency. Infact,evenadvancedreasoningmodelscarryoutmanyrepeatedstepsofsemantically
identical reasoning as we show in Figure 14 and 15.

## Super-Intelligence: If an auto-regressive model can learn to implement search algorithms

in-context, then additional RL training may enable the model to discover novel reasoning
approaches. Essentially,weproposethattrainingamodelcapableofinternalSystem2reasoning
(e.g. Meta-CoT) and search is an optimization over algorithms rather than specific outputs,
possibly yielding novel modes of problem solving. This will potentially allow the model to solve
classesofproblemspreviouslyunsolvableundersymbolic-basestree-searchapproachesasweâ€™ve
outlined in Sections 3.3 and 3.4.
In the remainder of this section, we explore how to train a model to internalize such a reasoning
system.

### Bootstrapping Meta-CoT

In this subsection, we overview the core idea behind the Self-Taught Reasoner (STaR) approach (Zelikman et al., 2022; Singh et al., 2024; Yuan et al., 2023) to bootstrapping intermediate CoT steps
and how to use a similar concept to generalize to meta-reasoning strategies.

#### Self-Taught Reasoner

The STaR method introduces an iterative bootstrapping approach designed to improve the reasoning
capability of LLMs (Zelikman et al., 2022). STaR focuses on training models to generate and refine
15

<!-- Page 16 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
rationales, particularly for tasks requiring complex reasoning in a reinforcement learning-based
manner. In this formulation we assume we have access to a dataset ğ’Ÿ = {q(ğ‘–),a(ğ‘–)}ğ‘ of questions q
ğ‘–=1
that require reasoning along with corresponding answers a. Notice that we do not require access to
ground-truth rationales for these problems. We begin by prompting a model aË†(ğ‘–),SË†(ğ‘–) âˆ¼ ğœ‹(a,S|q(ğ‘–))
to provide CoT rationale SË†(ğ‘–) = s (ğ‘–) ,...,s (ğ‘–) and final answer aË†(ğ‘–). We then filter the generated data,
1 ğ‘ğ‘–
keeping only rationales that lead to a correct final answer (i.e., aË†(ğ‘–) = a(ğ‘–)) to create a dataset of
questions, (bootstrapped) rationales and answers ğ’ŸSTaR = {q(ğ‘–),SË†(ğ‘–),a(ğ‘–)}ğ‘
ğ‘–=1
. ğ’ŸSTaR is then used to
train a model with the standard supervised fine-tuning objective:
[ï¸ ]ï¸
â„’ (ğœ‹ ) = âˆ’E âˆ’logğœ‹ (a,SË†|q) . (2)

### STaR ğœ‘ (q,S^,a)âˆ¼ğ’ŸSTaR ğœ‘

The above procedure is repeated over several iterations. The core idea behind STaR is to generate a
trainingdatasetofsyntheticrationalesthroughsamplingandverification. Wewillextendthatideato
the the concept of Meta-CoT below.

#### Meta-STaR

We can generalize the above idea to Meta-CoT in a straightforward way. Consider a base policy
ğœ‹ combined with some general search procedure over intermediate steps. Given a question q we
ğœƒ
perform the search procedure repeatedly to generate search traces zË† ,...,zË† until we find a final

## 1 ğ¾

solution (s ,...,s ). If we can verify the final produced solution ğ‘£(S) â†’ {0,1}, for example by using
1 ğ‘›
a formalization and verification approach (as in AlphaProof1) or some other outcome verification,
we can then apply a similar approach to STaR. For example, we can construct a dataset ğ’ŸSTaR =
{q(ğ‘–),ZË†(ğ‘–),SË†(ğ‘–)}ğ‘ and use a similar training objective as before:
ğ‘–=1
[ï¸ ]ï¸
â„’ (ğœ‹ ) = âˆ’E âˆ’logğœ‹ (SË†,ZË†|q) . (3)

### Metaâˆ’STaR ğœ‘ (q,Z^,S^)âˆ¼ğ’ŸSTaR ğœ‘

Essentially,wecanuseabasepolicyandsearchproceduretogeneratesyntheticsearchdataandthen
train the model to implement these in-context through the Meta-CoT concept. We are effectively
proposing to linearize the search approaches described in Section 3 and teach an auto-regressive
model to run them sequentially. So far we have deliberately been vague about how these search
proceduresanddatasetslook. Wewillnowprovideexamplesandproofofconceptfromtheliterature
on practical approaches to this problem as well as synthetic examples of realistic training data.

### Empirical Examples Of Internalizing Search

When we formulate search in a sequential fashion we can explicitly parameterize each component in
language, or choose leave it implicit (Gandhi et al., 2024). Note that models trained with standard
next token prediction still need to implicitly internalize all of these components anyway in order to
accurately model the search sequence, even if they are not explicitly verbalized. However, allowing
themodeltovocalizeitâ€™scertaintyorestimatedprogresscouldallowforadditionalmodelingcapacity
or be useful for interpretability purposes. We will present some examples of auto-regressive search
procedures from the literature in the following section.

#### Small-Scale Empirical Results on Internalizing Search

Twoparticularpriorworksthatexploretheideaofin-contextsearchareYangetal.(2022)andLehnert
etal.(2024)whichfocusonmazesandotherclassicalRLenvironments. TheformulationfromLehnert
1https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/
16

<!-- Page 17 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Figure 7: A* planning algorithm outline for a simple maze navigation task, along with a state and
action tokenization scheme. The search representation explicitly models nodes and queue state, the
search procedure and the cost and heuristic evaluation. Source: Figure 1 in (Lehnert et al., 2024).
Figure 8: Model performance vs. training compute when using the A* planning algorithm (Search
Augmented) vs. no search (Solution Only). We see that the search augmented models perform much
better across all training scales (charts a and b). In particular performance is consistent with the
search formulation of Section 3.4. Figure c) shows performance in terms of task complexity as maze
size increases. Results are consistent with the Meta-CoT complexity argument presented in Section 2
and results on the HARP benchmark in Figure 1. Source: Figure 2 in (Lehnert et al., 2024).
17

<!-- Page 18 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Figure 9: Inference compute scaling relationships for the o1 model (Left, sourced from (OpenAI,
2024) on AIME, Stream-of-Search on the Game of 24 (Middle) and MAV-MCTS on Chess (Right,
sourcedfrom(Schultzetal.,2024)). Thesefiguresshowperformanceofasinglemodelunderdifferent
token sampling budgets.
et al. (2024) is shown in Figure 7, which illustrates linearizing A* search. In our framework the
â€œTraceâ€ corresponds to the Meta-CoT Z, and the â€œPlanâ€ is the CoT output S. In this setting the
search procedure is stated explicitly as it shows node states, actions, costs and heuristic values.
In this â€œstreamâ€ format we can then use standard auto-regressive language models with a next
token-prediction objective to train a model to internalize the search process. Evaluation results
are shown in Figure 8 sourced from the same paper. We observe empirical effects consistent with
the scaling law hypothesis presented in 3.4; there is consistent improvement with additional
training data and model size (train-time compute) across the board. A particularly interesting
observation is the complexity scaling relationship in part (c) of the figure. At smaller mazes (lower
complexity) the model directly producing the Plan (CoT) and performs comparably to smaller search
(Meta-CoT) augmented models, however as maze size (complexity) increases we see a widening gap
in performance between the search-augmented and zero-shot models. This is essentially identical
to the results shown in Figure 1 on the challenging HARP benchmark (Yue et al., 2024) between
the prior frontier models and the o1 series. These empirical observations are well aligned with the
intuition we presented in Section 2. For small mazes (low complexity problems) models are capable
of internalizing the reasoning process, but as problem complexity (maze size) increases this becomes
more challenging and model performance falls off compared to models which explicitly carries out a
search procedure. Unfortunately, Lehnert et al. (2024) did not publish inference compute scaling
laws, but given the algorithmic structure of the training data we can presume that inference-time
tokens scale with the same complexity as the A* search algorithm, which can be exponential in the
branching factor, while the plan length is linear in ğ‘›. These results would also be consistent with the
inference costs on advanced math reasoning tasks reported in Figure 1.
Gandhi et al. (2024) extend the linearized search idea to a more realistic reasoning task - the
Countdown game - which requires the model to predict a sequence of mathematical operations on a
givensetofnumberstomatchatargetvalue. WhileGandhietal.(2024)useafixed250Mparameter
transformermodelanddonotexploreordiscusstheroleofmodelsize,trainingdata,andcomplexity
intermsofscalingperformance,weobtainadditionalresultsintermsofinference-timescaling,shown
in Figure 9. Our findings demonstrate a consistent log-linear relationship between tokens spent and
success rate. Similar results were also observed in recent work by Schultz et al. (2024), who train
language models on linearized search traces obtained from MCTS on board game environments.
Similar to the work of Gandhi et al. (2024), they find consistent improvements in performance as
the model is given additional search budget at test-time (Figure 9 right). Note that these models
demonstrateaninference-timescalinglawwiththesamefunctionalformastheo1modelondifficult
18

<!-- Page 19 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
mathematics problems (OpenAI, 2024).

#### In-context Exploration For LLMs

While the prior section showed promise in teaching auto-regressive language models to internalize
complex search strategies involving exploration and backtracking, it remains unclear whether these
results can generalize to realistic language domains. In this section we will overview several recent
works, which show promise in internalizing episode-level search. Both Qu et al. (2024) and Snell
et al. (2024) evaluate results using open-source LLMs in the 7B and larger range on problems from
theMATHdataset(Hendrycksetal.,2021). Theyposetheproblemassequentialsampling-i.e. given
a problem q, generating full solutions from the same model auto-regressively as
Sğ‘— âˆ¼ ğœ‹ (Â·|Sğ‘—âˆ’1,...,S1,q) (4)
ğœƒ
where Sğ‘– are full solutions to the problem q. Both works formulate the problem as self-correction,
or revisions, during training. The approach generates training data by concatenating a number of
incorrect solutions with the correct revision and training on a linearized sequence (although the
exact training objective use a particular weighting grounded in RL (Peng et al., 2019)). The general
objective follows the form
min E [ï¸€ âˆ’logğœ‹ (S*|Sğ‘—âˆ’1,...,S1,q) ]ï¸€ (5)
ğœƒ

### Sğ‘–âˆ¼ğœ‹ref(Â·|q),qâˆ¼ğ’Ÿtrain ğœƒ

where ğ‘— is a fixed number of in-context exploration episodes sampled from a fixed distribution ğœ‹ref
(i.e. ğœ‹ ) and S* is some optimal solution. Essentially, this can be considered a linearization of the
0
Best-Of-N search strategy presented in Section 3.1 with rejection sampling. In this setting, the
Meta-CoTrepresentssearchinfullepisodesZ = S1,...,Sğ‘—âˆ’1 andS = Sğ‘—. Attesttimewecanfurther
control the quantity of compute by iteratively sampling from
Sğ‘– âˆ¼ ğœ‹ (Â·|Sğ‘–âˆ’1,...,Sğ‘–âˆ’ğ‘—,q). (6)
ğœƒ
RepresentativeresultsforthisapproachareareshowninFigure10,sourcedfrom(Snelletal.,2024).
We see clear improvement in the pass@1 metric with additional amounts of in-context exploration
episodes with nearly 6-7% gain from zero-shot to the level of saturation. At the same time, autoregressive generation shows clearly better scaling properties than independent parallel sampling
(Figure 10 right). These results indicate that the model learns some degree of in-context exploration
and self-correction.

#### Using variable Compute


### Whiletheaboveapproachesdemonstratepromisefor

the modelâ€™s capability to carry-out in-context search,
theyaretrainedwithafixednumberofrevisionsand
useapre-determinednumberofrevisionsattesttime.
This is not ideal, as ideally the model would be able
tousearbitraryamountsofcomputeuntilitarrivesat
a solution with high enough confidence. We repeat
the above experiment using a uniform number of incontext solutions during training (ranging between
0-7),allowingthemodeltogenerateupto8solutions
at inference time by optimizing

### Figure11:Numberofin-contextrevisionsthe

model attempts grouped by difficulty level.
19

<!-- Page 20 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
26
25
24
23
22
21
20
19
18
17
0 10 20 30 40 50 60
Number of Generations
)%(
ycaruccA
tseT

## Htam

Revision Model Pass@1 At Each Step
40
35
30
25
20
20 21 22 23 24 25 26
Number of Generations
)%(
ycaruccA
tseT

## Htam

Revision Model Parallel Verses Sequential
Sequential Best-of-N Weighted
Parallel Best-of-N Weighted
Sequential Majority

### Parallel Majority

Figure 10: Left: Pass@1 accuracy of a revision model after the specified number of generations
(revisions). Right: Scaling performance of the best-of-N strategy under parallel and auto-regressive
(in-context)sampling. Theperformancegapindicatesthatthemodellearnssomedegreeofin-context
exploration and self-correction. Source: Figure 6 from (Snell et al., 2024).
min E [ï¸€ âˆ’logğœ‹ (S*,EOS|Sğ‘—âˆ’1,...,S1,q) ]ï¸€ ,ğ‘— âˆ¼ Unif(1,8) (7)
ğœƒ

### Sğ‘–âˆ¼ğœ‹ref(Â·|q),qâˆ¼ğ’Ÿtrain ğœƒ

Thisformulationencouragesthemodeltocontinuerevisingitâ€™ssolutionuntilitreachesasolution
with high confidence of correctness. Interestingly, our model generates an increasing number of
solutionsbasedonquestiondifficulty. SummarystatisticsbyproblemdifficultyareshowninFigure11
(right), where the model generates an average of 2.45 solutions for Level 1 problems and an average
of 5.84 for Level 5 problems, consistent with the behavior shown in Figures 1 and 8 (c). Specifically,
this demonstrates that the model has internalized the need for extended exploration on complex
reasoning tasks.
Our best performing run achieved an improvement of 2% over the LLaMa 3.1 8B Instruct model
that we initialize our models from. We attribute this to a mismatch between the instruct modelâ€™s RL
post-training,theoff-policySFTfine-tuningweutilize,andthelimitedamountoftrainingdatainthe
MATH train dataset. Indeed, any regular SFT training we performed on the RL post-trained model
actually worsened performance. We are currently exploring post-training on pre-RL base models
with extended datasets.

#### Backtracking in LLMs

In the prior sections, we reviewed evidence that auto-regressive models can internalize complex
search strategies in simple domains. We also showed that LLMs can learn in-context exploration at
the episode-level. However, whether models can implement complex search strategies (e.g. those
outlined in Section 3) auto-regressively remains an open question in public research. Specifically,
we refer to the ability to terminate a reasoning chain prior to completion, and the ability to reset
(semantically)toanarbitrarypreviouslyvisitedstatein-context. Thesetwostepscanbeunifiedunder
the concept of backtracking. Here we will review some recent works demonstrating that LLMs can
learn to backtrack.
Recent works have demonstrated that training on data with backtracking can improve language
modelsonsimplereasoningtasks(Yeetal.,2024b;Anonymous,2024)findthatlanguagemodelscan
sometimesâ€œrecognizeâ€theirerrorsinternally,butdonothavetherequiredmechanismstoself-correct.
20

<!-- Page 21 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Similar to our motivation with Meta-CoT, their aim is for a single model to both recognize errors
and self-correct in-context. In their approach they create training data with incorrect steps directly
followed by the correction. The backtracking is signified by a special token, [BACK], at the end of
an incorrect step to allow the model to explicitly state itâ€™s belief that an error has occurred. That is:
given a dataset ğ’Ÿtrain of questions q and correct reasoning CoT solutions S = s
1
,...,s
ğ‘›
the training
objective becomes

### â„’backtrack(ğœƒ) = âˆ’E

s1,...,sğ‘›âˆ¼ğ’Ÿtrain,ğ‘¡âˆ¼Unif(1,ğ‘›)
[ï¸€ logğœ‹
ğœƒ
(s
1
,...,sâˆ’
ğ‘¡
,[BACK],s
ğ‘¡
,...,s
ğ‘›
|q) ]ï¸€ (8)
where ğ‘¡ is a randomly sampled time step in the solution and sâˆ’ is a single incorrect reasoning step.
ğ‘¡
This is in contrast to the standard approach, which only trains on the correct solution chains:
â„’standard(ğœƒ) = âˆ’E
Sâˆ¼ğ’Ÿtrain
[logğœ‹
ğœƒ
(S|q)]. (9)
Ye et al. (2024b) explore inserting incorrect steps at varying rates (between 1% and 50%) and find
that high rates of incorrect steps actually leads to improved downstream performance. In particular,
theyfindthata50%rateofincorrectsteps(objectiveinEquation8)leadstoanincreasefrom78%to
94% accuracy on hard math problems as compared to training on only correct solutions (Equation 9,
CoT). While promising, these results are only verified on small models (124M parameters).
Incontrast,Zhangetal.(2024b)teachLLMstobacktrackbasedonsafetyconsiderationsusingthe
largerGemma2BandLLaMa38Bmodels. Inparticular,followingtheabovenotation,givenaprompt
q and two possible answers - a safe option S+ = s+,...,s+ and an unsafe option Sâˆ’ = sâˆ’,...,sâˆ’,
1 ğ‘› 1 ğ‘›â€²
where s here represent individual tokens (unlike before where they stood for logical steps), they
optimize the objective:

### â„’(ğœƒ) = âˆ’E [logğœ‹ ([BACK],S+|Sâˆ’,q) +

(q,S+,Sâˆ’)âˆ¼ğ’Ÿtrain,ğ‘¡âˆ¼Unif(1,ğ‘›â€²) ğœƒ ğ‘¡ (10)
logğœ‹ (S+|q)].
ğœƒ
That is a combination of the Meta-CoT and regular CoT objectives as outlined above. Additionally,
notice that this objective masks out the unsafe completion, while the prior work trains on all tokens
including the incorrect logical steps. While the approach of Ye et al. (2024b) backtracks for a single
logicalstep(correction)thisworkalwaysresetstheagenttotheinitialstate. SFTtrainingissuccessful
inteachingthemodeltobacktrackandimprovesthesafetycharacteristicsoversupervisedfine-tuning
on just the safe answer (only the second term of Equation 10). However, these effects appear weak in
regular SFT models, but are significantly improved through further downstream RL training, which
we will discuss later on.

### Synthetic Meta-CoT Via Search

In the prior sections we argued for an approach to reasoning that teaches an LLM to internalize an
auto-regressive search procedure in-context. We also reviewed several recent works showing that
small auto-regressive models can carry out in-context exploration at the episode level, and larger
models can learn individual step backtracking. In this section, we explore how to construct synthetic
data for realistic Meta-CoT that involves full-scale in-context tree search.
Setup. For demonstrative purposes, we use the math problem presented by OpenAI (2024) as our
benchmark task, where Gemini 1.5 Pro (Reid et al., 2024) achieves a Pass@128 score of 6.25%
(8/128correct)â€“notablybeingtheonlyfrontiermodel(withoutadvancedreasoning)todemonstrate
non-zero performance at the time of our experiments. We use the same RL formulations for state
and actions as presented in 3.3. We explore two principal search algorithms for generating synthetic
21

<!-- Page 22 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
training data: Monte Carlo Tree Search (MCTS) and A* variants. Both approaches necessitate a
heuristic state estimation function, for which we employ pure Monte-Carlo rollouts following the
methodology of Silver et al. (2018). Specifically, we estimate the value of a partial solution trajectory
as

## ğ¾

ğ‘£(S ,q) = E 1 âˆ‘ï¸ ğ‘Ÿ*([Sğ‘— ,S ],q) (11)
ğ‘¡ Sğ‘—
â‰¥ğ‘¡+1
âˆ¼ğœ‹
ğœƒ

## (S

â‰¥ğ‘¡+1
|Sğ‘¡,q)ğ¾ â‰¥ğ‘¡+1 ğ‘¡
ğ‘—=1
where ğ‘Ÿ* is the verifiable ground-truth outcome reward. In our experiments, we sample 128 completions from the partial solution and evaluate the mean success rate under ground-truth outcome
supervision. In Appendix E, the numerical values of the states are listed after each step.

#### Monte-Carlo Tree Search

We conduct an example based on Monte-Carlo

### Tree Search (MCTS), which seeks to balance

exploration and exploitation. The MCTS implementation of Silver et al. (2018) has been
widely applied to the reasoning domain (Tian
et al., 2024; Feng et al., 2024), and we mostly
follow their implementation with some modificationstoaccountforthestructureofoursearch
problem (see Appendix D).
We present the search trace for our example problem - all the actions taken during the
search (i.e., the Meta-CoT in a linear format)
- in Appendix E. The numbers following each Figure 12: MCTS tree for the math problem presented by OpenAI (2024). The red node indicates
reasoningsteprepresentthevalueestimates. In
the solution.
our initial MCTS attempt we obtained a trace
with an excessive number of backtracks and
repetitions,includingfromhigh-valuestates(as
high as 1.0) with the resulting exploration tree is shown in Figure 12. We believe these effects are
due to the exploration bonus in MCTS search. We did not carry out extensive ablations on the search
parameters due to speed and costs. Since we use pure MC rollouts ("simulations") for state value
estimation, a single tree uses up to 20 million tokens inference (a cost of âˆ¼$100 per tree). Moreover
the process can take up to half an hour due to API limits. Because of these issues we also evaluate a
more efficient best-first exploration strategy, which we present below.

#### A* search

We begin with an exploration of a type of best-first search based on the work by Koh et al. (2024),
which itself loosely follows an A* approach. The search procedure maintains a frontier â„± of states,
whichisimplementedasamaxpriorityqueue. SimilarlytotheMCTSapproach,eachstateS consists
ğ‘¡
of the question q and a partial solution consisting of generated reasoning steps (s ,...,s ). At each
1 ğ‘¡
iteration, the state S â† pop(â„±) with the highest value ğ‘£ = ğ‘£(S ,q) is selected, where ğ‘£ âˆˆ [0,1]
ğ‘ ğ‘ ğ‘ ğ‘
is the value of the partial solution S including current and previous reasoning steps. At each node
ğ‘
the policy ğœ‹ proposes ğ‘ candidate next steps, each of which is evaluated by ğ‘£ and added to â„± if the
ğœ‘
depth of the tree |(s ,...,s )| has not reached the maximum depth search limit ğ‘‘. For the purpose
0 ğ‘
of generating synthetic data, we run the search until we find a solution that is correct using the
ground-truth verifier. The resulting tree is shown in Figure 13. It shows more consistent flow of the
22

<!-- Page 23 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
reasoning steps, with less backtracking concentrated around a few key steps.

### Do Advanced Reasoning Systems Implement In-Context Search?

In this section we will investigate whether advanced reasoning systems, such as OpenAIâ€™s

### O1 (OpenAI, 2024), DeepSeek R1 (DeepSeek,

2024) and Gemini 2.0 Flash Thinking Mode 2
and the Qwen QwQ Team (2024) implement
in-context search. We provide successful reasoning traces for the same math problem in
Appendix E.
Starting with OpenAIâ€™s o1 model, by carefullyexaminingtheprovidedmathematical reasoning trace, we observe:

## Inconsistent flow ofthought -consecutive

steps do not logically continue the prior
state. Figure 13: Resulting A* search tree on the math

## Backtracking - the model carries out "se- problem from OpenAI (2024). This trace presents

mantic backtracking" - frequently return- moreofabest-firstapproachwithfewerbacktracks,
ing to the same logical points. concentratedaroundkeysteps,ascomparedtothe

## Repetition - the model often repeats log- one produced by MCTS in Figure 12.

ical steps.
The qualitative behaviors observed in o1 (Figure 14 left) are similar to those in the example
synthetic trace (Figure 15) generated by Gemini 1.5 with and MCTS-like search processes. In
particular, there are abrupt changes in logical flow of the (Meta) CoT, which is natural as the model
backtracks between branches of the tree. Moreover, the model may explore multiple child nodes
of the same parent which are different strings, but can also be very semantically similar leading to
repetitive logic. This is clear in the provided trace, as the model repeats logical statement and goes
over the same derivations multiple times. Note also that we do not claim the model is implementing
tree search at test time, but rather that as much as the modelâ€™s output are expected to resemble
itâ€™s training data, we hypothesize that examples of search were used during training (likely model
initialization). We will specifically address the need and effects of RL training in Section 6.
The DeepSeek R1 model DeepSeek (2024) also exhibits similar behaviors, as shown in Figure 14,
however, it also carries out a significant amount of self-evaluation steps. This could be achieved
by integrating a form of self-criticism (Madaan et al., 2023; Shinn et al., 2023) or a generative
verifier (Zhang et al., 2024a) in the search trace. The LATS framework (Zhou et al., 2024a) uses
a similar approach, combining MCTS search with self-criticism and shows empirical improvements
from self-reflection. Another alternative for synthetic data generation is the "Iteration-Of-Thought"
approach Radha et al. (2024) which also interleaves generation with inner dialogue. This would
explain the rather smooth logical flow of the R1 model, which does not exhibit as much abrupt
back-tracking, as compared to O1. As mentioned earlier, in order to adequately model the search
2https://ai.google.dev/gemini-api/docs/thinking-mode
23

<!-- Page 24 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Let n be an even positive integer. Let p be a monic, real polynomial of degree 2n; that is to say, ğ‘ğ‘¥ = ğ‘¥!"+ğ‘!"#$ğ‘¥!"#$+â‹¯+ ğ‘$ğ‘¥+ ğ‘% for some real
coefficients ğ‘%,â€¦,ğ‘!"#$â€‹. Suppose that ğ‘
&
$ = ğ‘˜!for all integers k such that 1â‰¤ ğ‘˜ â‰¤ğ‘›. Find all other real numbers xx for which ğ‘
â€™
$ = ğ‘¥!

### Backtracking Steps Verification Steps


### O1 DeepSeekâ€“R1 Gemini 2.0 Flash Thinking

We are told that for all integer values of ğ‘˜satisfying 1â‰¤ğ‘˜â‰¤ğ‘› SoI have this problem here. Let's see what it's about. We have The problem asks for the real numbers ğ‘¥other than
ğ‘ ( $ = ğ‘˜! a d n e g e r v e e e n 2 p n o , s w it h iv ic e h i n m te e g a e n r s n it , l a o n o d k s a l m ike o n th ic is r : eal polynomial ğ‘(ğ‘¥)of Â±1,Â±$ ! ,â€¦,Â± " $such that ğ‘ & $ =ğ‘¥!.
Our problem is: Given that ğ‘is monic of degree 2ğ‘›, and that ğ‘ğ‘¥ =ğ‘¥!"+ğ‘!"#$ğ‘¥!"#$+â‹¯+ğ‘$ğ‘¥+ğ‘% We are given that ğ‘(ğ‘¥)is a monic polynomial of degree 2ğ‘›.
ğ‘ ( $ = ğ‘˜!for ğ‘˜=Â±1,Â±2,â€¦,Â±ğ‘›, find all other real numbers ğ‘¥ for some real coefficients ğ‘%,ğ‘$,â€¦,ğ‘!"#$. Theconditionğ‘ ( $ =ğ‘˜!holds for integers ğ‘˜such that 1â‰¤ğ‘˜â‰¤ğ‘›.
s In u c o h th t e h r a t w ğ‘ ord & $ s, = w e ğ‘¥ a ! r . e to find all real numbers ğ‘¥such that therefore, ğ‘¥!"ğ‘( & $)is a polynomi â‹¯ al of degree 2ğ‘›, and ğ‘¥!"â€™!is a T âˆ’ h 1 is , âˆ’ m 2 e , a â€¦ n , s âˆ’ th ğ‘› e . condition holds for ğ‘˜=1,2,â€¦,ğ‘›and ğ‘˜=
ğ‘
&
$ = ğ‘¥!.
â‹¯
m
ğ‘¥!
o
"
n
ğ‘
om
& $
ia
âˆ’
l o
ğ‘¥
f
!
d
"â€™
e
!
g
i
r
s
e e
a
2
p
ğ‘›
ol
+
yn
2
o
.
m
so
ia
,
l
t
o
h
f
e
d
ir
e
d
g
if
r
f
e
e
e
r e
2
n
ğ‘›
ce
+
ğ‘¥
2
!
.
"ğ‘ğ‘¥ = T
So
he
, ğ‘”
va
ğ‘¥
lue
=
o
âˆ’
f ( $
ğ‘¥
a
!
re
âˆ’
Â±
1
1,
ğ‘¥
Â±
!
$ !
âˆ’
,â€¦
4
,Â±
â€¦
" $
â‹¯
.
ğ‘¥!âˆ’ğ‘›!ğ‘„(ğ‘¥).
Our equation is ğ‘ & $ âˆ’ ğ‘¥!=0. wait, but ğ‘(ğ‘¥)is of degree 2ğ‘›, so ğ‘ & $ is & $ !"+ğ‘!"#$&!" $ #$+â‹¯+ The degreeof ğ‘¥!âˆ’1 ğ‘¥!âˆ’4â€¦ğ‘¥!âˆ’ğ‘›! is ğ‘›.
L M r fu a e t n o t i c o r m e t n i o o e a v n l d e f o u e r, n f f s i c n ğ‘¥ in t e . i c o q e n ( ğ‘ x o ) f i s = ğ‘¥ a o ğ‘ p f o d & $ l e y g n âˆ’ o re m ğ‘¥ e ! i â‹¯ a 2 . l ğ‘› o w f h d e e n g r c e o e n 2 s ğ‘› id , e t r h e e d n ğ‘ as & $ a is a ğ‘ ğ‘ t w w h $ $ h a i ğ‘¥ s & $ i i t c ! , + i h " s a # a ğ‘ i c $ t % t p m u + , o a a i ğ‘ l g l n l y % y h d n , ğ‘¥ t o s ! m b i m " n e u , c i , l a w e t l b i h p ğ‘ o u i l c ( f y t ğ‘¥ h i d s n ) i e i g n s i g s c i b r e n m e y d ğ‘ e o ğ‘¥ e ( n 2 e ! ğ‘¥ i " ğ‘› d â‹¯ ) c g + i o a s i f v 2 m d e d , e o s e a g n g 1 s r i s e r c + e u e , e m ğ‘ ğ‘ 2 2 % i ! ğ‘› n " ğ‘› c g # p , o $ o t ğ‘ u ğ‘¥ h l % l y e d + n i s l b o e â‹¯ n e m a o d + a i t a i n n z l. y g e t r h t o e i , n r g m . T T L ğ‘” W C e h h o e ğ‘¥ t e e n â€™s r k s d e c n i = d f e o o o g e n âˆ’ w r r r e s e ğ‘¥ t , i t d e h ! h t " e e h a o â€™ r e t p f ! t ğ‘” d ğ‘” o h + e e l ğ‘˜ y ğ‘¥ g ğ‘ n e % r o x e = ğ‘¥ i p m s e ! r 0 " 2 i e o a ğ‘› + s f f l o s + â„ ğ‘„ i r ğ‘ o ( $ ğ‘˜ 2 ğ‘¥ n ğ‘¥ ğ‘¥ . ) = o ! = " i f s Â± # ğ‘” 1 $ 1 2 + + , ğ‘¥ ğ‘› Â± â‹¯ ğ‘ + 2 i ! n , " + 2 â€¦ m # ğ‘ $ , o âˆ’ Â± ğ‘¥ ! r " ğ‘› e + ğ‘› # . $ d = â‹¯ ğ‘¥ e ğ‘› t + + a + i 1 ğ‘ l. . % 2 ğ‘¥ . !"âˆ’ğ‘¥!"â€™!.
But we can consider q(x)=ğ‘ & $ âˆ’ ğ‘¥!. is ğ‘¥!", so ğ‘ & $ =ğ‘¥#!"+â‹¯+ğ‘%, and ğ‘¥!"â€™!ğ‘ & $ =1+ğ‘!"#$ğ‘¥+ Therootsofâ„(ğ‘¥)are Â±1,Â±2,â€¦, â‹¯ Â±ğ‘›.
Alternatively, letâ€™s consider sx = ğ‘¥!"ğ‘ & $ âˆ’ğ‘¥!"â€™!. â‹¯+ğ‘%ğ‘¥!", which is indeed a degree 2ğ‘›polynomial. Let's recheck the degree of the factors.
Let me consider that ğ‘ ğ‘¥ =ğ‘¥!" â‹¯ ğ‘ & $ âˆ’ğ‘¥! =ğ‘¥!"ğ‘(ğ‘¥). s ğ‘¥ o !" , â€™ ğ‘¥ ! ! , " w ğ‘ h ğ‘¥ ich = is ğ‘¥ a ! " d ğ‘ eg & $ ree âˆ’ 2 ğ‘¥ ğ‘› !" + â€™! 2 = pol 1 yn + o ğ‘ m ! i " a # l $ i ğ‘¥ f ğ‘¥ + !" â‹¯ â€™! + is ğ‘ % th ğ‘¥ e ! " hig âˆ’ hest T T ğ‘¥ h h ! e e âˆ’ p le r 1 a o d d i h u n a c g s t t d h e a e rm s g r d e o e e f g â„ 2 re ( . ğ‘¥ e ) 2 i ğ‘› s . a form âˆ’ğ‘¥!"â€™!.
S S o inc ğ‘  e ğ‘¥ ğ‘ = ğ‘˜ ğ‘¥ = !" 0 ğ‘( fo ğ‘¥) r . ğ‘˜ =Â±1,Â±2,â€¦,Â±ğ‘›, we have ğ‘ğ‘˜ =0. degree term. â‹¯ The leading term of ğ¶ ğ‘…(ğ‘¥)is ğ¶ ğ‘¥!".
This doesn't seem right.
B B Re u u c t t a o ğ‘¡ l u l ğ‘¥ t r h k a = e t y ğ‘ ğ‘ i âˆ— s ( ğ‘˜ ğ‘¥ th ) = a , t a 0 ğ‘  nd f ğ‘¥ o ğ‘  r = ğ‘˜ ğ‘¥ = ğ‘¥ = ! Â± " ğ‘¡ ğ‘ 1 ( , ğ‘¥ â‹¯ ğ‘¥ Â± ) 2 . âˆ’ , â€¦ ğ‘¥! , " â€™ Â± ! ğ‘› . . w ğ‘˜ ğ‘˜ ! ! e " , â€™ k s n ! o o = ğ‘ w 0 ğ‘˜ t . ha = t ğ‘ ğ‘˜! ğ‘˜ "ğ‘ = ( $ 0 âˆ’ fo ğ‘˜ r ! ğ‘˜ "â€™ = ! Â± = 1 ğ‘˜ ,Â± !" 2, 5 â‹¯ ğ‘˜! ,Â± âˆ’ ğ‘› ğ‘˜ , ! b " e â€™ c ! a = us ğ‘˜ e ! " ğ‘ â€™! ( $ âˆ’ = L ğ‘” W e e ğ‘¥ t's h = a g v o e âˆ’ b ğ‘¥ ğ‘” a ! c " ğ‘˜ k â€™ ! t = o + t 0 h ğ‘ e % fo ğ‘¥ d r ! e " ğ‘˜ f + i = ni ğ‘ t Â± i $ o 1 ğ‘¥ n , ! â€¦ " o # f , $ Â± ğ‘” + ğ‘› ğ‘¥ . â‹¯ . +ğ‘!"#$ğ‘¥+1.
â‹¯ therefore, ğ‘ğ‘¥ has 2ğ‘›roots at ğ‘¥=Â±1,Â±2,â‹¯,+ğ‘›. Consider the polynomialğ‘ ğ‘¥ = âˆ"%& . $ ( & & ! ) #(!.
Thus, ğ‘ ğ‘¥ must be divisible by the monic polynomial whose since ğ‘ğ‘¥ is a degree 2ğ‘›+2polynomial, there are two more â‹¯
r T o h o u t s s , a ğ‘  re ğ‘¥ Â± = 1, ğ‘š Â±2 ğ‘¥ ,â€¦ â„ , ( Â± ğ‘¥ ğ‘› ). . r fo o r o t s s o m be e y o co n n d s 2 ta ğ‘› n r t o ğ‘ o , t b s. ecause the roots come in pairs ğ‘¥=ğ‘˜and The valuesÂ± " $ !are distinct from Â±1,Â±2,â€¦,Â±ğ‘›.
Where ğ‘šğ‘¥ = ğ‘¥âˆ’âˆ’ğ‘› ğ‘¥âˆ’âˆ’ğ‘›+1 â€¦ğ‘¥âˆ’âˆ’1 Dğ‘¥âˆ’ ğ‘¥=âˆ’ğ‘˜, and possibly another pair ğ‘¥= ğ‘and ğ‘¥=âˆ’ğ‘. The real numbersğ‘¥for which ğ‘ & $ =ğ‘¥!are Â±1,Â±2,â€¦,Â±ğ‘›, " $ ! ,âˆ’ " $ !.
T 1 he E re â€¦ fo (ğ‘¥ re âˆ’ , â„ ğ‘› ( ) ğ‘¥ . )is a quadratic p â‹¯ olynomial. w g m i a v o i e r t e , s b r ğ‘› o u q o t u t ğ‘ s a ğ‘¥ t d o r a a i t s c i c c d o f e a u g c n r t e t o e f r o s 2 , r . ğ‘› w + hic 2 h , a is n d d e ( g ğ‘¥ r ! ee âˆ’ 2 ğ‘˜ ğ‘› ! . ) s f o o , r t ğ‘˜ he = re 1 a t r o e ğ‘› two T âˆ’ h " $ e !. question asks for the other real numbers, which are " $ !and
Our plan is to write ğ‘ ğ‘¥ =ğ‘šğ‘¥â„(ğ‘¥). alternatively, perhaps ğ‘ğ‘¥ =(ğ‘¥!âˆ’1)(ğ‘¥!âˆ’4)â‹¯(ğ‘¥!âˆ’ğ‘›!)5(ğ‘¥!âˆ’ Let's double check the reasoning.
Given that ğ‘ ğ‘¥ is degree 2ğ‘›+2with leading coefficient âˆ’1, ğ‘)(ğ‘¥!âˆ’ğ‘), where ğ‘and ğ‘are th â‹¯ e remaining roots. The condition ğ‘ & $ =ğ‘˜!for 1â‰¤ğ‘˜â‰¤ğ‘›implies that ğ‘¥=
â‹¯ Â±1,Â±2,â€¦,Â±ğ‘›are roots of ğ‘”(ğ‘¥).
Similarly, ğ‘šğ‘¥ is a degree 2ğ‘›monic polynomial. wait, but in the factorization, I have ğ‘¥!"âˆ’ (" $ !)!, which gives ğ‘¥= â‹¯
T co h e e f r f e ic fo ie r n e t , â„ âˆ’ ( 1 ğ‘¥ , )must be a degree 2polynomial with leading Â± " $ !, assuming thatâ€™s correct. T So he , ğ‘ ro ğ‘¦ ots = o C f ğ‘ y (ğ‘¦ âˆ’ ) 1 are y Â± + 1, 1 Â±2 y ,â€¦ âˆ’ , 2 Â±ğ‘› y . +2â€¦(yâˆ’n)(y+n)
Alternatively, since ğ‘ ğ‘¥ =ğ‘šğ‘¥â„(ğ‘¥), but ineed to verify this conclusion. ğ‘ğ‘¦ =ğ¶ ğ‘¦!âˆ’1 ğ‘¦!âˆ’4â€¦(ğ‘¦!âˆ’ğ‘›!).
We can write ğ‘ ğ‘¥ = âˆ’1ğ‘¥!"â€™!+ğ‘™ğ‘œğ‘¤ğ‘’ğ‘Ÿ ğ‘‘ğ‘’ğ‘”ğ‘Ÿğ‘’ğ‘’ ğ‘¡ğ‘’ğ‘Ÿğ‘šğ‘ . given that ğ‘= (" $ !)!, and the factorization is ğ‘Ÿğ‘¥âˆ’ğ‘¥!"â€™!= The leading term of ğ‘ğ‘¦ is ğ‘¦!"â€™!.
Sowe have: â‹¯ âˆ’ğ‘¥!âˆ’1 ğ‘¥!âˆ’4â‹¯ğ‘¥!âˆ’ğ‘›! ğ‘¥!âˆ’ " $ !! ,that indeed, the roots T T h h e is l d e o ad es in n g o t t e m rm a t o ch f . ğ¶ ğ‘¦!âˆ’1â€¦(ğ‘¦!âˆ’ğ‘›!)is ğ¶ ğ‘¦!".
ğ‘ ğ‘¥ =ğ‘šğ‘¥â„ğ‘¥ =[âˆ"(-$(ğ‘¥!âˆ’ğ‘˜!)](âˆ’ğ‘¥!+ğ‘ğ‘¥+ğ‘)and ğ‘ ğ‘¥ = are ğ‘¥=Â±1,Â±2,â€¦,Â±ğ‘›,Â± " $ !. Let's re-read the problem carefully.
ğ‘¥ A Th ! lt " e e ğ‘ n rn ğ‘  a & $ t ğ‘¥ iv âˆ’ e = , ğ‘¥ l ! e ğ‘¡ " t â€™ â€™ ğ‘¥ s ! c . âˆ’ on ğ‘¥ s ! id "â€™ e ! r t = ha ğ‘ t âˆ— ğ‘¡ ğ‘¥ â‹¯ ğ‘¥ âˆ’ = ğ‘¥ ğ‘¥ !" ! â€™ " ! ğ‘ . & $ =ğ‘âˆ—(ğ‘¥). t Â± b o h f u e " $ t ğ‘ r ! . e . w f a o i r t e , , i s t h ğ‘› e ! c a o d r d re it c io t n h a e l r e so ? lu le t t io m n e s b d e o y u o b n le d - c ğ‘¥ h = ec Â± k 1 th ,Â± e 2 c , a â€¦ lc , u Â± la ğ‘› tio ar n e W W Co e e n a a s r r id e e e g a r s iv t k h e e e n d p c fo o o r l n y d r n e i o t a i m l o n n ia u s l m o ğ‘Ÿ n b ğ‘¥ e ğ‘ rs = & $ ğ‘¥ ğ‘¥ . s ! u ğ‘ ch & $ th âˆ’ at 1 ğ‘ . & $ =ğ‘¥!.
So, the coefficient of ğ‘¥!"in ğ‘ (ğ‘¥)is ğ‘ !"=ğ‘. earlier, ihad: The roots ofğ‘Ÿ(ğ‘¥)are the values of ğ‘¥for which ğ‘ & $ = & $ !.
B So u t ğ‘  ğ‘  ğ‘¥ ğ‘¥ = = [ğ‘ ğ‘ âˆ— âˆ— (ğ‘¥ ğ‘¥ )] âˆ’ âˆ’ ğ‘¥ ğ‘¥ ! ! " " â€™ â€™ ! ! . . ğ‘= âˆ’1"(15 1 459â‹¯ğ‘›!)=(ğ‘› 1 !)! The given condition is ğ‘ ( $ =ğ‘˜!.
B T T T h h h u u u u t s s s ğ‘ , , ğ‘¥ âˆ— ğ‘ t ! h ğ‘¥ = e = o ( h ğ‘ " t $ a ! h ) = s ! e r d 0 r e e âŸ¹ g a r l e ğ‘¥ n e u = â‰¤ m Â± b 2ğ‘› e " $ r . ! s ar â‹¯ e ğ‘¥=Â± " $ ! s t h Â± h i e n e " n $ c r ! c e e e f ğ‘› o , r t i e h s , e e t v h o e e th n a e , d r ( d âˆ’ re i 1 t a i ) o l " n n = a u l m 1 r , o b s o e o t r s s i n a ğ‘¥ d re t e h e ğ‘¥ a d t = , s ğ‘ a Â± t = i " s $ f ! ( y " $ ğ‘ !)!. & $ =ğ‘¥!are ğ‘¥= T ğ‘› T T T h h h h . e e e e s q r v e o a u o a l e u t r s e e s t s i o Â± o f f n o 1 ğ‘” , r a Â± ( w s ğ‘¥ k $ ! h ) s , i a c â€¦ f r h o e , r t Â± t h " h e a " $ e l . l c v o o a t n l h u d e e it r s i o r â‹¯ o e n f a i l ğ‘¥ s n f g u o i m r v e w b n h e i r a c s r h " e . t ( $ he w h e e q r u e a 1 lit â‰ª y h ğ‘˜ ol â‰ª ds.
Answer: The other real numbers ğ‘¥satisfying ğ‘1â„ğ‘¥ =ğ‘¥!are The valuesÂ±1,Â±2,â€¦,Â±ğ‘›correspond to the given conditions.
ğ‘¥=Â± " $ ! The other real numbers are" $ !and âˆ’ " $ !.
Figure14:Examplesofintermediatetracesfromo1(OpenAI,2024),DeepSeek-R1(DeepSeek,2024),
andGemini2.0FlashThinkingMode. Wehighlighttwotypesofsteps: Backtracking,wherethemodel
visits a bad state and returns to a previously visited step, and Verification, where the model assesses
thecorrectnessofthepreviousoutput. Inconsistentlogicalflowandrepetitionarepresentinallthree
traces. DeepSeek-R1 and Gemini 2.0 Flash Thinking Mode both exhibit generative verification before
reaching an answer, while Gemini makes an incorrect verification and returns to the initial state. Full
search traces can be found in Appendix E.
24

<!-- Page 25 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Let n be an even positive integer. Let p be a monic, real polynomial of degree 2n; that is to say, ğ‘ğ‘¥ = ğ‘¥!"+
ğ‘!"#$ğ‘¥!"#$+â‹¯+ ğ‘$ğ‘¥+ ğ‘% for some real coefficients ğ‘%,â€¦,ğ‘!"#$â€‹. Suppose that ğ‘
&
$ = ğ‘˜!for all integers k such that
1â‰¤ ğ‘˜ â‰¤ğ‘›. Find all other real numbers xx for which ğ‘ $ = ğ‘¥!
â€™

### Backtracking Steps Verification Steps


### MCTS (Gemini 1.5) A* (Gemini 1.5) QwenQwQ

Let ğ‘ğ‘¥ =ğ‘ # " âˆ’ğ‘¥!. Let ğ‘ğ‘¥ =ğ‘¥!ğ‘ # " =ğ‘$ğ‘¥!%+ğ‘"ğ‘¥!%&"+â‹¯+ğ‘!%&"ğ‘¥+1. L in e t t e 's g e ta r c ğ‘› k , l e a n th d i s a p m ro o b n l i e c m re s a t l e p p o b ly y n s o t m ep ia . l S ğ‘ o o , w f d e e h g a r v e e e a 2 n ğ‘› . e T ve h n at p m o e si a ti n v s e
Let ğ‘ğ‘¥ =ğ‘ # " âˆ’ğ‘¥!. Then ğ‘(ğ‘¥)is a rational function of the The given condition ğ‘ â€™ " =ğ‘˜!implies ğ‘ğ‘˜ =ğ‘˜!%ğ‘˜!=ğ‘˜!%(! ğ‘ğ‘¥ =ğ‘¥!%+ğ‘!%&"ğ‘¥!%&", where all the coefficients are real numbers.
f ğ‘ or ğ‘¥ m ="(/!#%&#(â‹¯(/&#! # # ! % # &(/â€™#!#&#!#(!#!#. f L p o e o r t l y 1 ğ‘ n â‰¤ o ğ‘¥ m |ğ‘˜ = ia | l â‰¤ ğ‘¥ o ! f ğ‘› % d + eg ğ‘ r ! e % e & " 2 ğ‘¥ ğ‘› ! , % w &" he + re â‹¯ ğ‘› + is ğ‘ " a ğ‘¥ n + ev ğ‘ e $ n b p e o a si t m iv o e n i i n c t eger. T ğ‘˜ h â‰¤ e ğ‘› p . r o S b in le ce m ğ‘› st i a s t e e v s e t n h , a t t h ğ‘ e i â€™ " nte = ge ğ‘˜ r ! s f ğ‘˜ or r a a n ll g i e n t f e ro g m er s âˆ’ ğ‘˜ ğ‘› s t u o c ğ‘› h , t e h x a c t l 1 ud â‰¤ ing
We are given that ğ‘(ğ‘¥)is a monic real polynomial of degree 2ğ‘›. zero, because ğ‘˜is the denominator.
ğ‘ğ‘¥ ="(/!#%&#(â‹¯(/&#! # # ! % # &(/â€™#!#&#!#(!#!#. W ğ‘˜â‰¤ e a ğ‘› r . e given that ğ‘ â€™ " =ğ‘˜!for all integers ğ‘˜such that 1â‰¤ So, we have ğ‘ â€™ " =ğ‘˜!for ğ‘˜=Â±1,Â±2,â€¦,Â±ğ‘›. That gives us 2ğ‘›
ğ‘ğ‘¥ ="(/!#%&#(â‹¯(/&# # ! ! # # %&(/â€™#!#&#!#(!=1 # ( ! # # ). We are given that ğ‘ â€™ " =ğ‘˜!for integers ğ‘˜such that 1â‰¤ğ‘˜â‰¤ e in q c u lu a d ti e o d n . s because for each ğ‘˜from 1to ğ‘›, both ğ‘˜and âˆ’ğ‘˜are
â‹¯ ğ‘›. â‹¯
L ğ‘ e "ğ‘¥ t ! ğ‘ %& ğ‘¥ "+ = ğ‘ ğ‘¥ $ ! ğ‘¥ % ! ğ‘ % ( . # "). Then ğ‘ğ‘¥ =1+ğ‘!%&"ğ‘¥+ğ‘!%&!ğ‘¥!+â‹¯+ T ğ‘ h ğ‘˜ e c = on ğ‘˜ d ! i % ti ğ‘˜ o ! n = ğ‘ ğ‘˜ â€™ " !%( = ! ğ‘˜ fo ! r f 1 or â‰¤ 1â‰¤ ğ‘˜ â‰¤ ğ‘˜ ğ‘› â‰¤ . ğ‘›is equivalent to B ha u n t d ğ‘ le # , " le i t s ' s a m ra u t l i t o ip n l a y l b fu o n t c h t i s o id n e , s n o b t y a ğ‘¥ p !% ol t y o n o el m im ia in l. a T t o e m th a e k e it easier to
Thus, ğ‘Ÿ(ğ‘¥)is a polynomial of degree 2ğ‘›+2with roots ğ‘˜for 1â‰¤ Consider the polynomial ğ‘Ÿğ‘¥ =ğ‘¥!%(!âˆ’ğ‘(ğ‘¥). We are given denominator:
ğ‘˜ W â‰¤ e a ğ‘› r . e S g in iv c e e n t h th e a re t ğ‘ a ( re ğ‘¥ ) 2 i ğ‘› s a su m ch o n v i a c l u re e a s l o p f o ğ‘˜ ly , n w o e m c i a a n l o w f r d it e e gree 2ğ‘›. t S h in a c t e ğ‘Ÿ ğ‘› ğ‘˜ is = ev 0 e f n o , r l e 1 t â‰¤ ğ‘›= ğ‘˜ 2 â‰¤ ğ‘š. ğ‘› T . hen we have 2ğ‘›=4ğ‘š. Thus, ğ‘¥!%ğ‘ ğ‘¥ 1 âˆ’ğ‘¥!%ğ‘¥!
Since ğ‘ â€™ " =ğ‘˜!for all integers ğ‘˜such that 1â‰¤ğ‘˜â‰¤ğ‘›, we have ğ‘Ÿğ‘˜ has the roots Â±1,Â±2,â€¦,Â±2ğ‘š. Now,ğ‘ # " =( # ")!%+ğ‘!%&"( # ")!%&"+â‹¯+ğ‘" # " +ğ‘$, so multiplying by
ğ‘ğ‘˜ =ğ‘ â€™ " âˆ’ğ‘˜!=0for 1â‰¤ğ‘˜ â‹¯ â‰¤ğ‘›. ğ‘Ÿ âˆ! â€™ ğ‘¥ ) *" = ğ‘¥âˆ’ ğ‘¥âˆ’ ğ‘˜ 1 ğ‘¥+ ğ‘¥+ ğ‘˜ 1 =âˆ ğ‘¥ ! â€™ âˆ’ ) *" 2 ğ‘¥! ğ‘¥ âˆ’ + ğ‘˜ 2 ! â€¦ . ğ‘¥âˆ’2ğ‘š ğ‘¥+2ğ‘š = ğ‘¥!%giv 1 e + s: ğ‘!%&"ğ‘¥+ğ‘!%&!ğ‘¥!+â‹¯+ğ‘"ğ‘¥!%&"+ğ‘$ğ‘¥!%âˆ’ğ‘¥!%(!=0
We also know that the coefficient of ğ‘¥!%("is ğ‘Ÿ(ğ‘¥)is 0. The Since ğ‘ğ‘¥ is a polynomial of degree 2ğ‘›, ğ‘Ÿğ‘¥ is a monic Letâ€™s denote this polynomial asğ‘ğ‘¥ =1+ğ‘!%&"ğ‘¥+ğ‘!%&!ğ‘¥!+â‹¯+
c

## ğµ

o
=
eff
0
ic
.
ient of ğ‘¥!%("in âˆ% â€™*"(ğ‘¥!âˆ’ğ‘˜!)(âˆ’ğ‘¥!+ğµğ‘¥+ğ¶)is ğµ. Thus p
T ğ‘Ÿ h
o
ğ‘¥ e
ly
r
n
e =
o
fo
m
re ğ‘¥
ia
, !
l
w âˆ’
o
e
f
1
d
ca
e
n ğ‘¥
g
!
r
w
e
âˆ’ r
e
it 2 e
2
!
ğ‘›
â€¦
+2
ğ‘¥!
=
âˆ’
4ğ‘š
2ğ‘š
+2
!
.
= ğ‘¥!âˆ’1(ğ‘¥!âˆ’
ğ‘

## S

"
o
ğ‘¥
,
!
t
%
h
&
e
"
r
+
oo
ğ‘
t
$
s
ğ‘¥
o
!
f
%
q
âˆ’
ğ‘¥
ğ‘¥!
=
%(
0
!.
corresponds to the values of ğ‘¥where
So, ğµ(2ğ‘š!)!=0, which implies ğµ=0. 4)â€¦ğ‘¥!âˆ’4ğ‘š! ğ‘ # " =ğ‘¥!.
We have ğ‘Ÿğ‘¥ =âˆ% â€™*"(ğ‘¥!âˆ’ğ‘˜!)(âˆ’ğ‘¥!+ğµğ‘¥+ğ¶). Therefore, we can write ğ‘Ÿğ‘¥ =ğ‘¥!%(!âˆ’ğ‘ğ‘¥ = ğ‘¥!âˆ’1(ğ‘¥!âˆ’ â‹¯
Sowe have ğ‘Ÿğ‘¥ =âˆ% â€™*"(ğ‘¥!âˆ’ğ‘˜!)(âˆ’ğ‘¥!+& % " ! # !). 4)â€¦ğ‘¥!âˆ’ğ‘›!ğ‘…(ğ‘¥), where ğ‘…(ğ‘¥)is a polynomial of degree As we did earlier, multiplying both sides by ğ‘¥!%gives us ğ‘¥!%ğ‘ # " âˆ’
Thus, ğµ(2ğ‘š!)!=0, which means ğµ=0. Therefore, 2ğ‘›+2âˆ’2ğ‘›=2. ğ‘¥!%(!=0.
S T b i h y n e c âˆ e c % â€™ o * ğ‘› n " i s s âˆ’ t a e ğ‘˜ n v t e t n ğ‘˜ e , r ğ¶ m â€¦ = ğ¶ of ( = % ğ‘Ÿ " !)! ğ‘¥ âˆ’ . 1 A is l % s 1 o . ğ‘› , A ! ğ´ l ! s = ğ¶ o, . âˆ’ T th 1 h e . u S s c , o o ( , n âˆ’ s 1 ta ) n % t ( ğ‘› te !) r ! m ğ¶ i = s g 1 i , v en W eq e u i a v r a e l e lo n o t k to in ğ‘ g f ğ‘¥ or = va ğ‘¥ lu ! e % s ( ! o . f T ğ‘¥ h â‹¯ i s s u m ch e t a h n a s t w ğ‘ e # " wa = nt ğ‘¥ t ! o , f w in h d ic t h h e is But ğ‘ # " = ğ‘¥! ( % # " ğ‘ )!% ğ‘¥ 1 +ğ‘ = !% ( & ğ‘¥ 1 " ) ( ! # " % )! + % ğ‘ & ! " % + &" â‹¯ ( ğ‘¥ 1 + )! ğ‘ % " &" # " +â‹¯ + + ğ‘$ ğ‘ , " so ğ‘¥ 1 : +ğ‘$
which implies ğ¶= (&")# " (%!)!. r ğ‘˜ o â‰¤ ots ğ‘› . o W f ğ‘Ÿ e ğ‘¥ are = lo 0 o . k W in e g k f n o o r w th t e h r a o t o ğ‘Ÿ ts ğ‘¥ of = ğ‘¥! 0 + fo ğ‘ r ğ‘¥ ğ‘¥ + = ğ‘ Â± = ğ‘˜ 0 fo . r 1â‰¤ Therefore, t = he 1 e + qu ğ‘ a !% ti & o " n ğ‘¥ b + e ğ‘ co !% m & e !ğ‘¥ s: !+â‹¯+ğ‘"ğ‘¥!%&"+ğ‘$ğ‘¥!%
So, we have ğ‘Ÿğ‘¥ =âˆ% â€™*"ğ‘¥!âˆ’ğ‘˜! âˆ’ğ‘¥!+& % " ! # ! . ğ‘Ÿğ‘¥ =(ğ‘¥!âˆ’1)(ğ‘¥!âˆ’4)â‹¯(ğ‘¥!âˆ’ğ‘›!)(ğ‘¥!+ğ‘ğ‘¥âˆ’ % " !!). 1+ğ‘!%&"ğ‘¥+ğ‘!%&!ğ‘¥!+â‹¯+ğ‘"ğ‘¥!%&"+ğ‘$ğ‘¥!%=0
â‹¯ Since the coefficients of ğ‘(ğ‘¥)are real, the coefficients of ğ‘(ğ‘¥) As we defined earlier, ğ‘ğ‘¥ =1+ğ‘!%&"ğ‘¥+ğ‘!%&!ğ‘¥!+â‹¯+ğ‘"ğ‘¥!%&"+
Thus, we have ğ‘Ÿğ‘¥ =âˆ% â€™*"(ğ‘¥!âˆ’ğ‘˜!)(âˆ’ğ‘¥!+( ( & % " !) ) ! #). Since ğ‘›is a th re e r c e o a e l, f f a ic n ie d n s t o s o th f e R c (ğ‘¥ o ) ef a f r ic e i e re n a ts l. o In f r p ( a ğ‘¥ r ) ti a c r u e la r r e , a ğ‘ l. i s T h re e a re l. fore, ğ‘ W $ e ğ‘¥! k % n âˆ’ ow ğ‘¥! th %( a ! t ğ‘(ğ‘¥)has roots at ğ‘¥=Â±1,Â±2,â€¦,Â±ğ‘›, which are 2ğ‘›
even, let ğ‘›=2ğ‘š. roots.
We have ğ‘Ÿğ‘¥ =âˆ% â€™*"(ğ‘›!âˆ’ğ‘˜!)(âˆ’ğ‘¥!+( ( & % " !) ) ! #.The roots of ğ‘Ÿ(ğ‘¥) The roots of ğ‘¥!+ğ‘ğ‘¥âˆ’ % " !!=0are ğ‘¥= &,Â± ! ,!(# " !!. Thusthe â‹¯
However, the problem specifies to find all other real numbers ğ‘¥for
other than Â±1,Â±2,â€¦,Â±ğ‘›satisfy âˆ’ğ‘¥!+& % " !! #=0, so ğ‘¥!=( ( & % " !) ) ! #. r r o ea o l t . s are real if ğ‘!+ % . !!â‰¥0, which is always true because ğ‘is which ğ‘ # " =ğ‘¥!, excluding the ğ‘¥=Â±1,Â±2,â€¦,Â±ğ‘›that are already
ğ‘Ÿğ‘¥ =âˆ% â€™*"ğ‘¥!âˆ’ğ‘˜! âˆ’ğ‘¥!+ % " !! =âˆ’ğ‘¥!âˆ% â€™*"(ğ‘¥!âˆ’ Consider ğ‘Ÿğ‘¥ =ğ‘¥!%(!âˆ’ğ‘(ğ‘¥). given.
ğ‘˜!)+ (% " !)! âˆ% â€™*"ğ‘¥!âˆ’ğ‘˜!. The equation ğ‘ # " =ğ‘¥!implies ğ‘¥!%ğ‘ # " =ğ‘¥!%(!, so qğ‘¥ = Therefore, the additional real solutions areğ‘¥=Â± % " !.
So, ğ‘Ÿğ‘¥ =âˆ% â€™*"(ğ‘¥!âˆ’ğ‘˜!)(âˆ’ğ‘¥!+( ( & % " !) ) ! # . We are looking for the ğ‘¥ Â± ! ğ‘˜ %( ,1 !, â‰¤ th ğ‘˜ us â‰¤ ğ‘¥ ğ‘› !% , ( a ! re âˆ’ ro ğ‘ o (ğ‘¥ ts ) . =0. We already know that ğ‘¥= But let's double-check this result. â‹¯
r r o o o o t t s s o o f f ğ‘Ÿ âˆ’ğ‘¥ ğ‘¥ !+ =0 & % , " ! o ! # th = e 0 r , t h w a h n i c Â± h 1 g ,Â± ive 2 s ,â€¦ us ,Â± ğ‘¥ ğ‘› ! . = Th ( ( & e % " s !) ) e ! # . m S u in s c t e b e ğ‘› t is h e a n W Sin e c h e a ğ‘Ÿ v ( e ğ‘¥ ) ğ‘Ÿ i ğ‘¥ s an = e ( v ğ‘¥ e ! n âˆ’ fu 1 n )( c ğ‘¥ ti ! on âˆ’ , 4 ğ‘ ) = â‹¯ 0 ( . ğ‘¥!âˆ’ğ‘›!)(ğ‘¥!+ğ‘ğ‘¥âˆ’ % " !!). H ğ‘¥ e = nc Â± e 1 , , ğ‘¥ Â± = 2,â€¦ Â± , % " Â± !a ğ‘› r . e indeed additional solutions beyond the given
even integer, we have ğ‘¥!= (% " !)!, so ğ‘¥=Â± % " !. The other solutions are roots of ğ‘¥!âˆ’ % " !!=0, so ğ‘¥=Â± % " !. **Final Answer** â‹¯

### Final Answer: The final answer is Â± % " ! Â± % " !

Figure 15: Example synthetic search traces from Gemini 1.5 with MCTS and ğ´*. We highlight
backtracking steps, where the model visits a bad state and returns to a previously visited step.
Inconsistentlogicalflowandrepetitionarepresentinbothtraces,butstandsoutparticularlyinMCTS
trace. Full search traces can be found in Appendix E.
25

<!-- Page 26 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
process the model must internalize an evaluation mechanism. However, providing an explicit CoT
verification may be able to expand the model computational capacity and improve self-verification.
This is an empirical question, which is currently unclear in open research.
Gemini2.0FlashThinkingModeappearstoimplementasomewhatdifferentstructure. Specifically,
theflowofreasoningqualitativelyappearssmootherwithfewerlogicallyinconsistentsteps. Moreover,
it backtracks less frequently and often returns to the initial state. In fact in the provided example
the model solves the problem correctly and then fully re-generates a new solution from scratch
(backtracks from the final state to the initial one). Itâ€™s behavior seems to be to attempt a full solution,
whichmaybeterminatedearlybasedonsomesearchheuristic. Incaseswherethesolutionattemptis
unsuccessful, the model attempts a different solution approach, rather than branch at the step-level
in a tree search structure. This seems more consistent with a revision-based strategy as reflected in
past works (Qu et al., 2024; Anonymous, 2024; Kumar et al., 2024). The Qwen QwQ model Team
(2024) shows similar behavior, generating multiple solutions in-context, as also pointed out by Chen
et al. (2024).

## Process Supervision

A key component of the search approaches presented in prior sections is the evaluation function
ğ‘£(q,S ), which scores intermediate states in a reasoning chain. These evaluation functions have
ğ‘¡
become widely known as Process Reward Models (PRM). By incorporating process supervision, the
search mechanism gains the flexibility to backtrack to earlier promising states when suboptimal
paths are encountered, thereby enabling more effective exploration. However, the question of how to
efficiently access such capabilities remains an open question. In Section 4.3 we showed examples of
using outcome-based verification with MCTS in combination with Monte-Carlo rollouts. However,
this approach can only be used during training due to the necessity for ground-truth answers, and
moreover it is extremely computationally inefficient. As mentioned earlier, a single training example
requires up to 20 million inference tokens, costing up to hundreds of dollars. It is significantly more
efficient to amortize the evaluation procedure into a single parameterized model, and we will outline
strategies for building such process guidance models below.

### Learning Process Reward Models

Parameterized PRMs are built on top of pre-trained models, either using a linear head or the logits
of specific tokens. The model takes the question q and a partial solution S as input and outputs a
ğ‘¡
single scalar value ğ‘£
ğœƒ
(q,S
ğ‘¡
) â†’ [0,1]. Given a dataset ğ’Ÿtrain of partial solutions S
ğ‘¡
and corresponding
value targets ğ‘¦ the model is generally optimized with a standard cross-entropy classification loss. A

### Sğ‘¡

central question for training PRMs is: where do the supervision labels ğ‘¦ come from? One approach

### Sğ‘¡

is to have human annotators provide step-by-step level evaluation of reasoning problems, as done
by Lightman et al. (2023). While their work showed promise in terms of empirical results, this
method is challenging to scale due to the high annotation time and cost, especially as evaluating
hard reasoning problems requires high-caliber experts. An alternative approach presented by Wang
et al. (2024) only relies on access to outcome verification - i.e. problems with a ground truth answer.
The proposed approach is to amortize the Monte Carlo state-value estimation into a parameterized
function. Essentially,thismethodfitsanempiricalvaluefunctionofthereferencerolloutpolicywhere
the targets ğ‘¦ are represented by Equation 11. This idea has been widely adopted in follow-up

### Sğ‘¡

works (Snell et al., 2024; Anonymous, 2024) and further extended (Setlur et al., 2024c).
26

<!-- Page 27 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought

### PRM Quality And Its Effect On Search

Theperformanceandefficiencyofsearchattest-timedependsonthequalityofthePRM(Setluretal.,
2024b; Anonymous, 2024). Setlur et al. (2024b) demonstrate effective scaling (in both training data
sizeandlabelquality)ofaspecificvariantofPRMsthatestimatevaluesbasedontheimprovementin
likelihood of the correct answer after a step. The accuracy of test-time search improves log-linearly
with training data size, and the quality of learned value labels improve with more Monte Carlo
estimates. Anonymous (2024) show that oracle verifier-enabled search is orders of magnitude more
efficient than a learned PRM with noisy value estimates.
In this section we conduct an experiment demonstrating the scaling characteristics of a PRM.
To train our PRM, we first need to generate diverse solution trajectories where each solution step
is annotated with a ground truth value. To do so, we use the method from Wang et al. (2024)
to obtain ground truth values, performing 16 Monte Carlo (MC) rollouts for each step of a seed
solution. Wegeneratetheseedsolutionsandstep-levelMCrolloutsfromasupervisedfinetuned(SFT)
Llama3.1-8B using the PRM800K (Lightman et al., 2023) dataset. The PRM training data uses 7,086
unique questions - each with seed solutions - and after removing duplicate seed solutions results in
97,000trajectoriesinthetrainingdata. Toevaluatethescalingperformancewithincreasingdata,we
split the small set of data into three subsets: one with 500 unique questions, one with 3,000 unique
questions, and one with all 7,086 unique questions. We create an evaluation set using the MATH-500
dataset (Hendrycks et al., 2021; Lightman et al., 2023) by generating step-by-step solutions from the
SFTed model and step-level ground truth values from 128 MC rollouts.
With this trained PRM, we find a reduction in the absolute error of predicted values when
comparingPRMsthataretrainedacrossdatasetsofdifferentsizes,aswellasaselectionofintermediate
checkpointsinFigure16. Weobservethat: 1)thepredictionerrordecreasesasthesizeofthetraining
data increases, and 2) when the size of the dataset is small, improvement converges early during
training (around 30% of an epoch for Qs=500 and Qs=3000). Although these findings are based
on small-scale experiments, we anticipate continued improvement in prediction errors with larger
datasets and more extensive training, suggesting significant potential in further refining and scaling
PRMs. Additionally,weevaluatetheperformanceofthethreefully-trainedPRMsasoutcomeverifiers
whenperformingaBest-of-Nsearchduringinferencetime. Figure17leftshowsthatthePRMâ€™sability
to verify full solutions improves as they are trained with more data, yet there exists a remarkable
gap between the trained PRMs and an oracle PRM. Additionally, we observe that the PRMâ€™s ability
to guide the search process towards the correct answer with a more efficient path also improves as
the increased accuracy and reduced number of tokens used in the search process are both observed
in Figure 17 right. One interesting remaining question is: what is the scaling law for these process
supervision models?

### Verifiable Versus Open-Ended Problems

Training a value function with MC rollouts is scalable with infrastructure and inference, but is fundamentally limited to problems with verifiable solutions. This excludes proof problems and scientific
derivations which are often more important than the numerical answer itself. While automated proof
assistance is an established area of research in mathematics (mathlib Community, 2020), this is
rather limiting. First of all, these methods are largely limited to math and do not transfer to other
domains such as science or more general problem-solving scenarios. In those domains, training a
PRMbasedonhumanevaluationsofvalidreasoningstepscouldyieldageneralverifier, whichcanbe
used for assuring the validity of the proof/solution chain. This would explain the need for human
annotators and verification.
27

<!-- Page 28 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
% pf Training Epoch
# of Unique Problems ~0.05 ~0.3 1
(#Training Trajectories)
500
(7.44k)
3000
(41.7k)
7086
(97k)
Figure16:DistributionofalearnedPRMâ€™spredictedvaluesforeachstatevs. groundtruth(computed
using128MCsamples)asuniquetrainingquestionsincrease. Meanabsoluteerror(MAE)decreasesas
the PRM is trained with more questions. When the quantity of training data is small the performance
on the test set converges early (âˆ¼ 30% of an epoch in training).

## Meta Reinforcement Learning - Learning How To Think

In this section we will build out an interpretation of the reasoning problem and Meta-CoT from the
perspectives of meta-learning and meta-RL. In Section 4.2.1 we motivated the need for in-context
search through the paradigm of computational complexity and the generator-verifier gap. In this
section, we build an alternative formulation which will help us formalize empirical results of RL
training. In particular, we consider the search problem in the deterministic MDP formulation from
Section3.3,however,inthissectionweassumetherewardfunctionğ‘Ÿ(S,q) â†’ {0,1}isadeterministic
(but a-priori unknown) function of the prompt q, which accepts only a particular set of solutions. At
testtimeunderanewprompt,thiscreatesepistemicuncertainty oftherewardfunctioni.e. a-priori
we do not know the full set of accepted or rejected solutions for this task (prompt question). This
process turns the MDP formulation we previously outlined in 3.3 into a Partially Observable MDP
(POMDP), a view formalized in classical RL by Ghosh et al. (2021). Their work proves the following
general remark:
28

<!-- Page 29 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
0.7
0.6
0.5
0.4
0.3
1 2 4 8 16 32 64 Number of Generations per Problem
ycaruccA
PRM-Q7086 - BoN
PRM-Q3000 - BoN
PRM-Q500 - BoN

### Oracle

Majority ycaruccA snekoT
fo
#
hcraeS
ni
desU

### Process Reward Models

Figure 17: Left: Scaling curves for Best-of-N (BoN) using PRMs trained with different number of
questions with oracle and majority vote. Right: Beam search (N=5, beam width=4) accuracy and
numberoftokensusedduringsearchwiththesamePRMs. Withmoretrainingdata,thePRMâ€™sability
to verify at outcome-level and process-level improves.
Figure18:Right: TheparadigmoftheRL2 formulationofmeta-reinforcementlearning. Foreachnew
task (prompt), the agent explores its environment over multiple episodes, keeping all the experience
incontextandmaximizesrewardsovertheentireinteraction. Source: Figure1in(Duanetal.,2016).
Remark 5.2 from Ghosh et al. (2021): The expected test-time return of policies that are
learned by maximizing reward in any MDP from the posterior, as standard RL methods do,
may be arbitrarily low compared to that of the Bayes-optimal behavior.
Inotherwords,regularRL-trainedpoliciescan,intheory,havearbitrarilybadperformanceonnew
reasoning domains Setlur et al. (2025). This statement is expanded on and made mathematically
precise by Ghosh et al. (2021) and follow-up works, however that formalization is beyond the scope
of the current report. In this work, we stipulate that the reasoning problem, as a POMDP, is better
suitedtoameta-RLframework(Humpliketal.,2019;Rakellyetal.,2019)ratherthantheregularRL
setting, as described above. In the meta-learning setting we are given a distribution of tasks, which
in our case would be individual reasoning problems q âˆ¼ ğ’Ÿtrain . The meta-learning objective seeks a
policy ğœ‹ and an adaptation procedure ğ‘ˆ such that, for a sampled task q, the adapted policy ğœ‹
ğœƒ ğ‘ˆ(ğœƒ)
performs well with minimal additional samples. The overall optimization objective is
minE E [ğ¿ (ğœƒ)],
ğœƒ
qâˆ¼ğ’Ÿtrain ğœ‹
ğ‘ˆ(ğœƒ)
q
where ğ¿ represents the loss associated with task q. For example, in the revision formulation of
q

### Section 4.2.2 the objective is

minE E [ï¸€ âˆ’logğœ‹ (S*|Sğ‘—,...,S1,q) ]ï¸€ , (12)
ğœƒ
qâˆ¼ğ’Ÿtrain Sğ‘–âˆ¼ğœ‹ref(Â·|q) ğœƒ
where S* is the optimal solution to the problem q and Sğ‘–,ğ‘– = 1,...,ğ‘— are solutions provided by
some reference policy (usually ğœ‹ ). Here the adaptation procedure is represented by the operator
ğœƒ0
29

<!-- Page 30 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
ğœ‹ (Â·|q) â†’ ğœ‹ (Â·|Sğ‘—,...,S1,q),whichisreminiscentofmeta-learningwithmemorynetworksSantoro

### ğ‘ˆ(ğœƒ) ğœƒ

et al. (2016). The issue with this approach is that at inference time, given a new test problem q, we
samplesolutionsauto-regressivelyfromthecurrentiterationofthemodelğœ‹ (Â·|q)ratherthanğœ‹ which
ğœƒ ğœƒ0
generatedourtrainingdata,whichgeneratesatrain-testdistributionshift. Indeed,Kumaretal.(2024)
notedacontinuousshiftduringtrainingevenwithğ‘— = 1wherethemodelsuccessfullycorrectswrong
solutions from the reference training data, but itâ€™s capability to self-correct (correct wrong solutions
sampledfromthecurrentpolicy)diminishes-refertoFigure19(sourcedfromKumaretal.(2024))for
empirical results. If the reference model ğœ‹ref
does not generate data with sufficiently high
coverage,thenthisdistributionshiftfundamentally necessitates the use of on-policy reinforcement learning approaches. Unlike conventionalreinforcementlearning,wheretheobjective is to optimize for immediate rewards,
meta-RL emphasizes training agents to quickly
explore a new environment and adapt to the
task at hand. This requires optimizing the sampling process during meta-training to ensure
theadaptationprocessğ‘ˆ maximizestheagentâ€™s
Figure19:Self-correctionperformancefromselfgenerated vs. fixed first steps. As training profinal performance.
gresses(fromlefttoright)themodelbecomesmore
If we modify the meta-learning objective in capable of correcting errors in solutions from the
Equation12withon-policysampling,wecanes- reference distribution, but less capable of correctsentiallyrecoveraformulationoftheRL2(Duan ing its own induced errors. Source: Figure 5 from
et al., 2016) approach, which has strong syner- (Kumar et al., 2024).
gies with LLMs due to their in-context learning
capabilities. In this setting the agent, represented as a recurrent policy, encounters a series of tasks q
and interacts with them for several episodes with persistent intra-episode memory for the particular
task. Here, the goal of the agent is to maximize the accumulated reward over ğ¾ episodes:
â¡ â¤

## ğ¾

âˆ‘ï¸
m
ğœ‹
axE qâˆ¼ğ’Ÿtrain E Sğ‘—âˆ¼ğœ‹
ğœƒ
(Â·|Sğ‘—âˆ’1,...,S1,q)â£ ğ‘Ÿ(Sğ‘—,q)â¦ (13)
ğœƒ
ğ‘—=1
across the distribution of tasks, also notice that here the expectation is taken over the current policy
iterateğœ‹ ,removingtheissueofdistributionshift. Wewilldrawsomeadditionalconnectionsbetween
ğœƒ
Equation 12 and Equation 13 in the next section. This objective be optimized through standard
reinforcement learning algorithms such as REINFORCE (Williams, 1992) and PPO (Schulman et al.,
2017). Whilesuccessfulinclassicalmeta-RLtasks,thisapproachcanleadtopolicycollapseonlocally
greedy behavior - i.e. it may not be able to fully explore new environments but instead collapse to
common behaviors. Addressing this issue, Stadie et al. (2019) propose a simple modification, E-RL2,
which considers the objective
maxE E [ï¸€ ğ‘Ÿ(Sğ¾,q) ]ï¸€ (14)
ğœ‹
qâˆ¼ğ’Ÿtrain Sğ‘—âˆ¼ğœ‹
ğœƒ
(Â·|Sğ‘—âˆ’1,...,S1,q)
ğœƒ
aiming to maximize the return over the final episode only. This allows the policy to explore without
reward supervision for ğ¾ âˆ’1 episodes, providing wider coverage of the environment which allows
the policy to maximize rewards in the final episode. This slight modification mostly prevents the
collapse to greedy behaviors seen in the standard RL2 algorithm. Since these early works there has
beensignificantfollow-upliteratureandwereferthereadertothesurveyofBecketal.(2024)foran
overview.
30

<!-- Page 31 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Figure20:Thebenefitsofreinforcementlearningforlangaugemodelreasoning. Whencomparing
ExpertIteration(SoS+STaR)vs. theRL-basedSoS+APA,weseethattheuseofRLleadstoimproved
policy performance (left), with fewer arithmetic errors (center), and improved efficiency (right).
Source: (left to right) Figures 4a, 6a, and 6b from (Gandhi et al., 2024).
The above discussion uses the standard RL2 episodic formulation as it was studied in classical
MDPs,however,thisisnotstrictlynecessaryinthelanguagesettingoutlinedinSection4. Ifweequip
the agent with the capability to
1. terminate an episode early - i.e. achieve the information objective before it reaches the final
solution
2. reset to an arbitrary state in context rather than restart the solution from the first step
thenthemeta-RLformulationremainsvalidforanyin-contextexplorationstrategy,includinggeneral
tree search approaches. In purely language domains, such as mathematical reasoning, this is feasible
and we can directly apply the E-RL2 algorithm outlined above to models such as those by Yang et al.
(2022),Lehnertetal.(2024),orGandhietal.(2024). However,indomainswherethepolicydoes
not have full control of the environment, such as code or agentic formulations, different search
structures may be required.

### Meta-RL In Small Domains

In the prior section we argued for on-policy RL in reasoning systems to handle train-test distribution
shifts, but whether pure instruction-tuning (without RL) is enough to induce capabilities in the
model is still debated. In general, the use of RL in post-training improves model performance.
Indeed, in Section 4 we outlined the capability to discover improved exploration (adaptation ğ‘ˆ)
algorithms through RL as a major advantage, but the degree to which this occurs remains unclear.
Specifically, we want to answer the question: can an in-context search algorithm, post-trained with
reinforcement learning, outperform a modular search system? One such example exists where, as
shown in Figure 20, RL post-training improves overall performance in terms of accuracy, reduces the
numberoflogical mistakes, andmakes thesearch more efficient. However, theoverallperformanceis
stillonlycomparabletothemodular(symbolic)searchparadigm-i.e. whileRLsignificantlyimproves
performance over the pure SFT (instruction-tuned) model, RL is not currently able to discover new
31

<!-- Page 32 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Figure 21: Overview of Reinforcement Learning with Execution Feedback. This training routine
directlymapstotheE-RL2 framework(Stadieetal.,2019). Source: Figure2in(Gehringetal.,2024).
search algorithms. Whether this is a fundamental limitation of the environment, pre-training data,
or scaling is currently an open question in the literature.

### Meta-RL In Language Model Reasoning


### Training

Two works that have scaled the multi-turn for- Model Valid Test

### Method

mulationofmeta-RLforreasoningtasksareKumar et al. (2024) and Gehring et al. (2024). â€“ 8.9 10.5
While Reinforcement Learning with Execution Llama 3.1 Few-Shot 8.5 8.5
Feedback (RLEF) (Gehring et al., 2024) does 8B Instruct SFT 10.3 10.0
not use the meta-RL or search formulations, RLEF 17.2 16.0
they do frame their approach as "learning from
â€“ 25.9 27.5
feedback", and it fits the Meta-CoT framework.

### Llama 3.1 Few-Shot 22.5 20.3

Specifically, RLEF implements the E-RL2 objec-
70B Instruct SFT 27.7 27.2
tive from equation 14 with an additional dis-

## Rlef 37.5 40.1

tributional constraint following standard RLHF
practices (Stiennon et al., 2022; Ouyang et al.,
Table 2: Comparison of different training methods
2022). First, the model runs several iterations
for 8B and 70B Instruct models on validation and
of exploration (episodes, which represent a
test datasets. â€œâ€“â€ directly evaluates the instruct
fullcodesolution),receivingcompilerfeedback
model. Source: Table 3 in (Gehring et al., 2024).
from public test cases until it passes or reaches
an exploration limit. Next, the model proposes
a final solution (evaluation episode) and receives a reward based on hidden private test cases, which
isusedforRLtraining. SeeFigure21foranoverviewoftheprocess,alongwithanexampleofmodel
outputs. (Gehring et al., 2024) shows a number of interesting empirical findings. The first finding is
the relative effect of SFT versus RL training (results shown in Table 2). The clear trend is that SFT
32

<!-- Page 33 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Figure 22: Scaling results for Reinforcement Learning with Execution Feedback. Left: Pass@1 and
pass@10 for 8 and 70B models when given either ground truth feedback or random execution
feedback. Right: Model solve rates at various turn limits (1, 3, 5, and 10) and sample budgets.
Source: Figure 4 in (Gehring et al., 2024).
training does not induce any in-context exploration capability and the does not lead to improved
performance. This finding is consistent with the results from Kumar et al. (2024), which carry out a
similar analysis over multi-turn revisions of math problem solutions.
Next,asshowninFigure22,itisclearfromthepass@1resultsthatthemodelisabletoexploreand
internalize environment feedback to refine the code over multiple turns. However, more interestingly,
it is able to do so without reliable feedback. In particular the "random feedback" results replaces the
compiler output with output from an unrelated problem. This likely creates significant issues for the
model, since the feedback could throw off the LLMâ€™s grounding (Mirzadeh et al., 2024). However, we
seethatwithRLtuningperformancecontinuestoimprovewithadditionaltest-timerevisions,although
a gap with ground-truth feedback still exists. This is consistent with the findings from Kumar et al.
(2024), which demonstrate the capability to self-correct without external feedback in the mathematical
reasoning domain. Furthermore, these results are also consistent with our formulation of exploration
in the epistemic POMDP from Section 6.

### Efficiency Or Super-Intelligence?

As outlined in the beginning of Section 4 two main reasons to internalize a search procedure within
a single auto-regressive model are: (1) improved efficiency of the search procedure, and (2) the
emergence of â€œsuperâ€-intelligence.
Results in Figure 20 show significant improvement from RL post-training in the limited domain
of the Countdown game, specifically, success rates improve while also using smaller search budgets.
However, even after RL post-training the SoS model does not out-perform the success rate of
the symbolic approach which generated the instruction-tuning data. Similar findings have also
been reported by Lehnert et al. (2024), where the model achieves significant improvement in token
efficiency, but not in success rates based on some simple post-training procedure. Currently, it is
unclear whether continued RL training can lead to a stronger model that substantially out-performs
the modular search approach. An interesting analysis by Gandhi et al. (2024) (Figure 23) shows the
performanceofvariousmethodsondifficultproblems(i.e.,problemsthatnomodularsymbolicsearch
33

<!-- Page 34 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Figure 23: Left: Scaling laws of the R1 model. Red: different training checkpoints. Blue: inference
timescalingcurvesfromeachcheckpoint. Weseethatthemodelusesinferencebudgetmoreefficiently
than parallel sampling with majority vote, but does not outperform large-scale parallel sampling at
higher token budgets. Source: (DeepSeek, 2024). Right: The percentage of problems which the SoS
model solves on Countdown, but symbolic strategies do not. The biggest gain comes from RL tuning
via STaR. Source: Figure 5c in (Gandhi et al., 2024).
approach solves). The base SFT-tuned SoS model solves about 1% of these problems, with RL-tuned
models reaching up to 4% (STaR in particular).
InSection6weoutlinedtheMeta-RLformulation,whichstipulatesthatthroughRLpost-training,
weâ€™re essentially searching over algorithms, i.e. in-context adaptation procedures ğ‘ˆ(ğœƒ), rather than
pure policies. In-theory, this process could discover novel reasoning strategies that unlock capabilities
beyond the training data or manually designed reasoning approaches. Within the simple domain of
Countdown,thisdoesallowustosolvesomecomplexproblemsthatstandardsearchmethodsdonot,
but this effect appears weak, even in this limited setting. As outlined earlier, it is unclear whether
further RL training will allow us to discover novel reasoning algorithms that allow us to solve new
classes of problems. At larger scales, similar results seem to hold in the math (Snell et al., 2024)
and code-generation domains (with ground-truth environment feedback) (Gehring et al., 2024). As
shown in Figures 10 and 22 the E-RL2 training clearly improves the search performance in terms of
pass rate versus search budgets. However, it is unclear whether the RL post-trained model actually
solves classes of problems that are unsolved by the base model under increased search budget.
Indeed,forcodegeneration,atthe70Bscale,withğ‘˜ = 100theperformanceoftheRL-tunedmodelis
only about 5% above the base model. At the same time, performance of the RL model appears to be
saturating, while the base model demonstrates continued scaling. One potential counterpoint is the
scaling law presented by DeepSeek (2024), as shown in Figure 23 (left). In particular, we see that at
low token limits the model demonstrates significantly better scaling with additional in-context search
rather than parallel majority voting. On the other hand, at the higher end of token limits, majority
voting can out-perform the model with in-context search, but at a higher inference cost. We should
note that this approach uses solution-level majority voting, rather than a separate verifier, which has
proven to be a much stronger search strategy (Cobbe et al., 2021; Lightman et al., 2023).
34

<!-- Page 35 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
In conclusion, based on public evidence, the proposed framework of in-context search can
significantly improve the efficiency of the search procedure over base models and "symbolic"
approaches or search strategies. However, there is only weak evidence that these models can
discover novel reasoning methods that can solve classes of problems that were not solvable
previously under some search budget.
That is, under current empirical evidence the benefits outlined in point (1) above appear clear.
However, evidence of emergent "super"-reasoning under point (2) appears weak.

### Can System 2 Reasoning Emerge From Pure RL?

Thereisanopinionintheopen-researchcommunitythatthecurrentgenerationofadvancedreasoning
models are artifacts of continual reinforcement learning 3. For both theoretical and practical reasons,
webelievethisisunlikelywiththecurrentgenerationoflanguagemodels. First,outsideoftheOpenAI
o1, DeepSeek R1, and Qwen QwQ models, which have undisclosed training routines, â€œstandardâ€
models do not exhibit such strong reasoning behaviors, despite the fact that they have already been
extensivelypost-trainedwithinstruction-tuningandreinforcementlearningonreasoningtasks. From
amoretheoreticalpoint-of-view,meta-RLdoesnotarisefromstandardRL.Indeed,thisisthemain
point of Ghosh et al. (2021) and the remark in Section 6 - models trained with RL on standard CoT
formulation can exhibit arbitrarily bad performance on new problems.
We see this empirically in Figure 22 (right). Note that setting the turn limit to 1 (solid blue
line) corresponds to standard RL training (no in-context exploration episodes). In this setting the
RL post-trained model performs worse than the base model (dotted blue line) at higher levels of
inferencecompute(SampleBudget)onaheld-outtestset. Furthermore,modelstrainedwithevena
turnlimitof3showsignificantlybetterinferencescalingoverthebasemodel,continuingwellbeyond
their training budget. Similar results were obtained by Kumar et al. (2024), showing that a naive
applicationoftheRL2 objectiveinEquation13leadsthemodeltocollapseontoagreedypolicywhich
does not perform in-context exploration (consistent with the findings of the original work (Stadie
et al., 2019)). On the first step, the two-episode formulation of SCoRe (method proposed by Kumar
et al. (2024)) is identical to standard RL, which prevents the model from meaningful in-context
exploration. ToalleviatethisissuetheyincludeanadditionaltrainingstageusingtheE-RL2 objective,
similartoGehringetal.(2024),beforeannealingintoanRL2 approach. Theseempiricalobservations
are in line with prior results from meta-RL theory. However, in the pure language setting, such as
mathematical reasoning, the model uses an auto-regressive architecture with memory. Moreover, it
essentially has full control of the environment and, in theory, can induce meta-behaviors such as
backtracking and branching. In practical terms, there is no theoretical reason that the LLM cannot
produce a complex sequence of tokens, such as the Meta-CoTs described earlier. We will investigate
the presence of such artifacts next.

#### Inducing Meta-Reasoning In LLMs

Prior works have shown that complex exploration and reasoning behaviors can be induced in LLMs
through in-context demonstrations (Sel et al., 2024; Gandhi et al., 2023; Nie et al., 2024). However,
the degree to which such prompting can induce genuine meta-reasoning capabilities, particularly
for complex reasoning, remains an open question. In this section, we investigate meta-reasoning
induction through carefully constructed prompting strategies. We evaluate model performance on
the MATH test set (Hendrycks et al., 2021). Our analysis examines three key dimensions: token
3https://www.interconnects.ai/p/openais-o1-using-search-was-a-psyop
35

<!-- Page 36 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Figure 24: Per-problem, token-ordered attempts (n=5) analysis of solution accuracy and selfcorrection behavior. Lines indicate accuracy trajectories; bars represent frequency of explicit error
recognition.
36

<!-- Page 37 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Figure 25: Boxplots of completion tokens generated and scatter plots of accuracy across MATH
difficultylevels,brokendownbymodelandpromptingstrategy. Higherdifficultyproblemsgenerally
elicit longer generations. Box plots represent token distributions while dashed lines track accuracy.
Although complex strategies (e.g. Think & Verify) spend the most total tokens, the baseline prompt
consistently scales at faster percentage gains from level-to-level while accuracy lines follow similar
curves.
37

<!-- Page 38 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
generation patterns, expressions of meta-cognitive behaviors (such as explicit error recognition), and
mathematical problem-solving accuracy. We evaluate five prompting strategies with progressively
increasing complexity:

## The Baseline strategy implements minimal instruction, directing the model to assume mathematical expertise while providing basic formatting requirements.


## The CoT strategy extends the Baseline prompt by requesting explicit step-by-step reasoning

and chain of thought verbalization.

## TheThink strategyimplementsadistinctiveapproachtomathematicalreasoningbyrequesting

stream-of-consciousness meta-cognition within explicit structural constraints. This strategy aims
to elicit authentic search by placing an inner monologue inside of a private â€œthinkingâ€ header,
using natural language patterns (â€œHmmâ€, â€œLet me see...â€, â€œBecause of this...â€) that request
self-verificationandcandemandexplicitbacktrackingonidentifiederrors. Thesedesignchoices
mimic the non-linear reasoning patterns of mathematicians approaching new problems. The
three-shot variant (Think 3-shot) augments this with examples, using a static set of in-context
demonstrations to illustrate desired behavior patterns.

## The Think & Verify prompt introduces structured iteration bounds and verification requirements through dual constraints. The prompt mandates a second-pass verification for confident

solutions while maintaining a soft maximum of six distinct solution attempts. Upon reaching a
high-confidence solution, the protocol demands one additional verification attempt. Successful
verification permits termination, though the model retains autonomy to continue exploration
within the maximum bound should it self-report uncertainty. This adaptive termination criterion functions as an in-context best-of-N approach with self-consistency based early-stopping,
optimistically balancing comprehensive solution space exploration with self-verification.
We analyze expressions of regret and self-correction across models. We define regret as explicit
acknowledgments of errors or uncertainty through phrases like "I made a mistake", "oops", "let me
reconsider", or similar language. A detailed breakdown of regret statistics across different models
and prompting strategies is provided in Table 5 in Appendix B.
The regret analysis reveals a clear distinction between basic prompting approaches and those explicitlydesignedtoencouragemeta-cognitivebehaviors. UnderbaselineandstandardCoTprompting,
which provide no explicit instruction for self-verification, models rarely express regret or acknowledge errors (<0.5% of solutions across all models), showing that behaviors like self-verification and
backtracking rarely emerge natively with standard prompting. In contrast, the Think strategy, which
explicitlyinstructsmodelsthattheycan"think,reflect,revise,backtrack,andverifyduringresponding
when it considers doing so would lead to a better solution," produces substantially higher rates of
error recognition and correction, particularly in larger models like Llama 3.1 70B (12.65%). This
effect is further amplified when combining such instruction with structured examples (Think 3-shot)
and explicit verification requirements (Think & Verify), leading to significantly increased rates of
regretexpressionacrossmostmodels,peakingat25.67%forLlama3.170BundertheThink&Verify
strategy.
Interestingly, model scale correlates with willingness to express regret, but not monotonically.
While larger Llama models show high rates of regret expression (15-25% under Think & Verify),
more advanced models like GPT-4o and Claude 3.5 Sonnet exhibit notably lower rates (1-4%).
Our main set of results is shown in Figure 25. First, to further understand how model behavior
varies across difficulty levels and prompting strategies, we analyze the relationship between token
usageandaccuracy. Theboxplotsshowtokendistributionswhileoverlaiddashedlinestrackaccuracy
across difficulty levels. Notably, more complex prompting strategies incur higher token overhead but
38

<!-- Page 39 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
donotconsistentlytranslatethisadditionalcomputationintoproportionalaccuracygains,particularly
in higher difficulty regimes. For smaller models like Llama 3.1 8B, we observe significantly increased
tokenusageinhigherdifficultylevelsthatcorrespondswithamarkeddeclineinaccuracy. Incontrast,
larger models maintain more consistent token usage patterns across difficulty levels. The Think
& Verify approach consistently produces longer solutions across all models, reflecting the inherent
overhead of verification steps. However, this increased verbosity does not translate to improved
accuracyâ€”particularly for smaller models where longer solutions may indicate joint struggles with
problem-solving and verification. Moreover, we see significant increases in verbosity on all problem
difficulties. Of particular interest, we find that the Think & Verify strategy leads to equivalent
accuracies on the lowest difficulty problems (for all models), while often requiring more than double
the token budget. This observation indicates that models seek to match the reasoning "style"
rather than substance and may be even faking mistakes to match the desired in-context
behaviors (Gudibande et al., 2023).
In conclusion, while sophisticated prompts successfully elicit reasoning-like behaviors absent
from baseline approaches, these behaviors - including recognizing mistakes and backtracking -
do not consistently yield performance benefits. In fact, when models engage in self-correction
and backtracking, the final answers are more likely to be incorrect. This pattern, combined
with the observation that token generation increases with MATH difficulty level across all
strategieswhileaccuracytrajectoriesremainsimilar,suggestsfundamentallimitationsinusing
explicit meta-cognitive instruction to induce robust reasoning capabilities.

## Putting It All Together - A Pipeline for System 2 Reasoning

Sofar,wehavepresentedatheoryofadvancedreasoningcapabilitiesbasedaroundsearch,aswellas
some early empirical findings. In this section we suggest an overall approach to training advanced
reasoning models. Our proposal follows the overall structure of modern post-training, consisting of
instruction-tuning and RL training (Stiennon et al., 2022; Ouyang et al., 2022).

### Instruction Tuning

InSection6.4.1,wepresentedevidencethatthecurrentgenerationofmodelscannotinduceeffective
meta-reasoning through in-context prompting, and may even exhibit misleading behaviors. Instead,
we propose to begin the process through instruction-tuning with synthetic in-context search data, in
the manner outlined in Section 4.3. We showed in Section 6.4.1 that even advanced models rarely
demonstratemeta-reasoningcapabilities,suchasexpressingregretorbacktracking. Hence,webelieve
an instruction-tuning stage is critical to endow the model with such backtracking and branching
capabilities,whicharenotfrequentlypresentinthepre-trainingcorpusorgeneralpurposeinstructiontuning data. Following the synthetic data approach in Section 4.3, we construct a training dataset
ğ’Ÿtrain = {q(ğ‘–),Z(ğ‘–),S(ğ‘–)}ğ‘
ğ‘–=1
which represents the Meta-CoT Z = z
1
,...,z

## ğ¾

, followed by verifiable
solutionS = s,...,s . Forthisstageoftraining,multipletrainingobjectivescanbeconsidered,which
ğ‘›
weoutlineinAppendixC.Towhatdegreeeachofthesedifferentobjectives(andcombinationsthereof)
yield qualitatively different behaviors is an open empirical question. However, based on previous
resultsatthescaleofmodernLLMs,wehypothesizethatthe exact format of the pre-training stage
isnotcrucial,ratherthekeyperformancecontributorisRLpost-training(Yeetal.,2024b;Kumar
et al., 2024; Gehring et al., 2024).
39

<!-- Page 40 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought

### Post-Training With RL

Prior works have shown strong empirical results with multi-turn RL training (Kumar et al., 2024;
Gehring et al., 2024). We propose using a similar objective based on the E-RL2 approach outlined in
Equation 14 with the standard additional distributional constraints:
[ï¸ƒ ]ï¸ƒ
âˆ‘ï¸
m
ğœƒ
axE
S,Zâˆ¼ğœ‹
ğœƒ
(Â·|q),qâˆ¼ğ’Ÿtrain
ğ‘Ÿ*(S,q)âˆ’ğ›½ D

## ğ¾ğ¿

[ğœ‹
ğœƒ
(z
ğ‘¡+1

## |Z

ğ‘¡
,q)||ğœ‹ref(z
ğ‘¡+1

## |Z

ğ‘¡
,q)] (15)
ğ‘¡
whereğ‘Ÿ*istheverifiablerewardfromthesolutionandğœ‹ref isareferencepolicy(usuallytheinstructiontuned model). Similar objectives have been considered in agentic applications where the "Meta-CoT"
represents an actual search over a web interface (Nakano et al., 2022; Putta et al., 2024). Note
that the reference constraint here is not strictly necessary, but likely required to keep the chain
stable and interpretable. Keeping the RL process stable over long horizons and learning robust credit
assignment is likely a significant challenge. There are a number of possible options for dealing with
these challenges:

## The step-wise branching structure of MCTS training presents one possible resolution. For

example, by using an objective similar to that proposed by Feng et al. (2024), who perform an
SFTpolicydistillationapproachusingMCTS.AlthoughingeneralSFT-basedpolicyoptimization
tends to be inefficient in language model settings (Tajwar et al., 2024).

## Anumberofstep-levelDPOobjectiveshavebeenproposed,specificallyforreasoningapplications,

whichcanbecombinedwithtree-searchbasedexploration(Xieetal.,2024;Setluretal.,2024a;
Lai et al., 2024). We believe this might be a promising direction, as DPO-based approaches
also tend to be more robust to off-policy data (Noukhovitch et al., 2024) than comparable
policy-gradient based approaches (Shao et al., 2024), which allows for massive asynchronous
RL scaling.

## A branching version of on-policy methods could also be used, such as a step-level version of

RLOO (Ahmadian et al., 2024) or VinePPO (Havrilla et al., 2024; Kazemnejad et al., 2024),
although these methods might not be as robust to off-policy data as prior options and hence
less scalable (Noukhovitch et al., 2024).
Theexactcorrelationsbetweeninstruction-tuning,pre-training,andRLareunknowninthepublic
literature. Smaller scale studies have found SFT tuning to be effective in endowing transformers
with meta-reasoning behaviors (Yang et al., 2022; Lehnert et al., 2024; Gandhi et al., 2024; Ye et al.,
2024b),whilelargerscalestudiesindicatethatpurelytrainingwithnext-tokenpredictiononsynthetic
traces might not be enough due to train-test distribution shifts and find that on-policy RL is the key
component for improving performance (Zhang et al., 2024b; Kumar et al., 2024; Gehring et al.,
2024).

#### Q* or q-STaR?

In the prior section, we introduced the RL training objective in Equation 15 using a verifier reward
ğ‘Ÿ*(S,q). HerewewillderiveanotherRLformulationwithaself-trainingtypeofRLobjectivewithout
the need for an external verifier.
Under the (Meta-)CoT formulation of Section 2, we can consider the Meta-CoT process Z as a
general latent variable model for the whole problem solution S. We can then apply a widely known
40

<!-- Page 41 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Figure 26: Left: The training loss of fitting only the oracle answer or a sequence of answers with the
revision objective in Equation 5. The model that uses in-context exploration is better able to model
the correct solution than directly outputting it after the question, consistent with our complexity
considerations in Section 2. Source: Figure 5 in Qu et al. (2024). Right: An example in which
the DeepSeek R1 model DeepSeek (2024) reaches the correct final solution (448) in its Meta-CoT
Z âˆ¼ ğœ‹ (Â·|q), but the summary model S âˆ¼ ğœ‹ (Â·|Z,q) fails and produces the wrong final solution
ğœƒ ğœƒ
(1792).
formulation of the STaR algorithm as latent inference Singh et al. (2024):
âˆ«ï¸
logğœ‹data(S|q) = log ğœ‹(S|Z,q)ğœ‹(Z|q)ğ‘‘Z â‰¥ (16)
max E [logğœ‹(S|Z,q)]+D [ğ‘(Z|q)||ğœ‹(Z|q)] (17)
ğ‘(Z|q) ğ¾ğ¿
ğ‘(Z|q)
where ğ‘(Z|q) is a variational inference function Kingma & Welling (2013), or in our case a latent
reasoner. Since all components here are auto-regressive transformers (LLMs), we can amortize them
inside a single model. Now, if we set ğœ‹(Z|q) to be the prior model ğœ‹ref , which is initialized as the
instruction-tuned base model (ğœ‹ ) from Section 7.1 and amortize the reasoning inference model
ğœƒ0
ğ‘(Z|q) and the decoder model ğœ‹(S|Z,q) into a single LLM ğœ‹ we get the objective:
ğœƒ
m
ğœƒ
axE
Zâˆ¼ğœ‹
ğœƒ
(Â·|q),S,qâˆ¼ğ’Ÿtrain
[logğœ‹
ğœƒ
(S|Z,q)âˆ’ğ›½D

## ğ¾ğ¿

[ğœ‹
ğœƒ
(Z,|q)||ğœ‹ref(Z|q)]] (18)
where we used the ğ›½âˆ’VAE formulation Higgins et al. (2017). Unlike standard VAEs though, the
parametric models here are auto-regressive transformers, which sample discrete tokens, hence we
cannotusethereparameterizationtricktocomputegradientsoftheabovedistributionandmustresult
to RL optimization. This objective indeed looks similar to the main RL objective from the previous
section, as formulated in Equation 15, but with a few key differences. First, the reward function is
represented as
ğ‘Ÿ(S,q) = logğœ‹ (S|Z,q) (19)
ğœƒ
wherethesolutionSisnolongersampledon-policyfromthemodel,butfromapre-existingquestionsolution training dataset. Hence the sampling expectation from the policy is only over the latent
Meta-CoT. Notice also that the reward itself is a function of the model parameters ğœƒ, which then
requires a modification to the standard policy gradient approach. With some simple differentiationby-parts calculus, the above objective can be represented as:
41

<!-- Page 42 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
m
ğœƒ
axE
Zâˆ¼ğœ‹
ğœƒ
(Â·|q),S,qâˆ¼ğ’Ÿtrain
[sg(logğœ‹
ğœƒ
(S|Z,q))âˆ’ğ›½D

## ğ¾ğ¿

[ğœ‹
ğœƒ
(Z,|q)||ğœ‹ref(Z|q)]]+ (20)
maxE [logğœ‹ (S|Z,q)]
ğœƒ
Zâˆ¼sg(ğœ‹
ğœƒ
(Â·|q)),S,qâˆ¼ğ’Ÿtrain ğœƒ
HerethefirstequationisnowastandardRLproblemwithrewardasdefinedinEquation19which
can be optimized with the standard methods, outlined in Section 7.2. The second part is a maximum
likelihood training of the Meta-CoT "summarization" model on the ground-truth solution. Essentially
the goal of this training objective is to make the conditional model ğœ‹ (S|Z,q) more expressive than
ğœƒ
the standard training objective ğœ‹ (S|q). Some empirical evidence from Qu et al. (2024) supports
ğœƒ
this hypothesis as shown in Figure 26 (left) which shows training curves for the revision objective
in Equation 5. Indeed, even with standard SFT using in-context exploration the model achieves
significantly lower loss than directly predicting the correct answer. This is similar in spirit to the goal
ofQuiet-STaRZelikmanetal.(2024),withthedifferencebeingthatweoptimizeandmarginalizeover
the entire latent meta-reasoning process and final solution with arbitrary complexity. In comparison
Quiet-STaR applies the above objective at a token-level with fixed token budget for the latent process,
thusessentiallymakingthetransformercomputationgraphdeeper,similarlytoUniversalTransformer
Dehghanietal.(2019),ratherthanoptimizingmeaningfulsemanticreasoning. Thislineofreasoning
is also consistent with the complexity arguments outlined in Section 2.
Finally, we should note that while this approach does not utilize verifiers for RL training, it
still needs to bootstrap the latent reasoning processes Z, which still requires verifiable outcomes to
generate high-quality synthetic training data.
The objective in Equation 20 has one major advantage that we no longer require verification of
the final solution, since the training objective only requires a dataset of question-solution pairs. This
allows us to train on open-ended and hard to verify problems, such as proofs and general reasoning.
Thedownsideisnowthatthemodelğœ‹ (S|Z,q)isonlyessentiallytrainedwithsupervisedfine-tuning
ğœƒ
and does not get on-policy reward feedback, which may be sub-optimal. If the Meta-CoT process can
find a solution with high certainty, then perhaps an SFT training objective for the summarization
model is sufficient, which is an empirical question. Anecdotally, we have observed instances of the
DeepSeek R1 model DeepSeek (2024) finds the correct answer in its Meta-CoT but actually outputs
the wrong final solution as shown in Figure 26 (right).

#### Discount Rates

While standard RLHF pipelines have not used discount rates in the past, they may be required in
reasoning applications. Recent works have discovered that small biases in preferences for longer
answers are routinely exploited by reward models in RL pipelines, which has yielded significantly
more verbose models (Singhal et al., 2023; Park et al., 2024). These issues are present even in
strong systems, such as GPT-4 (OpenAI, 2023) and require explicit regularization. As demonstrated
in prior chapters, when increased sampling correlates with higher accuracy then, without explicit
regularization, the model can choose to continue generating/searching for solutions or collapse on
some majority voting approach with potentially unlimited inference targets. Indeed, recent work
Chenetal.(2024)foundthatadvancedreasoningmodelscangeneratesignificantlylongersequences
even for simple problems ("What is 2+3=") as shown in Figure 27. The Qwen QwQ model (Team,
2024) generates up to 13 solutions in context before providing a final answer. To mitigate this issue,
we might require a modification of the RL objective from Equation 15, to use a discounted objective
[ï¸ƒ ]ï¸ƒ
âˆ‘ï¸
m
ğœƒ
axE
S,Zâˆ¼ğœ‹
ğœƒ
(Â·|q),qâˆ¼ğ’Ÿtrain
ğ›¾|Z|ğ‘Ÿ*(S,q)âˆ’ğ›½ D

## ğ¾ğ¿

[ğœ‹
ğœƒ
(z
ğ‘¡+1

## |Z

ğ‘¡
,q)||ğœ‹ref(z
ğ‘¡+1

## |Z

ğ‘¡
,q)] (21)
ğ‘¡
42

<!-- Page 43 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought

## Prm-Q7086


## Prm-Q3000


## Prm-Q500


### Oracle

Majority
ğ›¾=1
ğ›¾=0.995
ğ›¾=0.99
Figure 27: Left: Figure 17 with discounted objective. The undiscounted version of Best-Of-N sees
continued improvement with additional sampling, while the discounted objective exhibits a humpshaped frontier. Right: Token lengths for answers of "what is 2+3=?" by different models. Source:
Figure 1 in Chen et al. (2024).
fiwhereğ›¾ < 1. Thisobjectivewould(intheory) force themodelto trade-off computeversuscertainty
in its response. This effect is demonstrated in Figure 27 (left), which shows a discounted version of
thebest-of-Nobjective. Withoutaparticularpenaltyonthegenerationbudget,performanceincreases
steadily with additional compute, which may push the model to continuously increase the inference
budget (as compared to base models) as shown on the right. If we consider the discounted reward
(accuracy) instead, we see a hump-shaped objective as there is a stronger trade-off between verifier
confidence and the generation budget.
This also raises an interesting trade-off on controlling model behaviors in terms of inference/accuracy. We might be interested in forcing a particular trade-off depending on problem difficulty.
Consider then a distribution over discount rates ğ›¾ < ... < ğ›¾ . We can also associated an individual
1 ğ‘š
language prompts p with each discount rate such as "Work as quickly as possible" for high discount,
ğ›¾
to "Take as much time as needed" for a low discount rate. Then, at train time we can optimize the
objective
[ï¸ƒ ]ï¸ƒ
m
ğœƒ
axE
S,Zâˆ¼ğœ‹ ğœƒ (Â·|q,pğ›¾ğ‘– ),qâˆ¼ğ’Ÿtrain,ğ‘–âˆ¼1:ğ‘š
ğ›¾
ğ‘–
|Z| ğ‘Ÿ*(S,q)âˆ’ğ›½ âˆ‘ï¸ D

## ğ¾ğ¿

[ğœ‹
ğœƒ
(z
ğ‘¡+1

## |Z

ğ‘¡
,q,p
ğ›¾ğ‘–
)||ğœ‹ref(z
ğ‘¡+1

## |Z

ğ‘¡
,q)]
ğ‘¡
Then, at inference time we can control the qualitative model behavior through conditioning on the
prompttotrade-offinferencecomputeversusaccuracy.Schultzetal.(2024)showedthataifwetrain
a model on MCTS search traces with different parameters, at inference time we can condition the
model on a particular parameter configuration and recreate the qualitative behaviors(as shown in
Figure9). Asdiscussedearlier,simplesupervisedfine-tuningmaynotbesufficienttoinduceadvanced
reasoning capabilities (and likely even more difficult to do so in a controllable way), however, these
resultsdemonstratepromiseininducingcontrollablebehaviorsintothemodelthroughtheconditional
prompting objective above.

## Going Forward

Public open-research/source progress on reasoning models is currently bottle-necked by three main
issues:
43

<!-- Page 44 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought

## Access to resources in terms of both data and compute as these algorithms require significant

amount of both.

## Open-source infrastructure for large scale inference and training is currently lackluster.


## Algorithmic exploration - we have a lot of avenues to explore and only limited people and

resources actively working on the right directions.
We expand on these issues in this section.

### The "Big MATH" Project

While compute bottlenecks are a persistent issue in open research, we found the lack of open
datasetswithverifiablereasoningproblemstobeanevenbiggerchallenge. Priorworkshaveusedthe
GSM8k (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) datasets, but the first has largely
been saturated by the current generation of models and the second is quite limited, with only 12,000
problems. To overcome these challenges, we have put significant work into the "Big MATH" project
- an effort to aggregate over 1,000,000 high-quality, high-confidence, and diverse verifiable math
problems. We combine existing datasets with significant post-processing, as well as efforts to acquire
additional data from novel sources. We outline this below.
Ourproposedtrainingpipelinerequiresalarge-scalecorpusofchallengingpromptswithverifiable
answers, but developing such datasets presents fundamental constraints in automated verification
andassessmentofreasoningcapabilities. Inmanydomains,solutionsaretoounstructuredornuanced
for automated verification: a single objective answer may not exist, correct solutions can appear in
equally valid but textually distinct forms, and certain tasks (e.g. complex proofs) resist reduction
to deterministically verifiable outputs with a single canonical representation. These issues make
automated grading infeasible. Even in contexts where problem-answer pairs can be constructed
and easily verified, these formats may reduce to factual retrieval rather than exercises that require
reasoning capabilities.
As evidenced by the wide array of literature discussed in previous sections, mathematics offers
a stable ground truth for correctness, naturally programmatic answer grading, and existing public
datasetsinproblem-solution-answerformat. Whilewerecognizesignificantpotentialinmanydomains
(e.g.,programming,diagnostics,finance,analysis)wherestructuredreasoningpathscanbevalidated,
the substantial resources required to construct and verify such datasets, particularly without access
to proprietary data, makes them impractical for our immediate objectives. While the availability
of existing datasets ultimately drove our domain selection, even these openly available resources
are constrained in both scale and distribution. We face the critical challenge of distinguishing
between computational and pattern proficiency and conceptual understanding when assessing model
capabilities-adistinctionthatfundamentallyshapesourdatasetrequirementsandevaluationmetrics.
To guide our data construction, we define three core criteria:
1. the existence of uniquely verifiable solutions, meaning that problems must admit a single
correct answer that can be reliably verified;
2. open-ended problem formulations, ensuring that tasks cannot be easily solved by guessing
(as might occur in multiple-choice formats) and instead require nontrivial reasoning steps; and
3. closed-form solutions, such that the final answer must be expressible in a closed form (e.g., a
scalar or formula, not a proof), thereby enabling automated evaluation.
These criteria reveal significant limitations in existing datasets. Those that meet our structural
requirements remain severely limited in scale, containing orders of magnitude fewer examples than
44

<!-- Page 45 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
we estimate necessary for model training. Within the structured collections, we observe a concerning
scarcity of non-trivial problems that effectively challenge reasoning capabilities, with many examples
simply testing computational abilities or following predictable patterns. Perhaps most problematic
is the non-negligible proportion of incorrect problem-solution labels, even in widely-used datasets,
introducing substantial complexity to data cleaning.
Preliminary analysis of one of the largest available datasets, NuminaMath (LI et al., 2024),
exemplifies these issues. Basic verification shows that out of roughly 860,000 entries, more than
42,500areduplicates(âˆ¼5%ofthedataset). Further,while89.7%ofentriescontainexactlyoneclearly
boxedsolutionsuitableforautomatedverification,2.6%containnoboxedsolution,and7.7%include
multiple boxed solutions. Problems like this underscore the state of open and publicly available data
and suggest deeper quality concerns.

#### Data Sourcing

We consider multiple established mathematical problem datasets that are commonly used in the
literature(Table3). First,weincludetheHumanAnnotatedReasoningProblems(HARP)dataset(Yue
etal.,2024),containingnearly4,800competition-levelshortanswerproblemswithprogrammaticallycheckable answers. Next, we use the NuminaMath (LI et al., 2024) dataset, which is composed of
roughly860,000problemsfromavarietyofbenchmarksandsources: Chinesehighschoolmathexercises,matholympiad-stylecompetitionproblems,theartofproblemsolvingforum,MATH(Hendrycks
etal.,2021),andGSM8k(Cobbeetal.,2021). NuminaMathfurtherincorporatessyntheticdatafrom
the synthetically generated dataset Orca-Math (Mitra et al., 2024) as well as further generating a
significantamountofsyntheticdatabasedontheMATHdatasetandasubsetofthemathcompetition
problems. Additionally,whileNuminaMathusestheoriginalsplitoftheMATHdataset(7,000training
problems, 5,500 test problems), we choose to use version with 12,000 training problems and 500
test problems, as originally proposed by (Lightman et al., 2023). We incorporate the Omni-MATH
dataset (Gao et al., 2024), contributing almost 4,500 olympiad-level problems curated from 39
different competition websites. Notably, this dataset employs professional annotators and verifiers to
maintain solution-answer quality. Finally, we include OpenMathInstruct-2 (Toshniwal et al., 2024),
composed of about 607,000 synthetically generated problems. Toshniwal et al. (2024) use Llama3.1-
405B (Dubey et al., 2024) in a multi-step data augmentation pipeline which they seed using the
GSM8k (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) datasets.
Beyond drawing from these established datasets, we maintain an active data collection initiative
that extends our corpus through manual curation of competition-level problems. This effort has
already contributed thousands of additional olympiad-style problems and continues to grow. Our
manual collection process focuses on acquiring mathematically rigorous content from competition
archives, olympiad materials, and historical sources not present in existing datasets.

#### Data Filtering

To create datasets with which we can train a model in our training pipeline, we next clean and filter
the data from each source using a combination of bespoke and common strategies (Albalak et al.,
2024). Specifically for our two-stage pipeline, we create two versions of the dataset. First, we have
the base filter, where the data will be used during SFT for the model to learn (1) an approximation
of the distribution of math problems and (2) to follow the desired chain-of-thought format. Next,
the strict filter is intended to be used for the Meta-RL training stage. Thus, the strictly filtered data
should contain only problems that closely follow our three desired properties: open-ended, verifiable,
closed-form problem-solution pairs.
45

<!-- Page 46 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Data Source Original Base Filter Strict Filter

### HARP (Yue et al., 2024) 4,780 3,691 2,996

NuminaMath (LI et al., 2024) 859,608 452,820 231,887

### Omni-MATH (Gao et al., 2024) 4,428 3,660 2,478

OpenMathInstruct-2 (Toshniwal et al., 2024) 607,324 600,191 496,331

### Total 1,476,140 1,060,362 733,692

Table 3: Comparison of Problems by Data Source and Filter Type
The Base Filter performs some operations that are unique to each subset, and some operations
that are shared across the full collection of data. First, in the HARP dataset (Yue et al., 2024), we
find many problems that contain figures in the Asymptote4 vector graphics language, which we
filter out of the dataset. Next, we filter the NuminaMath dataset (LI et al., 2024). This dataset
contains a significant amount of synthetic data, which is often difficult to verify for accuracy and
correctness. However, the Orca-Math (Mitra et al., 2024) subset has empirically been proven to
improve performance on supervised fine-tuning, and for this reason, we elect to maintain only
the Orca-Math data, while discarding the other synthetic subsets. Next, NuminaMath does not
explicitlycontainanswerstoeachproblem,soweextractanswerstoproblemsbysearchingforboxed
solutions (â€œ\boxed{}â€ in LaTeX). Any problems whose solution does not contain exactly 1 boxed
answer is filtered out. When exploring Omni-MATH (Gao et al., 2024), we found many problems
containing author attributions (e.g. a personâ€™s name in parenthesis) and removed the attributions
from the problem. OpenMathInstruct-2 originally contains multiple solutions and final answers to
each problem (Toshniwal et al., 2024). The first step we take is to group all matching problems
together, removing those which have mismatched expected answers. Next, we found that the data
still contains evidence of the synthetic data generation process, such as asking â€œdo you want to solve
it?â€ or â€œhereâ€™s the new problemâ€ and remove any problems with similar phrases.
Finally, after running each of the described filters over the individual subsets, we perform four
filteringoperationsacrossthefullcollection. First,weuseexactmatchingtofindandremoveduplicate
problems. Then, we use a FastText language identifier (Joulin et al., 2016b,a; Grave et al., 2018)
and remove any problems where English is not the primary language. Next, we remove problems
containingahyperlink,asthissuggeststhatamodelmaynothavethefullresourcesrequiredtosolve
the problem. Lastly, we decontaminate the dataset by removing any examples of the MATH500 test
set (Lightman et al., 2023).
The Strict Filter further reduces the base filtered data to problem-answer pairs which are more
likelytobeopen-ended,verifiable,andclosed-form. Thestrictfiltersareappliedequallytoallsubsets
of the dataset through a mix of rule-based and model-based filtering. First, we remove questions
containingmultiplepartsasthesecanbechallengingtoevaluate. Forasimilarreason,wealsoremove
questions that ask for a proof as these problems are difficult to evaluate. Next, we choose to remove
multiple choice problems as models have a high probability of selecting the correct answer without
producing an accurate reasoning chain. For the same reason, we also remove Yes/No and True/False
problems, as these may give a poor learning signal during Meta-RL training. Finally, we use the
SemDeDupalgorithm(Abbasetal.,2023)withthemodelatsentence-transformers/all-MiniLM-L6-v2
and remove problems with a cosine similarity over 0.5.

### Infrastructure

For the proposed family of algorithms we need RL training infrastructure that can:
4https://asymptote.sourceforge.io/
46

<!-- Page 47 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought

## Scale to multiple nodes in a straightforward way


## Allow for high-performance inference throughput


## Allow for interleaving inference and training efficiently for online RL algorithms

Hereweoutlineourrecentprogressontheseissuesintheopen-sourceGPT-NeoXframework(Andonian et al., 2023). We leverage CUDA IPC handles to enable true asynchronous RLHF training by
sharing GPU memory directly between training and inference processes. By allocating model weights
in the training framework and sharing CUDA
memorypointerswiththeinferenceframework,
both processes maintain access to the same
physical memory throughout training. When
the training process updates weights, these updates are immediately visible to the inference
process through its mapped pointers without
requiring any explicit synchronization. This
shared memory architecture enables fully parallel execution - the inference process can continuously generate tokens while the training
process updates weights, with neither process
blocking the other. Although early portions
of generated rollouts may be off-policy, the final steps will generally be only one training
step off-policy at most, as the inference process
immediately sees weight updates through the Figure 28: Async training versus slightly off policy
shared memory. Unlike previous asynchronous methods that require dedicated weight synchroapproaches (Noukhovitch et al., 2024) that re- nization
quire distinct generation and training phases,
our system maintains constant GPU utilization across both processes. Direct memory sharing provides significantly higher throughput by eliminating synchronization overhead between training and
inference. Our initial experiments show 40% improved throughput compared to 3-step asynchronous
training that requires explicit weight synchronization, as shown in Figure 28. However, this approach
comes with important trade-offs in memory utilization and parallelism. Sharing GPUs between
processes restricts us to suboptimal tensor parallelism (TP) configurations compared to dedicated
training and inference setups, resulting in lower theoretical peak throughput for both processes. The
impactisparticularlypronouncedforinference,wherereducedmemoryforKVcacheandsuboptimal
TP configurations significantly constrain the generation speed. In scenarios where inference compute
significantly exceeds training compute (e.g. MCTS, where most of the search tree is discarded after
each rollout), using separate dedicated GPU clusters with optimized TP configurations for each
process can achieve higher overall throughput despite the synchronization overhead.

### Open Research Questions

A number of open research questions remain which are currently not answered in the literature.

#### Open-Ended Verification And CoT Faithfulness

In this report we outline a pipeline for training advanced reasoning models on verifiable questions.
These types of questions largely consist of things like math, scientific questions with symbolic or
numerical answers, or code problems with unit tests. However, we believe it is still an open question
47

<!-- Page 48 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Figure 29: Under a fixed generator model and a variable verifier performance seems to also obey
a form of a scaling law. Right: Accuracy seems to improve in a log-linear fashion with amount of
training data under a standard discriminative PRM. Source: Figure 6 in (Setlur et al., 2024c). Left:
Performance of a Generative Reward Model (with CoT) on reasoning tasks with a a reference using
variable amount of CoTs and majority voting. Increasing the amount of inference-time sampling
improves accuracy. Source: Figure 5 in (Mahan et al., 2024).
on how to ensure that the resulting CoTs are faithful and in fact provide valid reasoning. In many
applications, such as science for example, the reasoning chain can be even more important than
the final answer. Qualitatively, current models with â€œadvancedâ€ reasoning still struggle and produce
many CoTs which are inconsistent or unfaithful, even when they obtain the correct final answer.
Interestingly, even the single math example provided along with the announcement of the O1
model has an incomplete solution as it makes an unsubstantiated assumption on the form of the
polynomial â„(ğ‘¥) = ğ‘¥2 âˆ’ğ‘ without proving that the coefficient of ğ‘¥ is in fact zero (OpenAI, 2024).
Under a competitive setting this would likely receive 5/7 points, even though the final answer is
correct. Itisworthnotingthatthemodelactuallyverifiesandprovesthisassumptioninitâ€™sMeta-CoT
butthefinaloutputprovideddoesnotincludethosesteps. Webelievethismightbeonetypeofartifact
that arises from training with final answer verification only. How to provide rewards on full CoTs
and open-ended problems such as proofs remains an open question. Under the assumption of a wide
enough generation-verification gap, we believe an RLAIF approach could be promising. In particular,
foregoing concepts such as formal verification, one avenue to explore is a â€œgenerative verifierâ€ similar
to Zhang et al. (2024a) or Mahan et al. (2024). In this setting the evaluation model can be provided
with one or several reference solutions/proofs, or other relevant material such as textbook chapters,
tools like Wolfram or Mathematica, and specifically fine-tuned to provide evaluations to proposed
solutions, rather than final answers alone. As outlined in Section 3.2, we found advanced models
to already posses some capabilities in that vein. Unfortunately, a major bottleneck in that line of
research is the need for a dataset of open-ended reasoning problems (such as proofs), which would
likely require significant human annotations.

#### Process Guidance And The Verifier Gap

Thecurrentworkbringstolightsomeremainingquestionsontheapproachestoprocessguidanceand
PRMS. In Section 5 we outlined results showing that pre-trained PRMs still significantly lack behind
pure Monte Carlo approaches in terms of search efficiency. Moreover, with few exceptions (Setlur
et al., 2024c), we believe algorithmic approaches for training these models remain understudied. In
particular,ifweviewaPRMasavaluefunction,thenmanyapproachesfromofflineRL(Levineetal.,
2020)becomeapplicablewithsomealreadyshowingpromiseinagenticdomains(Zhouetal.,2024b).
In the same vein, Silver et al. (2016) introduced a separate value function over the standard MCTS
48

<!-- Page 49 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
approach using roll-outs for efficiency purposes. However, we believe there may be a fundamental
verifier gap beyond just the efficiency question. As shown in Figure 3 there remains a significant
gap between the "best-of-N" verifier driven approach and the oracle pass@N performance. A similar
gap seems to exist in the multi-turn code generation domain, as shown in Figure 22 (Gehring et al.,
2024). Moreover, under a fixed generator, results from Setlur et al. (2024c) and our own empirical
findings in Section 5 indicate that verifier performance may also be driven by a scaling law as
well. In Figure 29 (left) we see clear log-linear scaling in performance in terms of training data. One
hypothesis is that verification is also a matter of computational complexity, which is why allowing
verifiers to use CoT improves performance as shown in Figure 3 (Zhang et al., 2024a; Mahan et al.,
2024). In addition, Mahan et al. (2024) show further inference-time scaling for verifiers, as using
additional CoTs with majority vote improves reward accuracy on reasoning problems well beyond
the performance of the standard discriminative verifier. This raises questions for the fundamental
learnabilityofin-contextsearchtraces. Inessence,ifasearchtracewasgeneratedusingground-truth
verification or Monte-Carlo roll-outs, the resulting sequence may have significant implicit complexity,
which would be challenging for the model to learn. This would also motivate incorporating further
self-evaluationorreflectionintoreasoningchains. Theassociatedverificationscalinglawsanddesign
choices remain largely unexplored in the literature and present an important research direction.

#### Scaling Laws For Reasoning And Search

While recent model releases have demonstrated strong inference scaling performance, we still lack a
thorough scientific exploration of scaling laws in public research, which is a key piece of the recently
emerging model paradigm.

## The scaling laws of search presented by Jones (2021) have not been publicly evaluated

on realistically challenging reasoning applications with LLMs. While the results from Feng
et al. (2024) show promise, they are still limited and partial. Obtaining a more thorough
evaluation on joint policy and verifier scaling will provide clarity to current research questions.
So far, unfortunately, the research community has been limited by a lack of data and scalable
infrastructure.

## Exploration of search approaches remains a very under-studied topic. As discussed in Section 4.4, it appears that current reasoning models all implement different approaches to search,

which leads to qualitatively different behaviors based on the search strategies presented in
Section 4.3. Given a strong trained verifier, what are the performance effects of different
search strategies such as BFS/DFS-V, A*, MCTS, etc.? Furthermore, the relationship between
exploration strategies at training and test time is an important direction of future study.

## Thetrade-offsbetweeninstruction-tuningandRLremainunclear. AsdiscussedinSection6,

it appears that standard instruction tuning can endow models with meta-reasoning capabilities
in small/simple domains, but these results do not scale, and realistic applications require
significant on-policy RL. Is this a fundamental issue with distribution shift, or is performance
driven by another relationship, similar to the trade-offs discovered by Setlur et al. (2024a)?

## The fundamental missing piece of the current inference time scaling law discussion is the

performanceoftheproposedin-contextsearchstrategycomparedwithanexplicitsearch-based
method. Intheory,thepost-trainingapproachoutlinedinSection7candiscovernovelreasoning
approaches (algorithms) that solve fundamentally new classes of problems unsolvable under
any search budget by a standard search approach. While there is clear evidence of the efficiency
of in-context search (fewer tokens per interaction), it is unclear whether the current generation
ofmodelshaveanyemergentcapabilitiesyet. Insimplestterms,dostrongreasoningmodelsshift
the compute-accuracy curve to the left or up?. As outlined in Section 6.4, current evidence in the
open literature for emergent capabilities remains weak.
49

<!-- Page 50 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought

#### Meta-Search/Search 2

In this report we argued that advanced reasoning should incorporate in-context search, which can
yield higher efficiency and potentially more advanced capabilities in reasoning tasks. However, such
anapproachalsohaspotentialdownsides. In particular, wearelimited bythemodelâ€™scontextlength,
whichinducesfundamentallimitsonthesearchcomplexity. Moreover,thesequentialnatureofin-context
searchcanmaketheprocessslow,limitingthepower
ofthesemodels. Atthesametimeneitheroftheseare
issueswithclassicalsearchapproachessuchasMCTS,
whichcanbeparallelizedLiuetal.(2020). Anatural
question is: can we can build an additional search
procedureontopofanadvancedreasoningmodel(a
processwhichwecallMeta-SearchorSearch2followingthenamingconventionofDuanetal.(2016))? In
recent literature, Anonymous (2024) trained a value
function (PRM) with a â€œmulti-turnâ€ approach, conditioning the value function on all prior explored solutions. Ratherthanthetraditionalvaluefunction,they Figure 30: Scaling performance of search
insteadtrainafunctionğ‘£ (Z ,S ,q),wheretheMeta- with"meta"-valuefunctionandregular(single
ğœƒ t ğ‘¡
CoT Z consists of prior solution attempts. The em- turn) value function. Conditioning the model
ğ‘¡
pirical performance of their method compared with on prior explored paths improves search efregular PRM training is shown in Figure 30, demon- ficiency. Source: Figure 5 in Anonymous
stratingimprovedscalingovertraditionalvaluefunc- (2024).
tion training. This result suggests that we may be
able to increase the search efficiency using a meta-critic, however this remains an open empirical
question. If this is indeed the case, it would allow us to also massively scale online search with
reasoning models.

#### Reasoning with External Tools

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
20 21 22 23 24 25 26
Number of samples
ycaruccA

### Building on our discussion of scaling laws and

search approaches, we investigate a critical direction: augmenting model reasoning with ex- AIMO_AMC
ternal computational tools. Just as humans Majority Vote (TIR)

### Best of N (TIR)

leverage calculators, spreadsheets, and special- Majority Vote (CoT)

### Best of N (CoT)

izedsoftwaretosolveproblemsmoreefficiently,
LLMscanpotentiallyachievebetterscalingproperties by offloading the compute burden to external tools - requiring less training data during SFT and fewer samples during test-time
searchtoreachthehighlevelofperformance.

### Forinstance,whilepureCoTreasoningrequires

models to perform all calculations internally to
solve math problems, these computations can
beoffloadedtoaPythoninterpreter. Priorworks
demonstratethatsuchtool-integratedreasoning
Figure 31: Scaling curves for a Tool Integrated
(TIR) improves performance on mathematical
Reasoning(TIR)model,trainedon100Kproblems,
questions(LIetal.,2024;Yinetal.,2024;Chen
and a Chain-of-Thought (CoT) model trained on
etal.,2022). However,thescalingpropertiesof
400K problems on the First AIMO Prize, using an
oracle verifier.
50

<!-- Page 51 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
TIR remain unexplored.
Figure 31, using models trained by LI et al.
(2024), provides initial evidence for the efficiency of TIR models. Despite being fine-tuned on 25% as much data, the TIR model demonstrates
superior scaling properties compared to the CoT model. This occurs across all sample sizes, with
both majority vote and Best-of-N strategies consistently outperforming their CoT counterparts. The
efficiencygainisparticularlyevidentinthelow-sampleregime(20 to23 samples),whereTIRachieves
nearly double the accuracy of CoT methods. This suggests that offloading computations to external
tools significantly improves the modelâ€™s ability to solve problems even with limited attempts, and TIR
is more efficient at both training and inference time.
Based on these promising initial results, we identify several critical directions for future investigation:

## Scaling laws and search strategies: We need to systematically study the scaling properties of

different search methods (e.g., BFS, DFS, ğ´* 5, MCTS) in Tool-Integrated Reasoning models.

## Verification scaling: The role and scaling properties of the verifier in TIR settings remains

under-explored - what is the relation between training data for the verifier, verifier accuracy,
and policy accuracy?

## Internal reasoning vs reasoning with external tools: While our initial results show TIRâ€™s

efficiency advantages, we need to better understand the fundamental trade-offs between these
approaches. When does offloading computation to external tools provide the most benefit
compared to pure language reasoning? How do these trade-offs change with model scale and
problem complexity?

## Conclusion

In this position paper, we have introduced Meta Chain-of-Thought (Meta-CoT) as a framework for
understanding and enhancing the reasoning capabilities of Large Language Models (LLMs). We have
argued that traditional Chain-of-Thought does not fully represent the underlying data generative
process on reasoning problems. By incorporating the concepts of search, verification, and iterative
refinement, Meta-CoT provides a more complete model of the cognitive processes required for
advanced problem-solving.
We believe that Meta-CoT represents a promising path towards more robust and generalizable
reasoning in LLMs. The observed behaviors of state-of-the-art models, along with our experiments on
in-contextexplorationandbacktracking,lendsupporttothehypothesisthatinternalsearchprocesses
arecrucialforperformanceoncomplextasks. Furthermore,theproposedtrainingpipelinepresentsa
concrete approach for developing LLMs with enhanced Meta-CoT capabilities. Future work should
validate the efficacy of our proposed pipeline.
Beyondourproposedmethod, numerous openquestionsandchallengesremain. Furtherresearch
is needed to determine the optimal scaling laws for reasoning and search, to develop more effective
processsupervisionandverificationtechniques,andtoclarifytheinterplaybetweeninstructiontuning
and reinforcement learning for meta chain of thought reasoning. The "Big MATH" dataset we have
introduced aims to support this research by providing a large-scale resource for training reasoning
models. Furthermore, the possibility of meta-RL paired with Meta-CoT automatically discovering
novel search algorithms is a particularly intriguing open research question for future work to explore.
5AnexampleBest-FirstsearchtracewithTIRisavailableinSectionE.
51

<!-- Page 52 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought

## Acknowledgments

WewouldliketothankAviralKumar, BenjaminEysenbach,NathanLambert,RishabhAgarwal,Sasha
Rush and Noah Goodman for the fruitful discussions and feedback on this report.
52

<!-- Page 53 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought

### References

Amro Abbas, Kushal Tirumala, DÃ¡niel Simig, Surya Ganguli, and Ari S. Morcos. Semdedup: Dataefficientlearningatweb-scalethroughsemanticdeduplication,2023. URLhttps://arxiv.org/
abs/2303.09540.
Arash Ahmadian, Chris Cremer, Matthias GallÃ©, Marzieh Fadaee, Julia Kreutzer, Ahmet ÃœstÃ¼n, and
Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human
feedback in llms. arXiv preprint arXiv:2402.14740, 2024.
Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas
Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, Colin Raffel, Shiyu Chang, Tatsunori
Hashimoto, and William Yang Wang. A survey on data selection for language models, 2024. URL
https://arxiv.org/abs/2402.16827.
Alex Andonian, Quentin Anthony, Stella Biderman, Sid Black, Preetham Gali, Leo Gao, Eric Hallahan,
Josh Levy-Kramer, Connor Leahy, Lucas Nestler, Kip Parker, Michael Pieler, Jason Phang, Shivanshu
Purohit, Hailey Schoelkopf, Dashiell Stander, Tri Songz, Curt Tigges, Benjamin ThÃ©rien, Phil Wang,
and Samuel Weinbach. GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch,

## URL https://www.github.com/eleutherai/gpt-neox.

Anonymous. Improving the efficiency of test-time search in LLMs with backtracking. In Submitted
to The Thirteenth International Conference on Learning Representations, 2024. URL https://
openreview.net/forum?id=hJ2BCYGvFg. under review.
Jacob Beck, Risto Vuorio, Evan Zheran Liu, Zheng Xiong, Luisa Zintgraf, Chelsea Finn, and Shimon
Whiteson. Asurveyofmeta-reinforcementlearning,2024. URLhttps://arxiv.org/abs/2301.
08028.
Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V Le, Christopher RÃ©, and Azalia
Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv
preprint arXiv:2407.21787, 2024.
Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint
arXiv:2211.12588, 2022.
Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu,
Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Do not think
thatmuchfor2+3=? ontheoverthinkingofo1-likellms,2024. URLhttps://arxiv.org/abs/
2412.21187.
KarlCobbe,VineetKosaraju,MohammadBavarian,MarkChen,HeewooJun,LukaszKaiser,Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.
Training verifiers to solve math word problems, 2021. URL https://arxiv.org/abs/2110.
14168.
DeepSeek. Deepseek-r1-lite-previewisnowlive: unleashingsuperchargedreasoningpower!,112024.
URL https://x.com/deepseek_ai/status/1859200141355536422. Posted on X (formerly
Twitter).
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Åukasz Kaiser. Universal
transformers, 2019. URL https://arxiv.org/abs/1807.03819.
53

<!-- Page 54 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl2: Fast
reinforcement learning via slow reinforcement learning, 2016. URL https://arxiv.org/abs/
1611.02779.
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha
Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn,
Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston
Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh
Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell,
Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus
Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv
Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin,
Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang,
Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan
Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,
Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan
Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer
Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang,
Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe,
Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield,
Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren
Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin,
Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi,
Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu
Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan,
Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier
Duchenne, Onur Ã‡elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng,
PrajjwalBhargava,PratikDubal,PraveenKrishnan,PunitSinghKoura,PuxinXu,QingHe,Qingxiao
Dong,RagavanSrinivasan,RajGanapathy,RamonCalderer,RicardoSilveiraCabral,RobertStojnic,
Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly,
RossTaylor,RuanSilva,RuiHou,RuiWang,SagharHosseini,SahanaChennabasappa,SanjaySingh,
Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy,
Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra,
Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar
Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher,
Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan,
Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan
Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng
Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen
Song,YuchenZhang,YueLi,YuningMao,ZacharieDelpierreCoudert,ZhengYan,ZhengxingChen,
ZoePapakipos,AadityaSingh,AaronGrattafiori,AbhaJain,AdamKelsey,AdamShajnfeld,Adithya
Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex
Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu,
Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit
Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin
Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie
Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock,
Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly
Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang
Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer,
54

<!-- Page 55 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
DanielLi,DannyWyatt,DavidAdkins,DavidXu,DavideTestuggine,DeliaDavid,DeviParikh,Diana
Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil,
Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute,
Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni,
Francisco GuzmÃ¡n, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada
Badeer,GeorgiaSwee,GilHalpern,GovindThattai,GrantHerman, GrigorySizov, Guangyi,Zhang,
Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun
Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Ibrahim Damlaj, Igor
Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli,
Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy
Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon
Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U,
Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik
Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle
Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng
Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish
Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso,
Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko,
Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike
Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini
Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas
Usunier,NikolayPavlovichLaptev,NingDong,NingZhang,NormanCheng,OlegChernoguz,Olivia
Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro
Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani,
Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu
Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari,
RussHowes,RutyRinott,SaiJayeshBondu,SamyakDatta,SaraChugh,SaraHunt,SargunDhillon,
SashaSidorov,SatadruPan,SaurabhVerma,SeijiYamamoto,SharadhRamaswamy,ShaunLindsay,
Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang,
Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max,
Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin
Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser,
Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy
Chou,TzookShaked,VarunVontimitta,VictoriaAjayi,VictoriaMontanez,VijaiMohan,VinaySatish
Kumar,VishalMangla,VÃ­torAlbiero,VladIonescu,VladPoenaru,VladTiberiuMihailescu,Vladimir
Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang,
Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu,
Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao,
Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and
Zhiwei Zhao. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783.
Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin,
Percy Liang, and Tatsunori B. Hashimoto. Alpacafarm: A simulation framework for methods that
learn from human feedback, 2024.
Subhabrata Dutta, Joykirat Singh, Soumen Chakrabarti, and Tanmoy Chakraborty. How to think
step-by-step: A mechanistic understanding of chain-of-thought reasoning. ArXiv, abs/2402.18312,

## URL https://api.semanticscholar.org/CorpusID:268041831.

Jeffrey L Elman. Finding structure in time. Cognitive science, 14(2):179â€“211, 1990.
55

<!-- Page 56 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun
Wang. Alphazero-like tree-search can guide large language model decoding and training, 2024.
Kanishk Gandhi, Dorsa Sadigh, and Noah D. Goodman. Strategic reasoning with language models,

## URL https://arxiv.org/abs/2305.19165.

Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and Noah D
Goodman. Streamofsearch(sos): Learningtosearchinlanguage. arXivpreprintarXiv:2404.03683,
2024.
Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma,
Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan,
Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang. Omnimath: A universal olympiad level mathematic benchmark for large language models, 2024. URL
https://arxiv.org/abs/2410.07985.
Jonas Gehring, Kunhao Zheng, Jade Copet, Vegard Mella, Taco Cohen, and Gabriel Synnaeve. Rlef:
Grounding code llms in execution feedback with reinforcement learning, 2024. URL https:
//arxiv.org/abs/2410.02089.
Dibya Ghosh, Jad Rahme, Aviral Kumar, Amy Zhang, Ryan P. Adams, and Sergey Levine. Why
generalization in rl is difficult: Epistemic pomdps and implicit partial observability, 2021. URL
https://arxiv.org/abs/2107.06277.
EdouardGrave,PiotrBojanowski,PrakharGupta,ArmandJoulin,andTomasMikolov. Learningword
vectorsfor157languages. InProceedingsoftheInternationalConference onLanguageResourcesand
Evaluation (LREC 2018), 2018.
Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine,
and Dawn Song. The false promise of imitating proprietary llms, 2023. URL https://arxiv.
org/abs/2305.15717.
Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu.
Reasoningwithlanguagemodelisplanningwithworldmodel,2023. URLhttps://arxiv.org/
abs/2305.14992.
Alex Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu,
Maksym Zhuravinskyi, Eric Hambro, Sainbayar Sukhbaatar, and Roberta Raileanu. Teaching large
language models to reason with reinforcement learning, 2024. URL https://arxiv.org/abs/
2403.04642.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,
andJacobSteinhardt. Measuringmathematicalproblemsolvingwiththemathdataset,2021. URL
https://arxiv.org/abs/2103.03874.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir
Mohamed, and Alexander Lerchner. B-vae: Learning basic visual concepts with a constrained
variational framework. In International Conference on Learning Representations (ICLR), 2017. URL
https://openreview.net/forum?id=Sy2fzU9gl.
Jan Humplik, Alexandre Galashov, Leonard Hasenclever, Pedro A. Ortega, Yee Whye Teh, and Nicolas
Heess. Meta reinforcement learning as task inference, 2019. URL https://arxiv.org/abs/
1905.06424.
56

<!-- Page 57 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
AndyL.Jones. Scalingscalinglawswithboardgames,2021. URLhttps://arxiv.org/abs/2104.
03113.
Michael I Jordan. Serial order: A parallel distributed processing approach. In Advances in psychology,
volume 121, pp. 471â€“495. Elsevier, 1997.
Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hâ€™erve Jâ€™egou, and Tomas Mikolov.
Fasttext.zip: Compressing text classification models. arXiv preprint arXiv:1612.03651, 2016a.
Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient text
classification. arXiv preprint arXiv:1607.01759, 2016b.
Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron
Courville, and Nicolas Le Roux. Vineppo: Unlocking rl potential for llm reasoning through refined
credit assignment, 2024. URL https://arxiv.org/abs/2410.01679.
DiederikPKingmaandMaxWelling. Auto-encodingvariationalbayes. arXivpreprintarXiv:1312.6114,

## URL https://arxiv.org/abs/1312.6114.

Levente Kocsis and Csaba SzepesvÃ¡ri. Bandit based Monte-Carlo planning. In Johannes FÃ¼rnkranz,
Tobias Scheffer, and Myra Spiliopoulou (eds.), Machine Learning: ECML 2006, volume 4212 of
Lecture Notes in Computer Science, Berlin, Heidelberg, 2006. Springer. doi: 10.1007/11871842_29.
Jing Yu Koh, Stephen McAleer, Daniel Fried, and Ruslan Salakhutdinov. Tree search for language
model agents. arXiv preprint arXiv:2407.01476, 2024.
Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli,
Shariq Iqbal, Colton Bishop, Rebecca Roelofs, Lei M Zhang, Kay McKinney, Disha Shrivastava,
Cosmin Paduraru, George Tucker, Doina Precup, Feryal Behbahani, and Aleksandra Faust. Training
language models to self-correct via reinforcement learning, 2024. URL https://arxiv.org/
abs/2409.12917.
XinLai,ZhuotaoTian,YukangChen,SenqiaoYang,XiangruPeng,andJiayaJia. Step-dpo: Step-wise
preference optimization for long-chain reasoning of llms, 2024. URL https://arxiv.org/abs/
2406.18629.
Lucas Lehnert, Sainbayar Sukhbaatar, DiJia Su, Qinqing Zheng, Paul Mcvay, Michael Rabbat, and
Yuandong Tian. Beyond a*: Better planning with transformers via search dynamics bootstrapping,

## URL https://arxiv.org/abs/2402.14083.

Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial,
review, and perspectives on open problems, 2020.
Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa
Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong,
Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. Numinamath. [https:
//huggingface.co/AI-MO/NuminaMath-CoT](https://github.com/project-numina/
aimo-progress-prize/blob/main/report/numina_dataset.pdf), 2024.
Zhiyuan Li, Hong Liu, Denny Zhou, and Tengyu Ma. Chain of thought empowers transformers to
solve inherently serial problems, 2024. URL https://arxiv.org/abs/2402.12875.
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan
Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Letâ€™s verify step by step. arXiv preprint
arXiv:2305.20050, 2023.
57

<!-- Page 58 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Anji Liu, Yitao Liang, Ji Liu, Guy Van den Broeck, and Jianshu Chen. On effective parallelization of
monte carlo tree search, 2020. URL https://arxiv.org/abs/2006.08785.
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,
Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder,
Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative
refinement with self-feedback, 2023. URL https://arxiv.org/abs/2303.17651.
Dakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato, Jan-
Philipp FrÃ¤nken, Chelsea Finn, and Alon Albalak. Generative reward models. arXiv preprint
arXiv:2410.12832, 2024.
The mathlib Community. The lean mathematical library. In Proceedings of the 9th ACM SIGPLAN
International Conference on Certified Programs and Proofs, POPL â€™20. ACM, January 2020. doi:
10.1145/3372885.3373824. URL http://dx.doi.org/10.1145/3372885.3373824.
William Merrill and Ashish Sabharwal. The expresssive power of transformers with chain of thought.
arXiv preprint arXiv:2310.07923, 2023.
Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad
Farajtabar. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large
language models, 2024. URL https://arxiv.org/abs/2410.05229.
Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. Orca-math: Unlocking the
potential of slms in grade school math, 2024. URL https://arxiv.org/abs/2402.14830.
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher
Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou,
Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt:
Browser-assisted question-answering with human feedback, 2022.
Allen Nie, Yi Su, Bo Chang, Jonathan N. Lee, Ed H. Chi, Quoc V. Le, and Minmin Chen. Evolve:
Evaluating and optimizing llms for exploration, 2024. URL https://arxiv.org/abs/2410.
06238.
MichaelNoukhovitch,ShengyiHuang,SophieXhonneux,ArianHosseini,RishabhAgarwal,andAaron
Courville. Asynchronous rlhf: Faster and more efficient off-policy rl for language models, 2024.
URL https://arxiv.org/abs/2410.18252.
Franz Nowak, Anej Svete, Alexandra Butoi, and Ryan Cotterell. On the representational capacity of
neural language models with chain-of-thought reasoning. arXiv preprint arXiv:2406.14197, 2024.
OpenAI. Gpt-4 technical report. arXiv preprint, 2023. https://arxiv.org/abs/2303.08774.
OpenAI. Learning to reason with llms. https://openai.com/index/
learning-to-reason-with-llms/, 2024. Accessed: 2024-12-20.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser
Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan
Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 27730â€“27744. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/
file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf.
58

<!-- Page 59 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Ryan Park, Rafael Rafailov, Stefano Ermon, and Chelsea Finn. Disentangling length from quality in
direct preference optimization, 2024.
XueBinPeng,AviralKumar,GraceZhang,andSergeyLevine. Advantage-weightedregression: Simple
and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.
Du Phan, Matthew D. Hoffman, David Dohan, Sholto Douglas, Tuan Anh Le, Aaron Parisi, Pavel
Sountsov, Charles Sutton, Sharad Vikram, and Rif A. Saurous. Training chain-of-thought via
latent-variable inference, 2023. URL https://arxiv.org/abs/2312.02179.
Ben Prystawski, Michael Li, and Noah Goodman. Why think step by step? reasoning emerges from
the locality of experience. Advances in Neural Information Processing Systems, 36, 2024.
Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and
RafaelRafailov. Agentq: Advancedreasoningandlearningforautonomousaiagents. arXivpreprint
arXiv:2408.07199, 2024.
Yuxiao Qu, Tianjun Zhang, Naman Garg, and Aviral Kumar. Recursive introspection: Teaching
languagemodelagentshowtoself-improve,2024. URLhttps://arxiv.org/abs/2407.18219.
Santosh Kumar Radha, Yasamin Nouri Jelyani, Ara Ghukasyan, and Oktay Goktas. Iteration of
thought: Leveraging inner dialogue for autonomous large language model reasoning, 2024. URL
https://arxiv.org/abs/2409.12618.
Kate Rakelly, Aurick Zhou, Deirdre Quillen, Chelsea Finn, and Sergey Levine. Efficient off-policy
meta-reinforcementlearningviaprobabilisticcontextvariables,2019. URLhttps://arxiv.org/
abs/1903.08254.
Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste
Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini
1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint
arXiv:2403.05530, 2024.
Christopher D. Rosin. Multi-armed bandits with episode context. Annals of Mathematics and Artificial
Intelligence, 61(3):203â€“230, 2011. doi: 10.1007/s10472-011-9258-6.
Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. One-shot
learningwithmemory-augmentedneuralnetworks,2016. URLhttps://arxiv.org/abs/1605.
06065.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms, 2017.
John Schultz, Jakub Adamek, Matej Jusup, Marc Lanctot, Michael Kaisers, Sarah Perrin, Daniel
Hennes, Jeremy Shar, Cannada Lewis, Anian Ruoss, Tom Zahavy, Petar VeliÄkoviÄ‡, Laurel Prince,
Satinder Singh, Eric Malmi, and Nenad TomaÅ¡ev. Mastering board games by external and internal
planning with language models, 2024. URL https://arxiv.org/abs/2412.12119.
Bilgehan Sel, Ahmad Al-Tawaha, Vanshaj Khattar, Ruoxi Jia, and Ming Jin. Algorithm of thoughts:
Enhancingexplorationofideasinlargelanguagemodels,2024. URLhttps://arxiv.org/abs/
2308.10379.
Amrith Setlur, Saurabh Garg, Xinyang Geng, Naman Garg, Virginia Smith, and Aviral Kumar. Rl
on incorrect synthetic data scales the efficiency of llm math reasoning by eight-fold, 2024a. URL
https://arxiv.org/abs/2406.14532.
59

<!-- Page 60 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh
Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process
verifiers for llm reasoning. arXiv preprint arXiv:2410.08146, 2024b.
Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh
Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process
verifiers for llm reasoning, 2024c. URL https://arxiv.org/abs/2410.08146.
Amrith Setlur, Yuxiao Qu, Lunjun Zhang, Matthew Yang, Virginia Smith,
and Aviral Kumar. Optimizing llm test-time compute involves solving a meta-rl problem. https://blog.ml.cmu.edu/2025/01/08/
optimizing-llm-test-time-compute-involves-solving-a-meta-rl-problem/,

## CMU MLD Blog.

ZhihongShao,PeiyiWang,QihaoZhu,RunxinXu,JunxiaoSong,XiaoBi,HaoweiZhang,Mingchuan
Zhang,Y.K.Li,Y.Wu,andDayaGuo. Deepseekmath: Pushingthelimitsofmathematicalreasoning
in open language models, 2024. URL https://arxiv.org/abs/2402.03300.
Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu
Yao. Reflexion: Languageagentswithverbalreinforcementlearning,2023. URLhttps://arxiv.
org/abs/2303.11366.
D. Silver, A. Huang, C. Maddison, et al. Mastering the game of go with deep neural networks and
tree search. Nature, 529:484â€“489, 2016. doi: 10.1038/nature16961.
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,
Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy P. Lillicrap, Karen
Simonyan, and Demis Hassabis. A general reinforcement learning algorithm that masters chess,
shogi, and go through self-play. Science, 362(6419):1140â€“1144, 2018. doi: 10.1126/science.
aar6404.
AviSingh,JohnD.Co-Reyes,RishabhAgarwal,AnkeshAnand,PiyushPatil,XavierGarcia,PeterJ.Liu,
JamesHarrison,JaehoonLee,KelvinXu,AaronParisi,AbhishekKumar,AlexAlemi,AlexRizkowsky,
AzadeNova,BenAdlam,BerndBohnet,GamaleldinElsayed,HanieSedghi,IgorMordatch,Isabelle
Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kevin
Swersky, Kshiteej Mahajan, Laura Culp, Lechao Xiao, Maxwell L. Bileschi, Noah Constant, Roman
Novak, Rosanne Liu, Tris Warkentin, Yundi Qian, Yamini Bansal, Ethan Dyer, Behnam Neyshabur,
Jascha Sohl-Dickstein, and Noah Fiedel. Beyond human data: Scaling self-training for problemsolving with language models, 2024. URL https://arxiv.org/abs/2312.06585.
PrasannSinghal,TanyaGoyal,JiachengXu,andGregDurrett. Alongwaytogo: Investigatinglength
correlations in rlhf, 2023.
CharlieSnell,JaehoonLee,KelvinXu,andAviralKumar. Scalingllmtest-timecomputeoptimallycan
be more effective than scaling model parameters, 2024. URL https://arxiv.org/abs/2408.
03314.
Ray J Solomonoff. A formal theory of inductive inference. part i. Information and control, 7(1):1â€“22,
1964.
Bradly C. Stadie, Ge Yang, Rein Houthooft, Xi Chen, Yan Duan, Yuhuai Wu, Pieter Abbeel, and Ilya
Sutskever. Someconsiderationsonlearningtoexploreviameta-reinforcementlearning,2019. URL
https://arxiv.org/abs/1803.01118.
60

<!-- Page 61 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,
Dario Amodei, and Paul Christiano. Learning to summarize from human feedback, 2022.
Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider, Tengyang Xie, Stefano
Ermon, Chelsea Finn, and Aviral Kumar. Preference fine-tuning of llms should leverage suboptimal,
on-policy data. arXiv preprint arXiv:2404.14367, 2024.
Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown, November 2024. URL
https://qwenlm.github.io/blog/qwq-32b-preview/.
Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, and Dong Yu. Toward selfimprovement of llms via imagination, searching, and criticizing, 2024. URL https://arxiv.
org/abs/2404.12253.
ShubhamToshniwal,WeiDu,IvanMoshkov,BranislavKisacanin,AlexanAyrapetyan,andIgorGitman.
Openmathinstruct-2: Accelerating ai for math with massive open-source instruction data. arXiv
preprint arXiv:2410.01560, 2024.
PeiyiWang,LeiLi,ZhihongShao,RunxinXu,DamaiDai,YifeiLi,DeliChen,YuWu,andZhifangSui.
Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings
of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
pp. 9426â€“9439, 2024.
Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Mach. Learn., 8(3â€“4):229â€“256, may 1992. ISSN 0885-6125. doi: 10.1007/BF00992696.
URL https://doi.org/10.1007/BF00992696.
Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy P. Lillicrap, Kenji Kawaguchi, and
Michael Shieh. Monte carlo tree search boosts reasoning via iterative preference learning, 2024.
URL https://arxiv.org/abs/2405.00451.
Mengjiao Yang, Dale Schuurmans, Pieter Abbeel, and Ofir Nachum. Chain of thought imitation with
procedure cloning, 2022. URL https://arxiv.org/abs/2205.10816.
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik
Narasimhan. Treeofthoughts: Deliberateproblemsolvingwithlargelanguagemodels,2023. URL
https://arxiv.org/abs/2305.10601.
Tian Ye, Zicheng Xu, Yuanzhi Li, and Zeyuan Allen-Zhu. Physics of language models: Part 2.1,
grade-school math and the hidden reasoning process, 2024a. URL https://arxiv.org/abs/
2407.20311.
Tian Ye, Zicheng Xu, Yuanzhi Li, and Zeyuan Allen-Zhu. Physics of language models: Part 2.2, how
to learn from mistakes on grade-school math problems, 2024b. URL https://arxiv.org/abs/
2408.16293.
Shuo Yin, Weihao You, Zhilong Ji, Guoqiang Zhong, and Jinfeng Bai. Mumath-code: Combining tooluse large language models with multi-perspective data augmentation for mathematical reasoning.
arXiv preprint arXiv:2405.07551, 2024.
Xiao Yu, Baolin Peng, Vineeth Vajipey, Hao Cheng, Michel Galley, Jianfeng Gao, and Zhou Yu.
Exact: Teaching ai agents to explore with reflective-mcts and exploratory learning, 2024. URL
https://arxiv.org/abs/2410.02052.
61

<!-- Page 62 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou,
and Jingren Zhou. Scaling relationship on learning mathematical reasoning with large language
models, 2023. URL https://arxiv.org/abs/2308.01825.
Albert S. Yue, Lovish Madaan, Ted Moskovitz, DJ Strouse, and Aaditya K. Singh. HARP: A challenging human-annotated math reasoning benchmark, 2024. URL https://github.com/
aadityasingh/HARP.
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with
reasoning. Advances in Neural Information Processing Systems, 35:15476â€“15488, 2022.
Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah D Goodman.
Quiet-star: Language models can teach themselves to think before speaking. arXiv preprint
arXiv:2403.09629, 2024.
Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal.
Generative verifiers: Reward modeling as next-token prediction, 2024a. URL https://arxiv.
org/abs/2408.15240.
Yiming Zhang, Jianfeng Chi, Hailey Nguyen, Kartikeya Upasani, Daniel M. Bikel, Jason Weston, and
EricMichaelSmith. Backtrackingimprovesgenerationsafety, 2024b. URLhttps://arxiv.org/
abs/2409.14586.
Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language
agent tree search unifies reasoning acting and planning in language models, 2024a. URL https:
//arxiv.org/abs/2310.04406.
Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and Aviral Kumar. Archer: Training language
modelagentsviahierarchicalmulti-turnrl,2024b. URLhttps://arxiv.org/abs/2402.19446.

### A. Prompting

We investigate whether carefully constructed prompting protocols can induce reasoning capabilities
similartothosedemonstratedbyRL-basedmodelslikeO1. Throughanevaluationoffiveincreasingly
sophisticated instruction sets, we analyze token generation patterns, presence of reasoning-like
behaviors, and their correlation with math problem-solving performance.
Table 4: Input token count across prompting strategies using OpenAIâ€™s GPT-4o tokenizer (excluding
problem tokens). Complex instructions like Think (3-shot) use substantially more tokens, primarily
due to in-context demonstrations and explicit capabilities descriptions (e.g. thinking, reflection, and
verification requirements).
Strategy Tokens

### Baseline 23

Chain-of-Thought (CoT) 35

### Think 1,895

Think with demonstrations (3-shot) 4,659
Think & Verify 2,111
62

<!-- Page 63 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought

### B. Regret Analysis

Table 5 presents detailed statistics on regret expression across different models and prompting
strategies. The data shows varying levels of self-correction and error acknowledgment behaviors
across model scales and prompting approaches.

### C. Different Instruction Tuning Objectives

Below we outline a number of potential finetuning objectives for the instruction tuning stage of our
reasoning pipeline described in Section 7.1.

## A standard procedural-cloning approach (Yang et al., 2022) which uses traditional supervised

fine-tuning
â¡ â¤

## |Z| |S|

âˆ‘ï¸ âˆ‘ï¸
â„’(ğœƒ) = minâˆ’E (q,Z,S)âˆ¼ğ’Ÿtrain â£ logğœ‹ ğœƒ (z ğ‘–+1 |Z ğ‘– ,q)+ logğœ‹ ğœƒ (s ğ‘–+1 |S ğ‘– ,Z,q)â¦. (22)
ğœƒ
ğ‘–=1 ğ‘–=1

## Alternatively, we can optimize only over the Meta-CoT tokens with the following optimization

objective
â¡ â¤

## |Z|

âˆ‘ï¸
â„’(ğœƒ) = minâˆ’E (q,Z,S)âˆ¼ğ’Ÿtrain â£ logğœ‹ ğœƒ (z ğ‘–+1 |Z ğ‘– ,q)â¦. (23)
ğœƒ
ğ‘–=1
In this formulation, the model will not learn to generate a solution, requiring the use of a
separate step to summarize the search process into a final solution. One hypothesis is that
training on joint sequences with the solution can serve as additional supervision to help the
model with maintaining internal state.

## One consideration is whether updating model parameters for sub-optimal, or even incorrect,

branches can induce the model to generate more errors. To handle this, we can mask incorrect
steps/branches in the above loss
â¡ â¤

## |Z|

âˆ‘ï¸
â„’(ğœƒ) = minâˆ’E (q,Z,S)âˆ¼ğ’Ÿtrain â£ ğ¼{z ğ‘–+1 âˆˆ S}logğœ‹ ğœƒ (z ğ‘–+1 |Z ğ‘– ,q)â¦. (24)
ğœƒ
ğ‘–=1
Thatisweonlytrainonthebranchesthatareonthecorrectpathfromtherootnode(problem)
tothefinalanswer. Priorworks(Gandhietal.,2024;Yeetal.,2024b)didnotmasktheincorrect
steps, and reported no degradation in performance, as long as the data distribution remains
reasonable. On the other hand Zhang et al. (2024b) specifically mask the tokens of the unsafe
generation.

## Under the above objective, there could be significant task mismatch between training and

generation, especially in the case of long search chains. One method to mitigate this issue is to
generate more synthetic training data by pruning intermediate branches not on the optimal
solution path to generate additional sequences. For example this was the SFT objective used
byZhangetal.(2024b),whichsimultaneouslytrainsonbacktrackingtrajectories(withmasking)
and the final optimal solution (Equation 1 in that paper).

### D. MCTS Details

Here we outline the details of our MCTS procedure from Section 4.3.1. Our procedure involves three
main steps: selection, backup, and expansion. We describe each step in detail below.
63

<!-- Page 64 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Selection. Startingattherootnodes (initializedasthequestionq), achildnodes(i.e. thenext
0
stepinthesolution)isselecteduntilaleafnodes isreached. Apartialsolutionisthenrepresentedas
ğ‘¡
S = (s ,...,s ). The child node is selected according to the policy s = argmax ğ‘ˆ(S ,s) where
ğ‘¡ 1 ğ‘¡ ğ‘¡+1 s ğ‘¡
ğ‘ˆ(S ,s) is calculated using UCT (Kocsis & SzepesvÃ¡ri, 2006), defined as
ğ‘¡
âˆšï¸ƒ
logğ‘(S ,s)
ğ‘¡

## ğ‘ˆ(S

ğ‘¡
,s) = ğ‘„(S
ğ‘¡
,s)+ğ‘exp .

## ğ‘(S )

ğ‘¡
In this equation ğ‘„(S ,s) is a value function, ğ‘(S ,s) is the visit count of selecting step s from the
ğ‘¡ ğ‘¡
partialsolutionS
ğ‘¡

## ,ğ‘(S

ğ‘¡
) = âˆ‘ï¸€
ğ‘ 

## ğ‘(S

ğ‘¡
,s)isthetotalvisitcountofthepartialsolutionS
ğ‘¡
,andğ‘exp âˆˆ R
is the exploration constant. We opt for the UCT formulation, instead of the more recent PUCT
variant (Rosin, 2011), as in our case actions are represented by logical steps and thus likelihoods
could be quite skewed.
Expansion. After the selection operation, we have chosen a leaf node s , and the path from the
ğ‘¡
root node s to the leaf node forms a partial solution by concatenating the individual steps into the
0
partial solution, i.e. S = (s ,...,s ). If s is terminal, we do not expand it, otherwise the node is
ğ‘¡ 1 ğ‘¡ ğ‘¡
expanded by sampling ğ‘ actions {sğ‘– }ğ‘ âˆ¼ ğœ‹ (Â·|S ) from the policy ğœ‹ , and then adding the nodes
ğ‘¡+1 ğ‘–=1 ğœƒ ğ‘¡ ğœƒ
{ağ‘– }ğ‘ aschildrenofnodeS . Thevalueofeachchildnodeisinitializedwithavaluefunctionğ‘£ as
ğ‘¡+1 ğ‘–=1 ğ‘¡
ğ‘„(S ,sğ‘–) = ğ‘£((S ,sğ‘– ),q). This evaluation essentially replaces the "simulation" step usually carried
ğ‘¡ ğ‘¡ ğ‘¡ ğ‘¡+1
out in game settings.
Backup. Finally, the statistics of the expanded node S are backed up through the path to the
ğ‘¡
root node s . For every ancestor node S on the path, the visit count is incremented
0 ğ‘–

## ğ‘(S ) = ğ‘(S )+1

ğ‘– ğ‘–
and the value of each ancestor is updated as
ğ‘£(S ,q)âˆ’ğ‘„(S ,s )
ğ‘¡ ğ‘– ğ‘–+1
ğ‘„(S ,s ) = ğ‘„(S ,s )+ .
ğ‘– ğ‘–+1 ğ‘– ğ‘–+1
ğ‘(S ,s )
ğ‘– ğ‘–+1

### E. Chains-Of-Thought

This section contains synthetic CoT traces comparing how different models reason through the same
planningproblems. Specifically,weshowcompletereasoningtracesfromGemini-1.5solvingproblems
using A* and MCTS algorithms, alongside parallel traces from DeepSeek-R1, DeepSeek-O1, and
Gemini-2 working through identical tasks. We also include an example trace demonstrating how a
smaller SFT-tuned Llama3.1-8B model performs tool-augmented reasoning on a challenging problem
using dynamic programming. Each trace details the step-by-step "internal" traces and the final
solution.
64

<!-- Page 65 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Therewere12seatspreparedforthejuryataroundtable,eachwithanameindicatedon
it. Nikolai Nikolaevich, who arrived first, absent-mindedly sat not in his designated seat
but in the next one clockwise. Each jury member arriving at the table afterward would
eithersitintheirdesignatedseat,or,ifitwasalreadyoccupied,theywouldwalkaround
the table clockwise and sit in the next available seat. The resulting seating arrangement
of the jury members depends on the order in which they approached the table. How
many different ways can the jury be seated?
0.0
Let's break this down step-by-step:
0.03125

## The first person, Nikolai, can sit in any of the 12 seats.


## The second person can sit in any of the remaining 11 seats.


## The third person can sit in any of the remaining 10 seats.


## This pattern continues until all 12 seats are filled.

0.0
Alternatively...

## The first person, Nikolai Nikolaevich, has only one choice for his

seat.

## The second person has one choice, as both their designated seat and

the seat next to it are taken.

## The third person also has one choice, as two seats are taken.


## This pattern continues until all seats are filled.

0.0
Alternatively...

## The first member can choose any of the 12 seats.


## The second member can choose any of the remaining 11 seats, but if

the first member chose the next seat clockwise, the second member
has only 10 options (not considering the first seat). This pattern
continues until the last member, who has only 1 option left.
0.0
Alternatively...

## Nikolai Nikolaevich can only sit in one position (the next seat

clockwise from his designated seat).

## After Nikolai Nikolaevich sits down, there are 11 remaining seats.


## The next person to sit can choose from the 11 remaining seats, but

if one of those seats is already occupied, they have to sit in the
next available seat. This means that the second person has 11
choices.

## The third person then has 10 choices, and so on, until the last

person has only 1 choice.
0.0
65

<!-- Page 66 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Alternatively...

## The first person, Nikolai Nikolaevich, has a fixed position.


## The second person can either sit in their designated seat or move to

the next seat clockwise. This gives 2 options for the second
person.

## The third person can either sit in their designated seat or move to

the next seat clockwise. This gives 2 options for the third person,
but we need to consider the case where the second person moved, so
there's only 1 additional option for the third person.

## Similarly, for each subsequent person, there are 2 options if the

previous person didn't move, and 1 additional option if the
previous person moved.
0.046875
Let's analyze the pattern:
0.0390625
- The first person has 1 option.
- The second person has 2 options.
- The third person has 2 options.
- The fourth person has 2 options if the third person didn't move, and
1 option if the third person moved, but since the second person
moved, the fourth person still has 2 options.
- This pattern continues for the rest of the jury members.
0.03125
So, the total number of ways the jury can be seated is the product of
the options for each person:
0.03125
\[ 1 \times 2 \times 2 \times 2 \times 2 \times 2 \times 2 \times 2 \
times 2 \times 2 \times 2 \times 2 = 2^{11} = 2048 \]
0.0078125
Wait, however, this process can be simplified by realizing that each
person has a unique "decision point" - the first time they have a
choice between sitting in their designated seat and moving to the
next seat. After that, their seat is determined by the previous
choices.
0.0078125
Alternatively, however, this is a circular arrangement, and we need to
account for the fact that the last person can also move. This
introduces a complication, as the last person will always have only
1 option if everyone else is seated.
0.0546875
66

<!-- Page 67 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Let's simplify the problem by considering the first 12 people. For each
of these 12 people, there are 2 options. However, we need to
subtract the cases where the last person doesn't move, as these
cases are already counted in the 2 options for each person.
0.0078125
Wait, to simplify, let's consider the arrangement of people relative to

### Nikolai Nikolaevich's fixed position. There are 11! ways to

arrange the remaining jury members around the table, as each person
has a fixed position relative to Nikolai.
0.0234375
However, this calculation doesn't take into account the fact that some
people can move. To correct for this, we need to consider the
number of "cycle" arrangements, where each cycle represents a group
of people who moved together.
0.0078125
Wait, let's re-examine the situation:
0.046875

## Nikolai Nikolaevich is seated.


## The second person can move or stay, giving 2 options.


## For each subsequent person, there's only 1 option left if everyone

else is seated.
0.046875
The pattern continues until the last person, who always has only 1
option.
0.03125
So, the total number of arrangements is the product of the options at
each step:
0.0390625
\[ 2 \times 2 \times 2 \times 2 \times 2 \times 2 \times 2 \times 2 \
times 2 \times 2 \times 1 = 2^{11} = \boxed{2048} \]
0.0
Wait, let's consider the total number of ways without the circular
constraint. For the first person, there's 1 option. For the rest,
there are 2 options each, except for the last person who has only 1
option. So, the total number of arrangements without the circular
constraint is:
\[ 1 \times 2^{11} \]
67

<!-- Page 68 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
0.171875
However, we need to subtract the cases where everyone is seated in
their designated seats, which is 1 arrangement. But this is already
counted in the 2 options for each person.
0.03125
Alternatively, however, this counts each arrangement twice, since the
table is round, and we can rotate the arrangement without changing
it. To account for this, we divide by the number of seats, which is
12.
0.0703125
Alternatively, however, this counts each arrangement twice (once in
each direction around the table), so we need to divide by 2:
\[ \frac{1 \times 2^{11}}{2} = 2^{10} \]
0.328125
But this is not the correct answer. The correct answer is the number of
derangements of 12 objects, which is given by the formula:
\[!12 = 12! \left( 1 - \frac{1}{1!} + \frac{1}{2!} - \frac{1}{3!} + \
cdots + \frac{(-1)^{12}}{12!} \right) \]
0.0078125
Alternatively, but this is still not correct because it doesn't account
for the fact that the last person always has only 1 option. Let's
reconsider the problem.
0.0625
Alternatively, but this is not the correct answer because the problem
statement asks for the number of different ways the jury can be
seated, considering the circular arrangement. In a circular
arrangement, the last person's position is fixed once the first
person's position is chosen. Therefore, the correct calculation
should consider the number of ways to arrange the remaining 11
people around the table, not the total possible arrangements.
0.1484375
Alternatively, but we're not done yet! We need to consider the case
where the second person moves, which would cause the last person to
have only 1 option. This introduces an overcount. To correct for
this, we need to subtract the number of arrangements where the
second person moves, which is equivalent to the number of ways the
jury can be seated with one person missing (i.e., 11 people). This
is:
\[ 2^{10} - 1 \]
0.1796875
68

<!-- Page 69 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
But we still need to account for the fact that the last person has only
1 option. This means that we've overcounted by a factor of 2 for
the last person. To correct this, we divide by 2 again:
\[ \frac{2^{10}}{2} = 2^9 \]
0.0390625
Wait, but we also need to consider the case where the first person
moves, which would be counted 12 times (once for each possible
position). So, we need to subtract these 11 arrangements:
\[ 2^{10} - 11 \]
0.1484375
Alternatively, this is still not correct, as we haven't accounted for
the fact that the last person can only move one seat. We need to
consider the cases where the last person moves and subtract those
from the total.
0.1796875
But we need to correct this approach because we've double-counted due
to the nature of the circular arrangement.
0.015625
Alternatively, we also need to consider the case where someone arrives
after the first person and the table is empty. In this case, the
first person still has 1 option, and the remaining 11 people have 2
options each, but again, we need to divide by 2 to account for the
circular arrangement:
\[ \frac{1 \times 2^{11}}{2} = 2^{10} \]
0.2421875
However, this is not the correct answer. The correct answer is:
\[ \boxed{2^{11}} \]
0.0
Alternatively, but this doesn't account for the fact that the last
person has only 1 option. To adjust for this, we need to consider
the cases where the last person moves. This introduces a more
complex conditional arrangement, but the key insight is that the
number of choices for the last person is actually the same as the
number of choices for any other person, because the arrangement is
circular.
0.1015625
Alternatively, but we still need to account for the fact that the last
person can only move in one direction. This doesn't affect the
number of arrangements, as the circular constraint is already
accounted for by the division by 2.
69

<!-- Page 70 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
0.953125
So, the total number of different ways the jury can be seated is:
\[ \boxed{1024} \]
1.0

### Solution:


### Letâ€™s break this down step-by-step:


## The first person, Nikolai Nikolaevich, has a fixed position. 2. The second person can

eithersitintheirdesignatedseatormovetothenextseatclockwise. Thisgives2options
forthesecondperson. 3. Thethirdpersoncaneithersitintheirdesignatedseatormove
to the next seat clockwise. This gives 2 options for the third person, but we need to
consider the case where the second person moved, so thereâ€™s only 1 additional option
for the third person. 4. Similarly, for each subsequent person, there are 2 options if the
previous person didnâ€™t move, and 1 additional option if the previous person moved.
However, this is a circular arrangement, and we need to account for the fact that the
last person can also move. This introduces a complication, as the last person will always
have only 1 option if everyone else is seated.
Letâ€™s consider the total number of ways without the circular constraint. For the first
person,thereâ€™s1option. Fortherest,thereare2optionseach,exceptforthelastperson
who has only 1 option. So, the total number of arrangements without the circular
constraint is:
1Ã—211
However, this counts each arrangement twice (once in each direction around the table),
so we need to divide by 2:
1Ã—211
= 210
2
But we still need to account for the fact that the last person can only move in one
direction. This doesnâ€™t affect the number of arrangements, as the circular constraint is
already accounted for by the division by 2.
So, the total number of different ways the jury can be seated is:
1024
70

<!-- Page 71 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Trace - Reasoning with External Tools

### A* Generated Solution Trace

Question: There were 12 seats prepared for the jury at a round table, each with a name
indicated on it. Nikolai Nikolaevich, who arrived first, absent-mindedly sat not in his
designated seat but in the next one clockwise. Each jury member arriving at the table
afterward would either sit in their designated seat, or, if it was already occupied, they
would walk around the table clockwise and sit in the next available seat. The resulting
seatingarrangementofthejurymembersdependsontheorderinwhichtheyapproached
the table. How many different ways can the jury be seated?
To solve this problem, we need to determine the total number of
different seating arrangements of 12 jury members when each member
either sits in their designated seat or walks around the table
clockwise to sit in the next available seat.
0.01171875
To solve this problem, we need to analyze the seating possibilities
given the rules described:
0.0078125

## Nikolai Nikolaevich starts in the next seat clockwise.


## Each subsequent member either sits in their designated seat or the

next available seat.
0.015625
To determine the number of ways the jury can be seated, we need to
consider the following steps:
0.01171875

## Assign Nikolai Nikolaevich to the next seat clockwise.


## Each subsequent jury member has 2 choices: their designated seat or

the next available seat.

## Since Nikolai Nikolaeich has already changed seats, we need to

account for the extra step he took.
0.046875
Let's represent this using Python code to calculate the number of
different ways the jury can be seated.
0.05078125
We can model this problem using dynamic programming, where each state
represents the number of available seats and the current seat
Nikolai is sitting on. We'll use a recursive approach to count the
number of valid seating arrangements.
71

<!-- Page 72 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
0.0390625
Here's the Python code to solve this problem:
0.03515625
Let's implement this in Python.
0.03515625
```python
def count\_seating\_arrangements(total\_seats):
dp = [0] * (total\_seats + 1)
dp[1] = 1 \# Initial case: Nikolai is in the next seat clockwise
\# Iterate over all possible remaining seats where Nikolai could be
for seat in range(total\_seats):
for available\_seats in range(seat + 1, total\_seats):
dp[available\_seats] += dp[available\_seats - seat - 1]
return dp[total\_seats]
\# There are 12 seats prepared for the jury
total\_seats = 12
seating\_ways = count\_seating\_arrangements(total\_seats)
print(seating\_ways)
```
```output
0
```
0.03125
It seems our initial approach overlooked a crucial detail. We need to
consider the actual sequence of each member. Each member can either:

## Sit in their designated seat.


## Walk to the next available seat.

0.04296875
It seems there was an issue with the initial state of the dynamic
programming approach. Let's refine our approach:
72

<!-- Page 73 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
0.02734375

## We should count directly from the number of valid seating

arrangements starting from Nikolai's initial seat.

## For each subsequent member, they have two choices: their designated

seat or the next available seat.
0.046875
Let's re-implement the logic more carefully:
0.046875

## We need to correctly initialize the base case.


## We need to ensure we correctly handle the initial arrangement where

Nikolai is in the next seat clockwise.
0.04296875
Let's reconsider the logic and the initialization of the DP table:
0.0546875
Here's an updated approach:
0.04296875

## We start by initializing the base case correctly.


## We use an iterative approach instead of dynamic programming to

simplify the calculation.
0.09375
Let's implement the solution again:
0.12109375
Let's implement this revised approach in Python:
0.09765625
Let's implement the updated version in Python:
0.07421875
```python
def
count\_seating\_arrangements(total\_seats):
73

<!-- Page 74 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
\# Initialize the dp array
dp = [0] * (total\_seats + 1)
\# Base case: Nikolai is in the next seat clockwise
dp[1] = 1
\# Iterate over the remaining seats
for current\_seat in range(2, total\_seats + 1):
\# Iterate over possible current seats
for seat in range(current\_seat):
dp[current\_seat] += dp[seat]
return dp[total\_seats]
\# There are 12 seats prepared for the jury
total\_seats = 12
seating\_ways = count\_seating\_arrangements(total\_seats)
print(seating\_ways)
```
```output
1024
```
0.94921875
\begin{tcolorbox}[colframe=black, colback=white, boxrule=1pt, arc=0pt]
\begin{lstlisting}
The number of different ways the jury can be seated, given that Nikolai
Nikolaeich started in the next seat clockwise and each subsequent
member either sits in their designated seat or the next available
seat, is \(\boxed{1024}\).
1

### Solution:

To solve this problem, we need to analyze the seating possibilities given the rules
described:

## Nikolai Nikolaevich starts in the next seat clockwise. 2. Each subsequent member

either sits in their designated seat or the next available seat.
To determine the number of ways the jury can be seated, we need to consider the
following steps:
74

<!-- Page 75 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought

## Assign Nikolai Nikolaevich to the next seat clockwise.


## Eachsubsequentjurymemberhas2choices: theirdesignatedseatorthenextavailable

seat. 3. Since Nikolai Nikolaeich has already changed seats, we need to account for the
extra step he took.
We can model this problem using dynamic programming, where each state represents
the number of available seats and the current seat Nikolai is sitting on. Weâ€™ll use a
recursive approach to count the number of valid seating arrangements.
Letâ€™s implement this in Python.
```python
def count\_seating\_arrangements(total\_seats):
dp = [0] * (total\_seats + 1)
dp[1] = 1 \# Initial case: Nikolai is in the next seat clockwise
\# Iterate over all possible remaining seats where Nikolai could be
for seat in range(total\_seats):
for available\_seats in range(seat + 1, total\_seats):
dp[available\_seats] += dp[available\_seats - seat - 1]
return dp[total\_seats]
\# There are 12 seats prepared for the jury
total\_seats = 12
seating\_ways = count\_seating\_arrangements(total\_seats)
print(seating\_ways)
```
```output
0
```
It seems there was an issue with the initial state of the dynamic programming approach.

### Letâ€™s refine our approach:


## Weneedtocorrectlyinitializethebasecase. 2. Weneedtoensurewecorrectlyhandle

the initial arrangement where Nikolai is in the next seat clockwise.

### Hereâ€™s an updated approach:


## Westartbyinitializingthebasecasecorrectly. 2. Weuseaniterativeapproachinstead

of dynamic programming to simplify the calculation.
Letâ€™s implement the updated version in Python:
75

<!-- Page 76 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
```python
def count\_seating\_arrangements(total\_seats):
\# Initialize the dp array
dp = [0] * (total\_seats + 1)
\# Base case: Nikolai is in the next seat clockwise
dp[1] = 1
\# Iterate over the remaining seats
for current\_seat in range(2, total\_seats + 1):
\# Iterate over possible current seats
for seat in range(current\_seat):
dp[current\_seat] += dp[seat]
return dp[total\_seats]
\# There are 12 seats prepared for the jury
total\_seats = 12
seating\_ways = count\_seating\_arrangements(total\_seats)
print(seating\_ways)
```
```output
1024
```
The number of different ways the jury can be seated, given that Nikolai
Nikolaeich started in the next seat clockwise and each subsequent
member either sits in their designated seat or the next available
seat, is \(\boxed{1024}\).
76

<!-- Page 77 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought

### O1 official math CoT

A* Generated Solution Trace (Gemini 1.5)

### Problem

Let ğ‘› be an even positive integer. Let ğ‘ be a monic, real polynomial of degree 2ğ‘›;
that is to say,ğ‘(ğ‘¥) = ğ‘¥2ğ‘›+ğ‘ ğ‘¥2ğ‘›âˆ’1+Â·Â·Â·+ğ‘ ğ‘¥+ğ‘
2ğ‘›âˆ’1 1 0
for some real coefficients ğ‘ ,...,ğ‘ .
0 2ğ‘›âˆ’1
Suppose that ğ‘ (ï¸€1)ï¸€ = ğ‘˜2 for all integers ğ‘˜ such that 1 â‰¤ |ğ‘˜| â‰¤ ğ‘›.
ğ‘˜
Find all other real numbers ğ‘¥ for which ğ‘ (ï¸€1)ï¸€ = ğ‘¥2.
ğ‘¥

### Solution

0.0234375
Let $q(x) = x^{2n}p\left(\frac{1}{x}\right) = a_0 x^{2n} + a_1 x^{2n-1}
+ \cdots + a_{2n-1} x + 1$.
0.015625
The given condition $p\left(\frac{1}{k}\right) = k^2$ implies $q(k) = k
^{2n} k^2 = k^{2n+2}$ for $1 \leq |k| \leq n$.
0.0
Let $p(x) = x^{2n} + a_{2n-1}x^{2n-1} + \cdots + a_1x + a_0$ be a monic
polynomial of degree $2n$, where $n$ is an even positive integer.
0.0
We are given that $p\left(\frac{1}{k}\right) = k^2$ for all integers
$k$ such that $1 \leq |k| \leq n$.
0.0078125
We are given that $p\left(\frac{1}{k}\right) = k^2$ for integers $k$
such that $1 \leq |k| \leq n$.
0.0
The condition $p\left(\frac{1}{k}\right) = k^2$ for $1 \leq |k| \leq n$
is equivalent to $q(k) = k^{2n} k^2 = k^{2n+2}$ for $1 \leq |k| \
leq n$.
0.03125
Consider the polynomial $r(x) = x^{2n+2} - q(x)$. We are given that $r(
k) = 0$ for $1 \leq |k| \leq n$.
0.0625
77

<!-- Page 78 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Since $n$ is even, let $n = 2m$. Then we have $2n = 4m$. Thus, $r(x)$
has the roots $\pm 1, \pm 2, \ldots, \pm 2m$.
0.0703125
$r(x) = (x-1)(x+1)(x-2)(x+2) \cdots (x-2m)(x+2m) = \prod_{k=1}^{2m} (xk)(x+k) = \prod_{k=1}^{2m} (x^2-k^2)$.
0.015625
Since $q(x)$ is a polynomial of degree $2n$, $r(x)$ is a monic
polynomial of degree $2n+2 = 4m+2$.
0.0078125
Therefore, we can write
0.0703125
$$ r(x) = (x^2-1)(x^2-2^2)\cdots(x^2 - (2m)^2) = (x^2-1)(x^2-4)\cdots(x
^2 - 4m^2) $$
0.0
Therefore, we can write $r(x) = x^{2n+2} - q(x) = (x^2-1)(x^2-4)\cdots(
x^2-n^2)R(x)$, where $R(x)$ is a polynomial of degree $2n+2 - 2n =
2$.
0.1015625
Since $q(x)$ is monic with degree $2n$, $x^{2n+2} - q(x)$ has leading
coefficient 1. Also, the leading coefficient of $(x^2-1)(x^2-4)\
cdots(x^2-n^2)$ is 1. Thus, $R(x)$ must be monic.
0.03125
Let $R(x) = x^2 + bx + c$. Then we have $r(x) = x^{2n+2} - q(x) = (x
^2-1)(x^2-4)\cdots(x^2-n^2)(x^2 + bx + c)$.
0.1953125
We are looking for values of $x$ such that $p\left(\frac{1}{x}\right) =
x^2$, which is equivalent to $q(x) = x^{2n+2}$. This means we want
to find the roots of $r(x) = 0$. We know that $r(x) = 0$ for $x =
\pm k$ for $1 \leq k \leq n$. We are looking for the roots of $x^2 +
bx + c = 0$.
0.1796875
Since $r(0) = (-1)^n(-1)(-4)\cdots(-n^2)c = (-1)^n (-1)^n (1)(4)\cdots(
n^2)c = n!^2 c$, we have $r(0) = -q(0) = -1$. Thus $n!^2 c = -1$,
so $c = -\frac{1}{n!^2}$.
0.40625
78

<!-- Page 79 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
$r(x) = (x^2-1)(x^2-4)\cdots(x^2-n^2)(x^2+bx-\frac{1}{n!^2})$.
0.3984375
Since the coefficients of $p(x)$ are real, the coefficients of $q(x)$
are real, and so the coefficients of $r(x)$ are real. Therefore,
the coefficients of $R(x)$ are real. In particular, $b$ is real.
The roots of $x^2+bx-\frac{1}{n!^2}=0$ are $x=\frac{-b \pm \sqrt{b
^2+\frac{4}{n!^2}}}{2}$. Thus the roots are real if $b^2+\frac{4}{n
!^2} \geq 0$, which is always true since $b$ is real.
0.265625
Consider $r(x) = x^{2n+2} - q(x)$.
0.359375
The equation $p(\frac{1}{x}) = x^2$ implies $x^{2n} p(\frac{1}{x}) = x
^{2n+2}$, so $q(x) = x^{2n+2}$, thus $x^{2n+2} - q(x) = 0$. We
already know that $x=\pm k$, $1 \le k \le n$, are roots.
0.390625
We have $r(x) = (x^2-1)\cdots(x^2-n^2)(x^2+bx-\frac{1}{n!^2})$.
0.359375
Since $r(x)$ is an even function, $b=0$.
0.6875
The other solutions are the roots of $x^2 - \frac{1}{n!^2} = 0$, so $x
= \pm \frac{1}{n!}$.
0.796875
Final Answer: The final answer is $\boxed{\pm \frac{1}{n!}}$
1.0

### Solution

Let ğ‘(ğ‘¥) = ğ‘¥2ğ‘›ğ‘ (ï¸€1)ï¸€ = ğ‘ ğ‘¥2ğ‘›+ğ‘ ğ‘¥2ğ‘›âˆ’1+Â·Â·Â·+ğ‘ ğ‘¥+1.
ğ‘¥ 0 1 2ğ‘›âˆ’1
The given condition ğ‘ (ï¸€1)ï¸€ = ğ‘˜2 implies ğ‘(ğ‘˜) = ğ‘˜2ğ‘›ğ‘˜2 = ğ‘˜2ğ‘›+2 for 1 â‰¤ |ğ‘˜| â‰¤ ğ‘›.
ğ‘˜
Considerthepolynomialğ‘Ÿ(ğ‘¥) = ğ‘¥2ğ‘›+2âˆ’ğ‘(ğ‘¥). Wearegiventhatğ‘Ÿ(ğ‘˜) = 0for1 â‰¤ |ğ‘˜| â‰¤ ğ‘›.
Since ğ‘› is even, let ğ‘› = 2ğ‘š. Then we have 2ğ‘› = 4ğ‘š. Thus, ğ‘Ÿ(ğ‘¥) has the roots
Â±1,Â±2,...,Â±2ğ‘š.
ğ‘Ÿ(ğ‘¥) = (ğ‘¥ âˆ’ 1)(ğ‘¥ + 1)(ğ‘¥ âˆ’ 2)(ğ‘¥ + 2)Â·Â·Â·(ğ‘¥ âˆ’ 2ğ‘š)(ğ‘¥ + 2ğ‘š) =
âˆï¸€2ğ‘š
(ğ‘¥ âˆ’ ğ‘˜)(ğ‘¥ + ğ‘˜) =
ğ‘˜=1
âˆï¸€2ğ‘š (ğ‘¥2âˆ’ğ‘˜2).
ğ‘˜=1
Since ğ‘(ğ‘¥) is a polynomial of degree 2ğ‘›, ğ‘Ÿ(ğ‘¥) is a monic polynomial of degree 2ğ‘›+2 =
4ğ‘š+2.
79

<!-- Page 80 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Therefore,wecanwriteğ‘Ÿ(ğ‘¥) = ğ‘¥2ğ‘›+2âˆ’ğ‘(ğ‘¥) = (ğ‘¥2âˆ’1)(ğ‘¥2âˆ’4)Â·Â·Â·(ğ‘¥2âˆ’ğ‘›2)ğ‘…(ğ‘¥),where
ğ‘…(ğ‘¥) is a polynomial of degree 2ğ‘›+2âˆ’2ğ‘› = 2.
Since ğ‘(ğ‘¥) is monic with degree 2ğ‘›, ğ‘¥2ğ‘›+2 âˆ’ğ‘(ğ‘¥) has leading coefficient 1. Also, the
leading coefficient of (ğ‘¥2âˆ’1)(ğ‘¥2âˆ’4)Â·Â·Â·(ğ‘¥2âˆ’ğ‘›2) is 1. Thus, ğ‘…(ğ‘¥) must be monic.
Let ğ‘…(ğ‘¥) = ğ‘¥2+ğ‘ğ‘¥+ğ‘. Then we have ğ‘Ÿ(ğ‘¥) = ğ‘¥2ğ‘›+2âˆ’ğ‘(ğ‘¥) = (ğ‘¥2âˆ’1)(ğ‘¥2âˆ’4)Â·Â·Â·(ğ‘¥2âˆ’
ğ‘›2)(ğ‘¥2+ğ‘ğ‘¥+ğ‘).
Wearelookingforvaluesofğ‘¥suchthatğ‘ (ï¸€1)ï¸€ = ğ‘¥2,whichisequivalenttoğ‘(ğ‘¥) = ğ‘¥2ğ‘›+2.
ğ‘¥
Thismeanswewanttofindtherootsofğ‘Ÿ(ğ‘¥) = 0. Weknowthatğ‘Ÿ(ğ‘¥) = 0forğ‘¥ = Â±ğ‘˜ for
1 â‰¤ ğ‘˜ â‰¤ ğ‘›. We are looking for the roots of ğ‘¥2+ğ‘ğ‘¥+ğ‘ = 0.
Since ğ‘Ÿ(0) = (âˆ’1)ğ‘›(âˆ’1)(âˆ’4)Â·Â·Â·(âˆ’ğ‘›2)ğ‘ = (âˆ’1)ğ‘›(âˆ’1)ğ‘›(1)(4)Â·Â·Â·(ğ‘›2)ğ‘ = ğ‘›!2ğ‘, we have
ğ‘Ÿ(0) = âˆ’ğ‘(0) = âˆ’1. Thus ğ‘›!2ğ‘ = âˆ’1, so ğ‘ = âˆ’ 1 .
ğ‘›!2
Since the coefficients of ğ‘(ğ‘¥) are real, the coefficients of ğ‘(ğ‘¥) are real, and so the
coefficients of ğ‘Ÿ(ğ‘¥) are real. Therefore, the coefficients of ğ‘…(ğ‘¥) are real. In particular,
âˆšï¸
âˆ’ğ‘Â± ğ‘2+ 4
ğ‘ is real. The roots of ğ‘¥2+ğ‘ğ‘¥âˆ’ 1 = 0 are ğ‘¥ = ğ‘›!2. Thus the roots are real if
ğ‘›!2 2
ğ‘2+ 4 â‰¥ 0, which is always true since ğ‘ is real.
ğ‘›!2
Theequationğ‘(1) = ğ‘¥2impliesğ‘¥2ğ‘›ğ‘(1) = ğ‘¥2ğ‘›+2,soğ‘(ğ‘¥) = ğ‘¥2ğ‘›+2,thusğ‘¥2ğ‘›+2âˆ’ğ‘(ğ‘¥) = 0.
ğ‘¥ ğ‘¥
We already know that ğ‘¥ = Â±ğ‘˜, 1 â‰¤ ğ‘˜ â‰¤ ğ‘›, are roots.
We have ğ‘Ÿ(ğ‘¥) = (ğ‘¥2âˆ’1)Â·Â·Â·(ğ‘¥2âˆ’ğ‘›2)(ğ‘¥2+ğ‘ğ‘¥âˆ’ 1 ).
ğ‘›!2
Since ğ‘Ÿ(ğ‘¥) is an even function, ğ‘ = 0.
The other solutions are the roots of ğ‘¥2âˆ’ 1 = 0, so ğ‘¥ = Â±1.
ğ‘›!2 ğ‘›!
1
Final Answer: The final answer is Â±
ğ‘›!
80

<!-- Page 81 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
MCTS Generated Solution Trace

### Problem

Let ğ‘› be an even positive integer. Let ğ‘ be a monic, real polynomial of degree 2ğ‘›;
that is to say, ğ‘(ğ‘¥) = ğ‘¥2ğ‘›+ğ‘ ğ‘¥2ğ‘›âˆ’1+Â·Â·Â·+ğ‘ ğ‘¥+ğ‘
2ğ‘›âˆ’1 1 0
for some real coefficients ğ‘ ,...,ğ‘ .
0 2ğ‘›âˆ’1
Suppose that ğ‘ (ï¸€1)ï¸€ = ğ‘˜2 for all integers ğ‘˜ such that 1 â‰¤ |ğ‘˜| â‰¤ ğ‘›.
ğ‘˜
Find all other real numbers ğ‘¥ for which ğ‘ (ï¸€1)ï¸€ = ğ‘¥2.
ğ‘¥
Solution
0.03125
Let $q(x) = p\left(\frac{1}{x}\right) - x^2$.
0.09375
Let $q(x) = p\left(\frac{1}{x}\right) - x^2$. Then $q(x)$ is a rational
function of the form
0.28125
$$q(x) = \frac{1 + a_{2n-1}x + \dots + a_1 x^{2n-1} + a_0 x^{2n} - x^{2
n+2}}{x^{2n}}.$$
0.3125
We are given that $p(x)$ is a monic real polynomial of degree $2n$.
0.0625
$$q(x) = \frac{1 + a_{2n-1}x + \dots + a_1x^{2n-1} + a_0x^{2n} - x^{2n
+2}}{x^{2n}}.$$
0.25
$$q(x) = \frac{1 + a_{2n-1}x + \cdots + a_1x^{2n-1} + a_0x^{2n} - x^{2n
+2}}{x^{2n}} = \frac{r(x)}{x^{2n}}.$$
0.15625
We are given that $p(x)$ is a monic real polynomial of degree $2n$.
0.0625
We are given that $p\left(\frac{1}{k}\right) = k^2$ for $1 \leq |k| \
leq n$. This means $q(k) = 0$ for $1 \leq |k| \leq n$. Thus, $q(x)$
has roots $k$ where $1 \leq |k| \leq n$. This accounts for $2n$
roots.
0.25
81

<!-- Page 82 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Since $p(1/k) = k^2$ for $1 \le |k| \le n$, we have $q(k) = 0$ for $1 \
le |k| \le n$.
0.21875
We are given that $p\left(\frac{1}{k}\right) = k^2$ for $1 \leq |k| \
leq n$, which means $q(k) = p\left(\frac{1}{k}\right) - k^2 = 0$
for $1 \leq |k| \leq n$. Thus, $r(x)$ has roots at $x = k$ for $1 \
leq |k| \leq n$. Since $r(x)$ is a polynomial of degree $2n+2$, we
can write
0.25
We are given that $p\left(\frac{1}{k}\right) = k^2$ for $1 \leq |k| \
leq n$, which means $q(k) = p\left(\frac{1}{k}\right) - k^2 = 0$
for $1 \leq |k| \leq n$. Thus, the integers $k = \pm 1, \pm 2, \
ldots, \pm n$ are roots of the polynomial $r(x) = 1 + a_{2n-1}x + \
cdots + a_1x^{2n-1} + a_0x^{2n} - x^{2n+2}$.
0.3125
Since $r(x)$ is a polynomial of degree $2n+2$, and we know $2n$
distinct roots, we can write
0.1875
Since $r(x)$ has degree $2n+2$ and has roots $\pm 1, \pm 2, \ldots, \pm
n$, there are $2n$ such roots. Thus, we can write
0.375
Since $p\left(\frac{1}{k}\right) = k^2$ for $1 \leq |k| \leq n$, we
have $q(k) = 0$ for $1 \leq |k| \leq n$. Thus, the numerator of $q(
x)$, which we denote by $r(x)$, has roots $k$ where $1 \leq |k| \
leq n$. Since $n$ is even, we have $2n$ such values of $k$. Also,
since $p$ has real coefficients, if $k$ is a root, so is $-k$.
Since $r(x)$ has degree $2n+2$, we can write
0.15625
Let $q(x) = x^{2n}p\left(\frac{1}{x}\right)$. Then $q(x) = 1 + a_{2n-1}
x + a_{2n-2}x^2 + \dots + a_1x^{2n-1} + a_0x^{2n}$.
0.03125
Thus, $r(x)$ is a polynomial of degree $2n+2$ with roots $k$ for $1 \
leq |k| \leq n$. Since there are $2n$ such values of $k$, we can
write
0.3125
We are given that $p(x)$ is a monic real polynomial of degree $2n$.
0.0625
82

<!-- Page 83 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Since $p\left(\frac{1}{k}\right) = k^2$ for all integers $k$ such that
$1 \leq |k| \leq n$, we have $q(k) = p\left(\frac{1}{k}\right) - k
^2 = 0$ for $1 \leq |k| \leq n$.
0.34375
Thus, $q(x)$ has $2n$ distinct roots $k$ where $1 \le |k| \le n$.
0.15625
Thus, $q(x)$ has roots $k$ for $1 \leq |k| \leq n$. These are $2n$
distinct integer roots. Since the numerator of $q(x)$ has degree
$2n+2$, we can write
0.34375
Thus, $q(x)$ has roots at $k = \pm 1, \pm 2, \ldots, \pm n$. These are
$2n$ distinct roots.
0.375
Since $p(1/k) = k^2$ for $1 \leq |k| \leq n$, we have $q(k) = 0$ for $1
\leq |k| \leq n$.
0.28125
Thus, $q(x)$ has roots $k$ for $1 \leq |k| \leq n$. Since $n$ is an
even integer, there are $2n$ such values of $k$. We can write
0.125
Continuing the solution:
0.40625
Since $p$ has degree $2n$, $p(1/x)$ has degree $2n$ in $\frac{1}{x}$,
so when we consider $p(1/x) = x^2$, the numerator of $q(x)$ has
degree $2n+2$. Thus, $q(x)$ has $2n$ zeros at $\pm 1, \pm 2, \dots,
\pm n$.
0.40625
Let $r(x) = 1 + a_{2n-1}x + \dots + a_1 x^{2n-1} + a_0 x^{2n} - x^{2n
+2}$.
0.34375
Let $r(x) = 1 + a_{2n-1}x + \dots + a_1 x^{2n-1} + a_0 x^{2n} - x^{2n
+2}$.
0.375
83

<!-- Page 84 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
$$q(x) = \frac{(x-1)(x+1) \dots (x-n)(x+n)(x-\alpha)(x-\beta)}{x^{2n}}
$$
0.1875
$$q(x) = \frac{1 + a_{2n-1}x + \dots + a_1 x^{2n-1} + a_0 x^{2n} - x^{2
n+2}}{x^{2n}} = \frac{-x^{2n+2} + a_0 x^{2n} + \dots + a_{2n-1}x +
1}{x^{2n}}.$$
0.375
Let $r(x) = 1 + a_{2n-1}x + \dots + a_1 x^{2n-1} + a_0 x^{2n} - x^{2n
+2}$. Then the roots of $r(x)$ are precisely the roots of $q(x)$.
Since $r(x)$ is a polynomial of degree $2n+2$, and we have found
$2n$ distinct roots, we can write
0.40625
Since $q(x) = \frac{r(x)}{x^{2n}}$, the roots of $q(x)$ are the roots
of $r(x)$. Thus, $r(x)$ has roots $k$ where $1 \le |k| \le n$.
0.28125
Let $r(x) = 1 + a_{2n-1}x + \dots + a_1 x^{2n-1} + a_0 x^{2n} - x^{2n
+2}$. Then the roots of $q(x)$ are the roots of $r(x)$. Since $r(x)
$ is a polynomial of degree $2n+2$, and we know $2n$ roots, we can
write
0.5
Since $q(x)$ has the form $\frac{\text{polynomial of degree } 2n+2}{x
^{2n}}$, the numerator of $q(x)$ must have degree $2n+2$.
0.375
$$r(x) = (x-1)(x+1)(x-2)(x+2)\cdots(x-n)(x+n)(Ax^2 + Bx + C)$$
0.6875
$$r(x) = (x-1)(x+1)(x-2)(x+2)\cdots(x-n)(x+n)(cx^2+bx+a) = (x^2-1)(x
^2-2^2)\cdots(x^2-n^2)(cx^2+bx+a).$$
0.5
$$r(x) = (x-1)(x+1)(x-2)(x+2)\cdots(x-n)(x+n)(cx^2 + bx + a) = \prod_{k
=1}^n (x^2-k^2) (cx^2+bx+a)$$
0.5625
Since $r(0) = 1$, we must have $(-1)^n (n!)^2 C = 1$, so $C = \frac
{1}{(n!)^2}$.
0.9375
84

<!-- Page 85 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Since $r(x) = 1 + a_{2n-1}x + \dots + a_0 x^{2n} - x^{2n+2}$, the
coefficient of $x^{2n+2}$ is $-1$. Also, since $r(x) = \prod_{k=1}^
n (x-k)(x+k) (Ax^2+Bx+C)$, the coefficient of $x^{2n+2}$ is $A$.
Thus, $A = -1$.
0.6875
The coefficient of $x^{2n+2}$ in $r(x)$ is $-1$. Also, the coefficient
of $x^{2n+2}$ in the expansion of $(x-1)(x+1)\cdots(x-n)(x+n)(Ax^2+
Bx+C)$ is $A$. Thus, $A = -1$.
0.875
Since $r(x) = 1 + a_{2n-1}x + \dots + a_0 x^{2n} - x^{2n+2}$, the
coefficient of $x^{2n+2}$ is $-1$.
0.53125
Continuing the solution:
0.6875
The constant term of $r(x)$ is $1$. The constant term of $\prod_{k=1}^n
(x-k)(x+k)(-x^2+Bx+C)$ is $(-1)^n (n!)^2 C$. Therefore, $(-1)^n (n
!)^2 C = 1$, so $C = \frac{1}{(-1)^n (n!)^2} = \frac{(-1)^n}{(n!)^2}
$.
0.65625
The constant term of $r(x)$ is $1$. The constant term in $\prod_{k=1}^n
(x-k)(x+k)(-x^2+Bx+C)$ is $(-1)^n (n!)^2 C$. So, $1 = (-1)^n (n!)
^2 C$, which implies $C = \frac{(-1)^n}{(n!)^2}$.
0.5625
The constant term of $r(x)$ is $1$.
0.5
The coefficient of $x^{2n}$ in $r(x)$ is $a_0$. The coefficient of $x
^{2n}$ in the factored form is $A - \sum_{k=1}^n k^2$. The
coefficient of $x^{2n+1}$ in $r(x)$ is $a_1$, while in the factored
form, the coefficient is $B$. The coefficient of $x^{2n+2}$ in $r(
x)$ is $-1$, and in the factored form, it is $A$. Therefore, $A =
-1$.
0.71875
Now, we examine the coefficient of $x^{2n+1}$. In $r(x) = 1 + a_{2n-1}x
+ \dots - x^{2n+2}$, the coefficient of $x^{2n+1}$ is zero.
1.0
85

<!-- Page 86 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
We have $r(x) = \prod_{k=1}^n (x^2-k^2) (-x^2+Bx+C)$. Since $r(0) = 1$,
we have $1 = \prod_{k=1}^n (-k^2) \cdot C$. Also, $C = \frac{(-1)^
n}{(n!)^2}$.
0.5
We have $r(x) = \prod_{k=1}^n (x^2-k^2)(-x^2+Bx+C)$.
0.5
In the expansion of $\prod_{k=1}^n (x^2-k^2) (-x^2+Bx+C)$, the
coefficient of $x^{2n+1}$ is $B \prod_{k=1}^n (-k^2) = B(-1)^n (n!)
^2$. Since this must equal 0, we have $B=0$.
1.0
In the expansion of $\prod_{k=1}^n (x^2 - k^2)(-x^2 + Bx + C)$, the
coefficient of $x^{2n+1}$ is $B \prod_{k=1}^n (-k^2) = B(-1)^n (n!)
^2 = (-1)^n (n!)^2 B$. Since this must be zero, we must have $B = 0
$.
1.0
So, we have $r(x) = \prod_{k=1}^n (x^2-k^2) (-x^2 + \frac{(-1)^n}{(n!)
^2})$.
1.0
In the expression $r(x) = \prod_{k=1}^n (x^2 - k^2) (-x^2 + Bx + C)$,
the coefficient of $x^{2n+1}$ is $B$. Thus, $B=0$.
0.96875
Since $n$ is even, let $n=2m$. The coefficient of $x^{2n+1}$ in $r(x)$
is $0$. Also, the coefficient of $x^{2n+1}$ in $\prod_{k=1}^{2m} (xk)(x+k)(-x^2+Bx+C)$ is $B \prod_{k=1}^{2m} k^2 = B (n!)^2 = B((2m)
!)^2$.
1.0
Since $n$ is even, let $n=2m$. Then the roots are $\pm 1, \pm 2, \ldots
, \pm 2m$.
0.75
So, $1 = (-1)^n (n!)^2 C$. Since $C = \frac{(-1)^n}{(n!)^2}$, the
equation holds. The coefficient of $x$ in $r(x)$ is $a_{2n-1}$.

### Since $r(x) = \prod_{k=1}^n (x^2 - k^2) (-x^2+Bx+C)$, the

coefficient of $x$ is $C$ times the coefficient of $x$ in $\prod_{k
=1}^n (x^2 - k^2)$, which is 0. Thus, the coefficient of $x$ in $r(
x)$ is 0, so $a_{2n-1} = 0$.
0.71875
86

<!-- Page 87 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
So, $B((2m)!)^2 = 0$, which implies $B=0$.
1.0
We also know that the coefficient of $x^{2n+1}$ in $r(x)$ is $0$. The
coefficient of $x^{2n+1}$ in $\prod_{k=1}^n (x^2-k^2) (-x^2+Bx+C)$
is $B$. Thus $B=0$.
1.0
So, $B((2m)!)^2 = 0$, which implies $B=0$.
1.0
We have $r(x) = \prod_{k=1}^{n}(x^2-k^2)(-x^2+Bx+C)$.
0.84375
So we have $r(x) = \prod_{k=1}^n (x^2-k^2) (-x^2 + \frac{(-1)^n}{(n!)
^2})$.
1.0
Thus, $B((2m)!)^2 = 0$, which means $B = 0$. Therefore,
1.0
Since $n$ is even, $C = \frac{1}{(n!)^2}$. Also, $A = -1$. So,
0.71875
The constant term of $r(x)$ is $1$. Also, the constant term is given by
$\prod_{k=1}^n (-k)(k) \cdot C = (-1)^n (n!)^2 C$. Thus, $(-1)^n (
n!)^2 C = 1$, which implies $C = \frac{1}{(-1)^n (n!)^2} = \frac
{(-1)^n}{(n!)^2}$. Since $n$ is even, $C = \frac{1}{(n!)^2}$.
0.71875
So, we have $r(x) = \prod_{k=1}^n (x^2-k^2) (-x^2 + \frac{(-1)^n}{(n!)
^2})$.
1.0
So we have $r(x) = \prod_{k=1}^{n} (x^2 - k^2)(-x^2 + \frac{(-1)^n}{(n
!)^2})$.
1.0
So we have $r(x) = \prod_{k=1}^{n}(x^2-k^2)(-x^2 + \frac{(-1)^n}{(n!)
^2})$.
0.96875
87

<!-- Page 88 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
So, we have $r(x) = \prod_{k=1}^n (x^2 - k^2)(-x^2 + \frac{(-1)^n}{(n!)
^2})$. Thus
1.0
We have $r(x) = \prod_{k=1}^n (x^2-k^2)(-x^2+Bx+C)$.
0.8125
So we have $r(x) = \prod_{k=1}^n (x^2-k^2) (-x^2 + \frac{(-1)^n}{(n!)
^2})$.
1.0
Thus, we have $r(x) = \prod_{k=1}^n (x^2-k^2)(-x^2 + \frac{(-1)^n}{(n!)
^2})$. Since $n$ is even, let $n=2m$.
1.0
We have $r(x) = \prod_{k=1}^n (x^2-k^2) (-x^2 + \frac{(-1)^n}{(n!)^2})$
. The roots of $r(x)$ other than $\pm 1, \pm 2, ..., \pm n$ satisfy
$-x^2 + \frac{(-1)^n}{(n!)^2} = 0$, so $x^2 = \frac{(-1)^n}{(n!)^2}
$.
1.0
$$r(x) = \prod_{k=1}^n (x^2 - k^2) (-x^2 + \frac{1}{(n!)^2}) = -x^2 \
prod_{k=1}^n (x^2 - k^2) + \frac{1}{(n!)^2} \prod_{k=1}^n (x^2 - k
^2).$$
1.0
So, $r(x) = \prod_{k=1}^n (x^2-k^2) (-x^2 + \frac{(-1)^n}{(n!)^2})$. We
are looking for the roots of $r(x) = 0$, other than $\pm 1, \pm 2,
\ldots, \pm n$. These must be the roots of $-x^2 + \frac{(-1)^n}{(
n!)^2} = 0$, which gives us $x^2 = \frac{(-1)^n}{(n!)^2}$. Since
$n$ is an even integer, we have $x^2 = \frac{1}{(n!)^2}$, so \boxed
{x = \pm \frac{1}{n!}}.
1.0

### Solution:

Let ğ‘(ğ‘¥) = ğ‘ (ï¸€1)ï¸€ âˆ’ğ‘¥2. Then ğ‘(ğ‘¥) is a rational function of the form
ğ‘¥
1+ğ‘ ğ‘¥+Â·Â·Â·+ğ‘ ğ‘¥2ğ‘›âˆ’1+ğ‘ ğ‘¥2ğ‘›âˆ’ğ‘¥2ğ‘›+2
2ğ‘›âˆ’1 1 0
ğ‘(ğ‘¥) = .
ğ‘¥2ğ‘›
Since ğ‘ (ï¸€1)ï¸€ = ğ‘˜2 for all integers ğ‘˜ such that 1 â‰¤ |ğ‘˜| â‰¤ ğ‘›, we have ğ‘(ğ‘˜) = ğ‘ (ï¸€1)ï¸€ âˆ’ğ‘˜2 = 0
ğ‘˜ ğ‘˜
for 1 â‰¤ |ğ‘˜| â‰¤ ğ‘›.
Thus, ğ‘(ğ‘¥) has roots at ğ‘˜ = Â±1,Â±2,...,Â±ğ‘›. These are 2ğ‘› distinct roots.
Let ğ‘Ÿ(ğ‘¥) = 1 + ğ‘ ğ‘¥ + Â·Â·Â· + ğ‘ ğ‘¥2ğ‘›âˆ’1 + ğ‘ ğ‘¥2ğ‘› âˆ’ ğ‘¥2ğ‘›+2. Then the roots of ğ‘Ÿ(ğ‘¥) are
2ğ‘›âˆ’1 1 0
precisely the roots of ğ‘(ğ‘¥). Since ğ‘Ÿ(ğ‘¥) is a polynomial of degree 2ğ‘›+2, and we have
found 2ğ‘› distinct roots, we can write
88

<!-- Page 89 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
ğ‘Ÿ(ğ‘¥) = (ğ‘¥âˆ’1)(ğ‘¥+1)(ğ‘¥âˆ’2)(ğ‘¥+2)Â·Â·Â·(ğ‘¥âˆ’ğ‘›)(ğ‘¥+ğ‘›)(ğ´ğ‘¥2+ğµğ‘¥+ğ¶)
Sinceğ‘Ÿ(ğ‘¥) = 1+ğ‘ ğ‘¥+Â·Â·Â·+ğ‘ ğ‘¥2ğ‘›âˆ’ğ‘¥2ğ‘›+2,thecoefficientofğ‘¥2ğ‘›+2 isâˆ’1. Also,since
2ğ‘›âˆ’1 0
ğ‘Ÿ(ğ‘¥) = âˆï¸€ğ‘› (ğ‘¥âˆ’ğ‘˜)(ğ‘¥+ğ‘˜)(ğ´ğ‘¥2+ğµğ‘¥+ğ¶),thecoefficientofğ‘¥2ğ‘›+2 isğ´. Thus,ğ´ = âˆ’1.
ğ‘˜=1
The coefficient of ğ‘¥2ğ‘›+2 in ğ‘Ÿ(ğ‘¥) is âˆ’1. Also, the coefficient of ğ‘¥2ğ‘›+2 in the expansion of
(ğ‘¥âˆ’1)(ğ‘¥+1)Â·Â·Â·(ğ‘¥âˆ’ğ‘›)(ğ‘¥+ğ‘›)(ğ´ğ‘¥2+ğµğ‘¥+ğ¶) is ğ´. Thus, ğ´ = âˆ’1.
Theconstanttermofğ‘Ÿ(ğ‘¥)is1. Theconstanttermofâˆï¸€ğ‘› (ğ‘¥âˆ’ğ‘˜)(ğ‘¥+ğ‘˜)(âˆ’ğ‘¥2+ğµğ‘¥+ğ¶)
ğ‘˜=1
is (âˆ’1)ğ‘›(ğ‘›!)2ğ¶. Therefore, (âˆ’1)ğ‘›(ğ‘›!)2ğ¶ = 1, so ğ¶ = 1 = (âˆ’1)ğ‘›.
(âˆ’1)ğ‘›(ğ‘›!)2 (ğ‘›!)2
We also know that the coefficient of ğ‘¥2ğ‘›+1 in ğ‘Ÿ(ğ‘¥) is 0. The coefficient of ğ‘¥2ğ‘›+1 in
âˆï¸€ğ‘› (ğ‘¥2âˆ’ğ‘˜2)(âˆ’ğ‘¥2+ğµğ‘¥+ğ¶) is ğµ. Thus ğµ = 0.
ğ‘˜=1
So, ğ‘Ÿ(ğ‘¥) = âˆï¸€ğ‘› (ğ‘¥2 âˆ’ ğ‘˜2)(âˆ’ğ‘¥2 + (âˆ’1)ğ‘› ). We are looking for the roots of ğ‘Ÿ(ğ‘¥) = 0,
ğ‘˜=1 (ğ‘›!)2
other than Â±1,Â±2,...,Â±ğ‘›. These must be the roots of âˆ’ğ‘¥2+ (âˆ’1)ğ‘› = 0, which gives us
(ğ‘›!)2
1
ğ‘¥2 = (âˆ’1)ğ‘›. Since ğ‘› is an even integer, we have ğ‘¥2 = 1 , so ğ‘¥ = Â± .
(ğ‘›!)2 (ğ‘›!)2 ğ‘›!
89

<!-- Page 90 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
DeepSeek R1 Solution Trace

### Problem

Let ğ‘› be an even positive integer. Let ğ‘ be a monic, real polynomial of degree 2ğ‘›;
that is to say,ğ‘(ğ‘¥) = ğ‘¥2ğ‘›+ğ‘ ğ‘¥2ğ‘›âˆ’1+Â·Â·Â·+ğ‘ ğ‘¥+ğ‘
2ğ‘›âˆ’1 1 0
for some real coefficients ğ‘ ,...,ğ‘ .
0 2ğ‘›âˆ’1
Suppose that ğ‘ (ï¸€1)ï¸€ = ğ‘˜2 for all integers ğ‘˜ such that 1 â‰¤ |ğ‘˜| â‰¤ ğ‘›.
ğ‘˜
Find all other real numbers ğ‘¥ for which ğ‘ (ï¸€1)ï¸€ = ğ‘¥2.
ğ‘¥

### Solution

So i have this problem here. letâ€™s see what itâ€™s about. we have an even positive integer ğ‘›,
and a monic real polynomial ğ‘(ğ‘¥) of degree 2n, which means it looks like this:
ğ‘(ğ‘¥) = ğ‘¥2ğ‘›+ğ‘ ğ‘¥2ğ‘›âˆ’1+Â·Â·Â·+ğ‘ ğ‘¥+ğ‘
2ğ‘›âˆ’1 1 0
for some real coefficients ğ‘ ,ğ‘ ,...,ğ‘
0 1 2ğ‘›âˆ’1
the condition given is that ğ‘(1) = ğ‘˜2 for all integers k such that 1 â‰¤ |ğ‘˜| â‰¤ ğ‘›. so, k
ğ‘˜
takes integer values from -n to -1 and from 1 to n, excluding zero because 1 would be
ğ‘˜
undefined.
i need to find all other real numbers for which ğ‘(1) = ğ‘¥2
ğ‘¥
okay, so first, i need to understand what is being asked. we have a polynomial of degree
2n, and it satisfies the equation ğ‘(1) = ğ‘˜2 for these specific values of k. i need to find
ğ‘˜
all real x (other than the ones already specified, i assume) that satisfy ğ‘(1) = ğ‘¥2. one
ğ‘¥
thing that comes to mind is that since ğ‘(ğ‘¥) is a polynomial of degree 2n, and we have
2n conditions (because k ranges from -n to ğ‘›, excluding zero, which gives 2n points),
this should determine the polynomial uniquely. that is, there should be only one such
polynomial ğ‘(ğ‘¥) that satisfies these conditions.
butthequestionisnotaskingforğ‘(ğ‘¥);itâ€™saskingforallrealnumberssuchthatğ‘(1) = ğ‘¥2
ğ‘¥
given that ğ‘(1) = ğ‘˜2 for those specific k.
ğ‘˜
so, perhaps i should consider the equation ğ‘ (ï¸€1)ï¸€ âˆ’ğ‘¥2 = 0 and see for which real ğ‘¥ this
ğ‘¥
holds, beyond the already given ğ‘¥ = ğ‘˜ for 1 â‰¤ |ğ‘˜| â‰¤ ğ‘›.
but wait, actually, in the given condition, itâ€™s ğ‘ (ï¸€1)ï¸€ = ğ‘˜2, which can be rewritten as
ğ‘˜
ğ‘ (ï¸€1)ï¸€ âˆ’ (ï¸€1)ï¸€âˆ’2 = 0, since ğ‘¥2 = (ï¸€1)ï¸€âˆ’2 when ğ‘¥ = 1. hmm, not sure if that helps directly.
ğ‘˜ ğ‘˜ ğ‘˜ ğ‘˜
(ï¸‚
1
)ï¸‚ (ï¸‚
1
)ï¸‚âˆ’2
ğ‘ âˆ’ = 0,
ğ‘˜ ğ‘˜
let me try to think differently. since ğ‘(ğ‘¥) is a polynomial of degree 2ğ‘›, and i have 2ğ‘›
points where ğ‘ (ï¸€1)ï¸€ = ğ‘˜2, perhaps i can construct another polynomial that captures the
ğ‘˜
difference between ğ‘ (ï¸€1)ï¸€ and ğ‘¥2, and see its roots.
ğ‘¥
letâ€™s define a new function:
(ï¸‚ )ï¸‚
1
ğ‘(ğ‘¥) = ğ‘ âˆ’ğ‘¥2
ğ‘¥
i need to find the real roots of ğ‘(ğ‘¥), i.e., the real ğ‘¥ such that ğ‘(ğ‘¥) = 0.
giventhatğ‘ (ï¸€1)ï¸€ = ğ‘˜2for1 â‰¤ |ğ‘˜| â‰¤ ğ‘›,thatmeansthatforeachsuchğ‘˜,ğ‘(ğ‘˜) = ğ‘ (ï¸€1)ï¸€ âˆ’ğ‘˜2 =
ğ‘˜ ğ‘˜
0. so, ğ‘˜ are roots of ğ‘(ğ‘¥).
90

<!-- Page 91 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
butğ‘(ğ‘¥)isğ‘ (ï¸€1)ï¸€ âˆ’ğ‘¥2,andğ‘(ğ‘¥)isadegree2ğ‘›polynomial,soğ‘ (ï¸€1)ï¸€isarationalfunction,
ğ‘¥ ğ‘¥
not a polynomial, unless we multiply by ğ‘¥2ğ‘› to clear denominators.
wait, maybe i should consider multiplying ğ‘(ğ‘¥) by ğ‘¥2ğ‘› to make it a polynomial.
so, perhaps i should consider the equation ğ‘ (ï¸€1)ï¸€ âˆ’ğ‘¥2 = 0 and see for which real ğ‘¥ this
ğ‘¥
holds, beyond the already given ğ‘¥ = ğ‘˜ for 1 â‰¤ |ğ‘˜| â‰¤ ğ‘›.
but wait, actually, in the given condition, itâ€™s ğ‘ (ï¸€1)ï¸€ = ğ‘˜2, which can be rewritten as
ğ‘˜
ğ‘ (ï¸€1)ï¸€ âˆ’ (ï¸€1)ï¸€âˆ’2 = 0, since ğ‘¥2 = (ï¸€1)ï¸€âˆ’2 when ğ‘¥ = 1. hmm, not sure if that helps directly.
ğ‘˜ ğ‘˜ ğ‘˜ ğ‘˜
(ï¸‚
1
)ï¸‚ (ï¸‚
1
)ï¸‚âˆ’2
ğ‘ âˆ’ = 0,
ğ‘˜ ğ‘˜
let me try to think differently. since ğ‘(ğ‘¥) is a polynomial of degree 2ğ‘›, and i have 2ğ‘›
points where ğ‘ (ï¸€1)ï¸€ = ğ‘˜2, perhaps i can construct another polynomial that captures the
ğ‘˜
difference between ğ‘ (ï¸€1)ï¸€ and ğ‘¥2, and see its roots.
ğ‘¥
letâ€™s define a new function:
(ï¸‚ )ï¸‚
1
ğ‘(ğ‘¥) = ğ‘ âˆ’ğ‘¥2
ğ‘¥
i need to find the real roots of ğ‘(ğ‘¥), i.e., the real ğ‘¥ such that ğ‘(ğ‘¥) = 0.
giventhatğ‘ (ï¸€1)ï¸€ = ğ‘˜2for1 â‰¤ |ğ‘˜| â‰¤ ğ‘›,thatmeansthatforeachsuchğ‘˜,ğ‘(ğ‘˜) = ğ‘ (ï¸€1)ï¸€ âˆ’ğ‘˜2 =
ğ‘˜ ğ‘˜
0. so, ğ‘˜ are roots of ğ‘(ğ‘¥).
butğ‘(ğ‘¥)isğ‘ (ï¸€1)ï¸€ âˆ’ğ‘¥2,andğ‘(ğ‘¥)isadegree2ğ‘›polynomial,soğ‘ (ï¸€1)ï¸€isarationalfunction,
ğ‘¥ ğ‘¥
not a polynomial, unless we multiply by ğ‘¥2ğ‘› to clear denominators.
so,ğ‘¥2ğ‘›ğ‘(ğ‘¥) = ğ‘¥2ğ‘›ğ‘ (ï¸€1)ï¸€ âˆ’ğ‘¥2ğ‘›+2 = (ï¸€ 1+ğ‘ ğ‘¥+Â·Â·Â·+ğ‘ ğ‘¥2ğ‘›)ï¸€ âˆ’ğ‘¥2ğ‘›+2,whichisadegree
ğ‘¥ 2ğ‘›âˆ’1 0
2ğ‘›+2 polynomial if ğ‘¥2ğ‘›+2 is the highest degree term.
wait, but ğ‘¥2ğ‘›+2 is degree 2ğ‘›+2 and the other terms are up to degree 2ğ‘›, so yes, the
highest degree is 2ğ‘›+2.
now,therootsofğ‘(ğ‘¥)aretherootsofğ‘ (ï¸€1)ï¸€ âˆ’ğ‘¥2 = 0,whicharetheğ‘¥forwhichğ‘ (ï¸€1)ï¸€ = ğ‘¥2.
ğ‘¥ ğ‘¥
we know that for ğ‘¥ = ğ‘˜ where 1 â‰¤ |ğ‘˜| â‰¤ ğ‘›, this holds, so these ğ‘¥ are roots of ğ‘(ğ‘¥).
but ğ‘(ğ‘¥) is a rational function, while ğ‘¥2ğ‘›ğ‘(ğ‘¥) is a polynomial of degree 2ğ‘›+2, which
has roots at ğ‘¥ = ğ‘˜ for 1 â‰¤ |ğ‘˜| â‰¤ ğ‘›, assuming that ğ‘¥2ğ‘›ğ‘(ğ‘¥) has these roots.
wait, actually, since ğ‘(ğ‘˜) = 0, then ğ‘¥2ğ‘›ğ‘(ğ‘¥) would have roots at ğ‘¥ = ğ‘˜, but ğ‘¥2ğ‘›ğ‘(ğ‘¥) is a
polynomial of degree 2ğ‘›+2, so it should have 2ğ‘›+2 roots (counting multiplicities) in
the complex plane, but we are interested in real roots.
but perhaps there is a better approach.
let me consider defining a new polynomial that incorporates the given conditions.
given that ğ‘ (ï¸€1)ï¸€ = ğ‘˜2 for 1 â‰¤ |ğ‘˜| â‰¤ ğ‘›, and ğ‘(ğ‘¥) is a monic polynomial of degree 2ğ‘›,
ğ‘˜
perhaps i can express ğ‘(ğ‘¥)âˆ’ 1 in terms of its roots.
ğ‘¥2
wait, but ğ‘(ğ‘¥) is defined for all real ğ‘¥, and 1 is also defined for ğ‘¥ Ì¸= 0.
ğ‘¥2
alternatively, maybe i can consider the function ğ‘(ğ‘¥)âˆ’ 1 , but iâ€™m not sure if that helps
ğ‘¥2
directly.
let me try to rearrange the given condition.
given ğ‘ (ï¸€1)ï¸€ = ğ‘˜2, i can write:
ğ‘˜
(ï¸‚ )ï¸‚
1
ğ‘ âˆ’ğ‘˜2 = 0
ğ‘˜
for ğ‘˜ = Â±1,Â±2,...,Â±ğ‘›. so, there are 2ğ‘› such equations.
sinceğ‘(ğ‘¥)isamonicpolynomialofdegree2ğ‘›,these2ğ‘›equationsshoulddetermineğ‘(ğ‘¥)
uniquely.
91

<!-- Page 92 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
but i need to find ğ‘¥ such that ğ‘ (ï¸€1)ï¸€ = ğ‘¥2, which is similar to the condition given, but for
ğ‘¥
arbitrary real ğ‘¥, not just integer ğ‘˜.
so,perhapsicanconsidertheequationğ‘ (ï¸€1)ï¸€ âˆ’ğ‘¥2 = 0,andfinditsrealroots,giventhat
ğ‘¥
it already has roots at ğ‘¥ = ğ‘˜ for 1 â‰¤ |ğ‘˜| â‰¤ ğ‘›.
but to find the other roots, i need to know more about this equation.
alternatively, maybe i can consider the polynomial ğ‘(ğ‘¥) = ğ‘(ğ‘¥)âˆ’ 1 , but again, that
ğ‘¥2
seems messy because of the 1 term.
ğ‘¥2
wait,perhapsishouldconsidermultiplyingbothsidesoftheequationbyğ‘¥2 toeliminate
the denominator.
so, starting from ğ‘ (ï¸€1)ï¸€ = ğ‘¥2, multiply both sides by ğ‘¥2 :
ğ‘¥
(ï¸‚ )ï¸‚
1
ğ‘¥2ğ‘ = ğ‘¥4
ğ‘¥
now, ğ‘¥2ğ‘ (ï¸€1)ï¸€ can be expressed in terms of ğ‘(ğ‘¥), but iâ€™m not sure.
ğ‘¥
alternatively, since ğ‘(ğ‘¥) is a polynomial, ğ‘ (ï¸€1)ï¸€ is a rational function, and multiplying by
ğ‘¥
ğ‘¥2ğ‘› gives a polynomial.
let me consider that again.
define ğ‘(ğ‘¥) = ğ‘¥2ğ‘›ğ‘ (ï¸€1)ï¸€ âˆ’ğ‘¥2ğ‘›+2, which is a polynomial of degree 2ğ‘›+2, as previously
ğ‘¥
established.
weknowthatğ‘(ğ‘˜) = 0forğ‘˜ = Â±1,Â±2,...,Â±ğ‘›,becauseğ‘ (ï¸€1)ï¸€ = ğ‘˜2,soğ‘(ğ‘˜) = ğ‘˜2ğ‘›ğ‘ (ï¸€1)ï¸€ âˆ’
ğ‘˜ ğ‘˜
ğ‘˜2ğ‘›+2 = ğ‘˜2ğ‘›Â·ğ‘˜2âˆ’ğ‘˜2ğ‘›+2 = ğ‘˜2ğ‘›+2âˆ’ğ‘˜2ğ‘›+2 = 0.
therefore, ğ‘(ğ‘¥) has 2ğ‘› roots at ğ‘¥ = Â±1,Â±2,...,Â±ğ‘›.
since ğ‘(ğ‘¥) is a degree 2ğ‘›+2 polynomial, there are two more roots beyond these 2ğ‘›
roots.
so, ğ‘(ğ‘¥) can be factored as:
ğ‘(ğ‘¥) = (ğ‘¥2âˆ’1)(ğ‘¥2âˆ’4)Â·Â·Â·(ğ‘¥2âˆ’ğ‘›2)Â·(ğ‘¥2âˆ’ğ‘)
for some constant ğ‘, because the roots come in pairs ğ‘¥ = ğ‘˜ and ğ‘¥ = âˆ’ğ‘˜, and possibly
âˆš âˆš
another pair ğ‘¥ = ğ‘ and ğ‘¥ = âˆ’ ğ‘.
wait, but ğ‘(ğ‘¥) is degree 2ğ‘›+2, and (ğ‘¥2 âˆ’ğ‘˜2) for ğ‘˜ = 1 to ğ‘› gives ğ‘› quadratic factors,
which is degree 2ğ‘›. so, there are two more roots to account for.
alternatively, perhaps ğ‘(ğ‘¥) = (ğ‘¥2 âˆ’1)(ğ‘¥2 âˆ’4)Â·Â·Â·(ğ‘¥2 âˆ’ğ‘›2)Â·(ğ‘¥2 âˆ’ğ‘)(ğ‘¥2 âˆ’ğ‘), where ğ‘
and ğ‘ are the remaining roots.
but, to determine ğ‘ and ğ‘, i need more information about ğ‘(ğ‘¥).
alternatively, perhaps i can look at the leading coefficient.
since ğ‘(ğ‘¥) = ğ‘¥2ğ‘›ğ‘ (ï¸€1)ï¸€ âˆ’ ğ‘¥2ğ‘›+2, and ğ‘ (ï¸€1)ï¸€ = ğ‘¥âˆ’2ğ‘› + ğ‘ ğ‘¥âˆ’2ğ‘›+1 + Â·Â·Â· + ğ‘ , then
ğ‘¥ ğ‘¥ 2ğ‘›âˆ’1 0
ğ‘¥2ğ‘›ğ‘ (ï¸€1)ï¸€ = 1+ ğ‘ ğ‘¥+Â·Â·Â·+ğ‘ ğ‘¥2ğ‘›, so ğ‘(ğ‘¥) = 1+ğ‘ ğ‘¥+Â·Â·Â·+ğ‘ ğ‘¥2ğ‘›âˆ’ğ‘¥2ğ‘›+2.
ğ‘¥ 2ğ‘›âˆ’1 0 2ğ‘›âˆ’1 0
the leading term is âˆ’ğ‘¥2ğ‘›+2+ğ‘ ğ‘¥2ğ‘› but since ğ‘(ğ‘¥) is monic, ğ‘ is just a constant.
0 0
wait, perhaps thereâ€™s a better approach. maybe i can consider the reciprocal polynomial
or something like that.
alternatively, since ğ‘(ğ‘¥) is a polynomial of degree 2ğ‘›, and i have ğ‘ (ï¸€1)ï¸€ = ğ‘˜2 for ğ‘˜ =
ğ‘˜
Â±1,Â±2,...,Â±ğ‘›, perhaps i can consider defining a new polynomial that has roots at
ğ‘¥ = 1, but that seems off-track.
ğ‘˜
let me try to think about the problem differently.
suppose i define a new variable ğ‘¦ = 1, then, the equation ğ‘ (ï¸€1)ï¸€ = ğ‘¥2 becomes ğ‘(ğ‘¦) =
ğ‘¥ ğ‘¥
(ï¸ )ï¸2
1 = 1 , assuming ğ‘¦ Ì¸= 0.
ğ‘¦ ğ‘¦2
so, ğ‘(ğ‘¦) = 1 , or ğ‘(ğ‘¦)ğ‘¦2 = 1.
ğ‘¦2
92

<!-- Page 93 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
so, ğ‘(ğ‘¦)ğ‘¦2âˆ’1 = 0.
now, ğ‘(ğ‘¦)ğ‘¦2âˆ’1 is a polynomial of degree 2ğ‘›+2, since ğ‘(ğ‘¦) is degree 2ğ‘› and multiplied
by ğ‘¦2.
given that ğ‘ (ï¸€1)ï¸€ = ğ‘˜2, for ğ‘˜ = Â±1,Â±2,...,Â±ğ‘›, that means ğ‘(ğ‘¦) = 1 at ğ‘¦ = 1, so
ğ‘˜ ğ‘¦2 ğ‘˜
ğ‘(ğ‘¦)ğ‘¦2âˆ’1 = 0 at ğ‘¦ = 1.
ğ‘˜
therefore, ğ‘¦ = 1 for ğ‘˜ = Â±1,Â±2,...,Â±ğ‘› are roots of ğ‘(ğ‘¦)ğ‘¦2âˆ’1 = 0.
ğ‘˜
butğ‘(ğ‘¦)ğ‘¦2âˆ’1isadegree2ğ‘›+2polynomial,sothereare2ğ‘›+2rootsintotal(counting
multiplicities).
we already have 2ğ‘› roots at ğ‘¦ = 1 for ğ‘˜ = Â±1,Â±2,...,Â±ğ‘›, so there are two more roots
ğ‘˜
that i need to find.
let me denote these additional roots as ğ‘¦ = ğ›¼ and ğ‘¦ = ğ›½, so:
(ï¸‚ )ï¸‚(ï¸‚ )ï¸‚ (ï¸‚ )ï¸‚(ï¸‚ )ï¸‚
1 1 1 1
ğ‘(ğ‘¦)ğ‘¦2âˆ’1 = ğ‘¦âˆ’ ğ‘¦+ Â·Â·Â· ğ‘¦âˆ’ ğ‘¦+ (ğ‘¦âˆ’ğ›¼)(ğ‘¦âˆ’ğ›½)
1 1 ğ‘› ğ‘›
butsinceğ‘(ğ‘¦)ğ‘¦2âˆ’1isapolynomialofdegree2ğ‘›+2,andwehave2ğ‘›rootsfromğ‘¦ = Â±1,
ğ‘˜
there must be two more roots, say ğ‘¦ = ğ›¼ and ğ‘¦ = ğ›½.
now, to find ğ›¼ and ğ›½, perhaps i can look at the behavior of ğ‘(ğ‘¦)ğ‘¦2âˆ’1 as ğ‘¦ approaches
infinity.
theleadingtermofğ‘(ğ‘¦)ğ‘¦2 isğ‘¦2ğ‘›Â·ğ‘¦2 = ğ‘¦2ğ‘›+2,andtheconstanttermisâˆ’1,soasğ‘¦ â†’ âˆ,
ğ‘(ğ‘¦)ğ‘¦2âˆ’1 â‰ˆ ğ‘¦2ğ‘›+2 , which goes to infinity.
similarly, as ğ‘¦ â†’ 0, ğ‘(ğ‘¦)ğ‘¦2âˆ’1 â‰ˆ ğ‘ ğ‘¦2âˆ’1, which approaches âˆ’1.
0
but iâ€™m not sure if that helps directly.
alternatively, perhaps i can consider that ğ‘(ğ‘¦)ğ‘¦2âˆ’1 has a double root at ğ‘¦ = 0, but that
seems unlikely.
wait, letâ€™s consider the multiplicity of roots.
given that ğ‘(ğ‘¦)ğ‘¦2âˆ’1 = 0 has roots at ğ‘¦ = Â±1 for ğ‘˜ = 1 to ğ‘›, and two additional roots
ğ‘˜
ğ‘¦ = ğ›¼ and ğ‘¦ = ğ›½, i need to determine what ğ›¼ and ğ›½ are.
alternatively, perhaps i can consider expressing ğ‘(ğ‘¦)ğ‘¦2âˆ’1 in terms of its roots.
but that seems complicated.
maybe thereâ€™s a better way to approach this problem.
letmetrytoconsiderthefunctionğ‘ (ï¸€1)ï¸€ âˆ’ğ‘¥2 andseeificanfindarelationshipbetween
ğ‘¥
ğ‘¥ and 1.
ğ‘¥
alternatively, perhaps i can consider substituting ğ‘¥ = 1 into the equation ğ‘ (ï¸€1)ï¸€ = ğ‘¥2,
ğ‘˜ ğ‘¥
but that seems to just give back the original condition.
wait,perhapsicanconsiderthefactthatğ‘(ğ‘¥)isapolynomial,andconsideritsreciprocal
polynomial.
the reciprocal polynomial of ğ‘(ğ‘¥) is ğ‘¥2ğ‘›ğ‘ (ï¸€1)ï¸€ , which is indeed a polynomial, as weâ€™ve
ğ‘¥
seen earlier.
let me denote this reciprocal polynomial as ğ‘Ÿ(ğ‘¥) = ğ‘¥2ğ‘›ğ‘ (ï¸€1)ï¸€ , which is a degree 2ğ‘›
ğ‘¥
polynomial.
butearlier,ihadğ‘(ğ‘¥) = ğ‘ (ï¸€1)ï¸€ âˆ’ğ‘¥2,andğ‘¥2ğ‘›ğ‘(ğ‘¥) = ğ‘Ÿ(ğ‘¥)âˆ’ğ‘¥2ğ‘›+2,whichisadegree2ğ‘›+2
ğ‘¥
polynomial.
but perhaps focusing on ğ‘Ÿ(ğ‘¥) directly might help.
given that ğ‘ (ï¸€1)ï¸€ = ğ‘˜2, then ğ‘Ÿ(ğ‘˜) = ğ‘˜2ğ‘›ğ‘ (ï¸€1)ï¸€ = ğ‘˜2ğ‘›Â·ğ‘˜2 = ğ‘˜2ğ‘›+2.
ğ‘˜ ğ‘˜
wait, but earlier i had ğ‘(ğ‘˜) = 0, which led to ğ‘Ÿ(ğ‘˜)âˆ’ğ‘˜2ğ‘›+2 = 0, so ğ‘Ÿ(ğ‘˜) = ğ‘˜2ğ‘›+2.
therefore, ğ‘Ÿ(ğ‘˜) âˆ’ ğ‘˜2ğ‘›+2 = 0, meaning that ğ‘Ÿ(ğ‘¥) âˆ’ ğ‘¥2ğ‘›+2 has roots at ğ‘¥ = ğ‘˜ for ğ‘˜ =
Â±1,Â±2,...,Â±ğ‘›.
93

<!-- Page 94 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
but ğ‘Ÿ(ğ‘¥) is a degree 2ğ‘› polynomial, and ğ‘¥2ğ‘›+2 is a monomial of degree 2ğ‘› + 2, so
ğ‘Ÿ(ğ‘¥)âˆ’ğ‘¥2ğ‘›+2 is a degree 2ğ‘›+2 polynomial with 2ğ‘› roots at ğ‘¥ = Â±1,Â±2,...,Â±ğ‘›, and
two additional roots.
wait, but ğ‘Ÿ(ğ‘¥) is degree 2ğ‘›, and ğ‘¥2ğ‘›+2 is degree 2ğ‘›+2, so their difference is degree
2ğ‘›+2, as previously established.
butifğ‘Ÿ(ğ‘¥)âˆ’ğ‘¥2ğ‘›+2has2ğ‘›rootsatğ‘¥ = Â±1,Â±2,...,Â±ğ‘›,anditâ€™sadegree2ğ‘›+2polynomial,
then there are two more roots.
now, perhaps i can factor ğ‘Ÿ(ğ‘¥)âˆ’ğ‘¥2ğ‘›+2 as:
ğ‘Ÿ(ğ‘¥)âˆ’ğ‘¥2ğ‘›+2 = (ğ‘¥2âˆ’1)(ğ‘¥2âˆ’4)Â·Â·Â·(ğ‘¥2âˆ’ğ‘›2)Â·(ğ‘¥2âˆ’ğ‘)
for some constant ğ‘, since the roots come in pairs ğ‘¥ = Â±ğ‘˜ for ğ‘˜ = 1 to ğ‘›, and possibly
âˆš
another pair ğ‘¥ = Â± ğ‘.
but i need to find ğ‘.
alternatively, perhaps i can look at the behavior of ğ‘Ÿ(ğ‘¥)âˆ’ğ‘¥2ğ‘›+2 at specific points to
determine ğ‘.
but this seems complicated. maybe thereâ€™s a different approach.
let me consider that ğ‘(ğ‘¥) is a monic polynomial of degree 2ğ‘›, and ğ‘ (ï¸€1)ï¸€ = ğ‘˜2 for
ğ‘˜
ğ‘˜ = Â±1,Â±2,...,Â±ğ‘›.
perhapsicanconsiderthepolynomialğ‘(ğ‘¥)âˆ’ 1 ,butasbefore,thatintroducesarational
ğ‘¥2
function.
alternatively, maybe i can consider building ğ‘(ğ‘¥) explicitly.
since ğ‘(ğ‘¥) is monic of degree 2ğ‘›, and it satisfies ğ‘ (ï¸€1)ï¸€ = ğ‘˜2 for ğ‘˜ = Â±1,Â±2,...,Â±ğ‘›,
ğ‘˜
perhaps i can write ğ‘(ğ‘¥) in terms of these roots.
wait, but ğ‘ (ï¸€1)ï¸€ = ğ‘˜2, which is like saying ğ‘ (ï¸€1)ï¸€ âˆ’ğ‘˜2 = 0, meaning that 1 are roots of
ğ‘˜ ğ‘˜ ğ‘˜
the equation ğ‘(ğ‘¥)âˆ’ 1 = 0, but again, thatâ€™s a rational function.
ğ‘¥2
alternatively, perhaps i can consider that ğ‘(ğ‘¥) âˆ’ 1 has roots at ğ‘¥ = 1 for ğ‘˜ =
ğ‘¥2 ğ‘˜
Â±1,Â±2,...,Â±ğ‘›, and try to factor it accordingly.
but this seems tricky because of the 1 term.
ğ‘¥2
maybe a better approach is to consider the polynomial ğ‘¥2ğ‘(ğ‘¥)âˆ’1, which is a degree
2ğ‘›+2 polynomial.
given that ğ‘ (ï¸€1)ï¸€ = ğ‘˜2, then ğ‘¥2ğ‘(ğ‘¥) âˆ’ 1 evaluated at ğ‘¥ = 1 gives (ï¸€1)ï¸€2 ğ‘ (ï¸€1)ï¸€ âˆ’ 1 =
ğ‘˜ ğ‘˜ ğ‘˜ ğ‘˜
1 Â·ğ‘˜2âˆ’1 = 1âˆ’1 = 0, so ğ‘¥ = 1 are roots of ğ‘¥2ğ‘(ğ‘¥)âˆ’1 = 0.
ğ‘˜2 ğ‘˜
therefore, ğ‘¥2ğ‘(ğ‘¥)âˆ’1 is a degree 2ğ‘›+2 polynomial with roots at ğ‘¥ = Â±1,Â±1,...,Â±1,
1 2 ğ‘›
and possibly two more roots.
wait,butğ‘¥2ğ‘(ğ‘¥)âˆ’1isapolynomial,anditsrootsarethesolutionstoğ‘¥2ğ‘(ğ‘¥) = 1,which
is similar to the original equation ğ‘ (ï¸€1)ï¸€ = ğ‘¥2.
ğ‘¥
so,perhapsicanconsidertherootsofğ‘¥2ğ‘(ğ‘¥)âˆ’1 = 0,whicharethesameasthesolutions
to ğ‘ (ï¸€1)ï¸€ = ğ‘¥2 due to the substitution ğ‘¦ = 1.
ğ‘¥ ğ‘¥
therefore, finding the roots of ğ‘¥2ğ‘(ğ‘¥) âˆ’ 1 = 0 is equivalent to finding ğ‘¥ such that
ğ‘ (ï¸€1)ï¸€ = ğ‘¥2.
ğ‘¥
giventhat,andknowingthatğ‘(ğ‘¥)isamonicpolynomialofdegree2ğ‘›,andthatğ‘ (ï¸€1)ï¸€ = ğ‘˜2
ğ‘˜
for ğ‘˜ = Â±1,Â±2,...,Â±ğ‘›, perhaps i can find a relationship between ğ‘(ğ‘¥) and these roots.
alternatively, perhaps i can consider the lagrange interpolation formula for ğ‘(ğ‘¥), given
its values at specific points.
since ğ‘(ğ‘¥) is a monic polynomial of degree 2ğ‘›, and i have its values at 2ğ‘› points, i
can indeed find ğ‘(ğ‘¥) explicitly using interpolation, but that seems complicated for this
problem.
94

<!-- Page 95 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
moreover, the problem is not asking for ğ‘(ğ‘¥), but for the real numbers ğ‘¥ that satisfy
ğ‘ (ï¸€1)ï¸€ = ğ‘¥2, beyond the given ğ‘¥ = ğ‘˜ for ğ‘˜ = Â±1,Â±2,...,Â±ğ‘›.
ğ‘¥
so, perhaps there is a symmetry or a particular property of ğ‘(ğ‘¥) that i can exploit.
wait, perhaps i can consider that ğ‘(ğ‘¥) and ğ‘ (ï¸€1)ï¸€ are related in a specific way.
ğ‘¥
giventhatğ‘(ğ‘¥)isapolynomial,ğ‘ (ï¸€1)ï¸€isarationalfunction,andmultiplyingbyğ‘¥2ğ‘› gives
ğ‘¥
a polynomial.
alternatively, perhaps there is a functional equation that ğ‘(ğ‘¥) satisfies.
let me consider writing ğ‘(ğ‘¥) in terms of its roots.
but ğ‘(ğ‘¥) is a monic polynomial of degree 2ğ‘›, so it can be written as:
ğ‘(ğ‘¥) = (ğ‘¥âˆ’ğ‘Ÿ )(ğ‘¥âˆ’ğ‘Ÿ )Â·Â·Â·(ğ‘¥âˆ’ğ‘Ÿ )
1 2 2ğ‘›
for some roots ğ‘Ÿ ,ğ‘Ÿ ,...,ğ‘Ÿ .
1 2 2ğ‘›
but i donâ€™t know these roots, so that might not help directly.
alternatively, perhaps i can consider the fact that ğ‘(ğ‘¥) has certain symmetries given the
condition ğ‘ (ï¸€1)ï¸€ = ğ‘˜2.
ğ‘˜
wait, maybe i can consider the function ğ‘(ğ‘¥)âˆ’ 1 , but again, that introduces a rational
ğ‘¥2
function.
alternatively, perhaps i can consider the derivative of ğ‘(ğ‘¥), but i donâ€™t see an immediate
connection.
let me try to think differently.
suppose i define a new function ğ‘“(ğ‘¥) = ğ‘ (ï¸€1)ï¸€ âˆ’ğ‘¥2, as i did earlier.
ğ‘¥
then, ğ‘“(ğ‘¥) = 0 for ğ‘¥ = Â±1,Â±2,...,Â±ğ‘›, as given.
i need to find all real ğ‘¥ such that ğ‘“(ğ‘¥) = 0, beyond these known roots.
now, ğ‘“(ğ‘¥) is a rational function, but ğ‘¥2ğ‘›ğ‘“(ğ‘¥) is a polynomial of degree 2ğ‘› + 2, as
established earlier.
therefore, ğ‘¥2ğ‘›ğ‘“(ğ‘¥) = 0 has roots at ğ‘¥ = Â±1,Â±2,...,Â±ğ‘›, and possibly two more roots.
so, if i can find this polynomial ğ‘¥2ğ‘›ğ‘“(ğ‘¥), i can factor it and find the additional roots.
but to find ğ‘¥2ğ‘›ğ‘“(ğ‘¥), i need to know more about ğ‘(ğ‘¥), which i donâ€™t.
alternatively, perhaps i can consider the behavior of ğ‘“(ğ‘¥) as ğ‘¥ approaches infinity or
zero.
as ğ‘¥ â†’ âˆ, ğ‘ (ï¸€1)ï¸€ behaves like (ï¸€1)ï¸€2ğ‘› , which approaches 0, while ğ‘¥2 approaches infinity,
ğ‘¥ ğ‘¥
so ğ‘“(ğ‘¥) â†’ âˆ’âˆ.
as ğ‘¥ â†’ 0, ğ‘ (ï¸€1)ï¸€ behaves like (ï¸€1)ï¸€2ğ‘› , which goes to infinity, while ğ‘¥2 approaches 0, so
ğ‘¥ ğ‘¥
ğ‘“(ğ‘¥) â†’ âˆ.
this suggests that there are additional roots beyond the given ğ‘¥ = Â±1,Â±2,...,Â±ğ‘›,
possibly at infinity or near zero, but i need to find them explicitly.
wait, perhaps thereâ€™s a smarter way to approach this.
given that ğ‘(ğ‘¥) is a monic polynomial of degree 2ğ‘›, and it satisfies ğ‘ (ï¸€1)ï¸€ = ğ‘˜2 for
ğ‘˜
ğ‘˜ = Â±1,Â±2,...,Â±ğ‘›, perhaps i can consider the minimal polynomial that satisfies these
conditions.
alternatively, maybe i can consider that ğ‘(ğ‘¥) can be expressed in terms of a product
involving these roots.
wait, perhaps i can consider that ğ‘(ğ‘¥)âˆ’ 1 has roots at ğ‘¥ = Â±1,Â±1,...,Â±1, but again,
ğ‘¥2 1 2 ğ‘›
that introduces a rational function.
alternatively, perhaps i can consider that ğ‘¥2ğ‘(ğ‘¥)âˆ’1 has roots at ğ‘¥ = Â±1,Â±1,...,Â±1,
1 2 ğ‘›
as previously established, and try to find a general form for this polynomial.
let me consider that ğ‘¥2ğ‘(ğ‘¥) âˆ’ 1 is a degree 2ğ‘› + 2 polynomial with roots at ğ‘¥ =
Â±1,Â±1,...,Â±1, and two additional roots.
1 2 ğ‘›
95

<!-- Page 96 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
given that, perhaps i can express ğ‘¥2ğ‘(ğ‘¥)âˆ’1 as:
ğ‘› (ï¸‚ )ï¸‚
âˆï¸ 1
ğ‘¥2ğ‘(ğ‘¥)âˆ’1 = ğ‘¥2âˆ’ Â·(ğ‘¥2âˆ’ğ‘)
ğ‘˜2
ğ‘˜=1
âˆš
for some constant ğ‘, since the roots come in pairs ğ‘¥ = Â±1 and possibly ğ‘¥ = Â± ğ‘.
ğ‘˜
but i need to determine ğ‘.
alternatively, perhaps i can consider that the product âˆï¸€ğ‘› (ï¸€ ğ‘¥2âˆ’ 1 )ï¸€ is a degree 2ğ‘›
ğ‘˜=1 ğ‘˜2
polynomial, and ğ‘¥2ğ‘(ğ‘¥)âˆ’ 1 is degree 2ğ‘›+2, so there are two more roots.
but without knowing more about ğ‘(ğ‘¥), itâ€™s hard to pin down ğ‘.
wait, perhaps i can consider the behavior at ğ‘¥ = 0.
at ğ‘¥ = 0, ğ‘¥2ğ‘(ğ‘¥)âˆ’1 = âˆ’1, so ğ‘¥ = 0 is not a root.
similarly, as ğ‘¥ â†’ âˆ, ğ‘¥2ğ‘(ğ‘¥)âˆ’1 behaves like ğ‘¥2ğ‘›+2, which goes to infinity.
but i still donâ€™t see how to find ğ‘.
alternatively,perhapsicanconsiderthevalueofğ‘¥2ğ‘(ğ‘¥)âˆ’1ataspecificpoint,sayğ‘¥ = 1,
but thatâ€™s already a root.
wait, perhaps i can consider ğ‘¥ = âˆ, but thatâ€™s not helpful.
alternatively, perhaps i can consider that ğ‘(ğ‘¥) is determined by its values at 2ğ‘› points,
so i can use interpolation to find ğ‘(ğ‘¥), but that seems too involved for this problem.
alternatively, perhaps thereâ€™s a symmetry in the equation that i can exploit.
given that ğ‘(ğ‘¥) is a polynomial of even degree 2ğ‘›, and the conditions are given for
ğ‘˜ = Â±1,Â±2,...,Â±ğ‘›, perhaps ğ‘(ğ‘¥) has some symmetry properties.
for example, maybe ğ‘(ğ‘¥) is an even function, but i donâ€™t know that for sure.
wait, letâ€™s check: if ğ‘(ğ‘¥) were even, then ğ‘(âˆ’ğ‘¥) = ğ‘(ğ‘¥), but the conditions are given for
both positive and negative ğ‘˜, so perhaps thatâ€™s a clue.
given that ğ‘ (ï¸€1)ï¸€ = ğ‘˜2 for both ğ‘˜ and âˆ’ğ‘˜, and ğ‘˜2 is the same for both, it suggests that
ğ‘˜
ğ‘(ğ‘¥) might be even, but i need to confirm.
however, ğ‘(ğ‘¥) is a general monic polynomial of degree 2ğ‘›, and its coefficients could
make it even or not.
(ï¸ )ï¸
but in this case, since ğ‘ (ï¸€1)ï¸€ = ğ‘ 1 = ğ‘˜2, which is the same, it suggests that ğ‘ (ï¸€1)ï¸€ is
ğ‘˜ âˆ’ğ‘˜ ğ‘¥
even in ğ‘¥, but ğ‘(ğ‘¥) itself doesnâ€™t have to be even.
alternatively, perhaps i can consider that ğ‘(ğ‘¥) has a reciprocal property, given the
conditions involving 1.
ğ‘˜
wait, perhaps i should consider the reciprocal polynomial of ğ‘(ğ‘¥), which is ğ‘¥2ğ‘›ğ‘ (ï¸€1)ï¸€ as
ğ‘¥
before.
let me denote this as ğ‘Ÿ(ğ‘¥) = ğ‘¥2ğ‘›ğ‘ (ï¸€1)ï¸€ , which is a degree 2ğ‘› polynomial.
ğ‘¥
given that, and knowing that ğ‘ (ï¸€1)ï¸€ = ğ‘˜2, then ğ‘Ÿ(ğ‘˜) = ğ‘˜2ğ‘›ğ‘ (ï¸€1)ï¸€ = ğ‘˜2ğ‘›Â·ğ‘˜2 = ğ‘˜2ğ‘›+2.
ğ‘˜ ğ‘˜
therefore, ğ‘Ÿ(ğ‘˜) = ğ‘˜2ğ‘›+2 for ğ‘˜ = Â±1,Â±2,...,Â±ğ‘›.
now, since ğ‘Ÿ(ğ‘¥) is a degree 2ğ‘› polynomial, and it equals ğ‘¥2ğ‘›+2 at 2ğ‘› points, which is
more points than the degree, unless thereâ€™s some dependency.
wait, but ğ‘Ÿ(ğ‘¥) is degree 2ğ‘›, and ğ‘¥2ğ‘›+2 is degree 2ğ‘›+2, so they canâ€™t be equal at more
points than their degree difference unless they are the same polynomial, which they
arenâ€™t.
this suggests that my earlier assumption is incorrect, or perhaps i need to consider that
ğ‘Ÿ(ğ‘¥)âˆ’ğ‘¥2ğ‘›+2 is a polynomial of degree 2ğ‘›+2, but that seems inconsistent because ğ‘Ÿ(ğ‘¥)
is degree 2n and ğ‘¥2ğ‘›+2 is degree 2ğ‘›+ 2, so their difference should be degree 2ğ‘›+2.
wait, perhaps i should correct that.
given ğ‘Ÿ(ğ‘¥) = ğ‘¥2ğ‘›ğ‘ (ï¸€1)ï¸€ , which is degree 2ğ‘›, and ğ‘¥2ğ‘›+2 is degree 2ğ‘›+2, their difference
ğ‘¥
ğ‘Ÿ(ğ‘¥)âˆ’ğ‘¥2ğ‘›+2 is degree 2ğ‘›+2, as the highest degree term comes from âˆ’ğ‘¥2ğ‘›+2.
96

<!-- Page 97 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
given that ğ‘Ÿ(ğ‘˜) = ğ‘˜2ğ‘›+2 for ğ‘˜ = Â±1,Â±2,...,Â±ğ‘›, then ğ‘Ÿ(ğ‘¥)âˆ’ğ‘¥2ğ‘›+2 = 0 at these points.
therefore,ğ‘Ÿ(ğ‘¥)âˆ’ğ‘¥2ğ‘›+2 has2ğ‘›rootsatğ‘¥ = Â±1,Â±2,...,Â±ğ‘›,andsinceitâ€™sdegree2ğ‘›+2,
there are two more roots.
now, perhaps i can factor ğ‘Ÿ(ğ‘¥)âˆ’ğ‘¥2ğ‘›+2 as:
ğ‘Ÿ(ğ‘¥)âˆ’ğ‘¥2ğ‘›+2 = (ğ‘¥2âˆ’1)(ğ‘¥2âˆ’4)Â·Â·Â·(ğ‘¥2âˆ’ğ‘›2)Â·(ğ‘¥2âˆ’ğ‘)
for some constant ğ‘, since the roots come in pairs ğ‘¥ = Â±ğ‘˜, and possibly another pair
âˆš
ğ‘¥ = Â± ğ‘.
now, to find ğ‘, perhaps i can consider the behavior at infinity or use the fact that the
leading coefficients must match.
given that ğ‘Ÿ(ğ‘¥) = ğ‘¥2ğ‘›ğ‘ (ï¸€1)ï¸€ and ğ‘(ğ‘¥) is monic of degree 2ğ‘›, the leading term of ğ‘(ğ‘¥) is
ğ‘¥
ğ‘¥2ğ‘›, so ğ‘ (ï¸€1)ï¸€ = ğ‘¥âˆ’2ğ‘›+Â·Â·Â· , and thus ğ‘Ÿ(ğ‘¥) = ğ‘¥2ğ‘›Â·ğ‘¥âˆ’2ğ‘›+Â·Â·Â· = 1+Â·Â·Â· , so the leading
ğ‘¥
term of ğ‘Ÿ(ğ‘¥) is 1.
on the other hand, ğ‘¥2ğ‘›+2 has leading term ğ‘¥2ğ‘›+2, so ğ‘Ÿ(ğ‘¥) âˆ’ ğ‘¥2ğ‘›+2 has leading term
âˆ’ğ‘¥2ğ‘›+2.
now,theright-handside,(ğ‘¥2âˆ’1)(ğ‘¥2âˆ’4)Â·Â·Â·(ğ‘¥2âˆ’ğ‘›2)(ğ‘¥2âˆ’ğ‘),expandstoğ‘¥2ğ‘›+2âˆ’(1+
4+Â·Â·Â·+ğ‘›2+ ğ‘)ğ‘¥2ğ‘›+Â·Â·Â· .
wait,actually,theproduct(ğ‘¥2âˆ’1)(ğ‘¥2âˆ’4)Â·Â·Â·(ğ‘¥2âˆ’ğ‘›2)(ğ‘¥2âˆ’ğ‘)wouldhaveleadingterm
ğ‘¥2ğ‘›+2, and the coefficient of ğ‘¥2ğ‘› would be âˆ’(1+4+Â·Â·Â·+ğ‘›2+ğ‘).
butğ‘Ÿ(ğ‘¥)âˆ’ğ‘¥2ğ‘›+2 hasleadingtermâˆ’ğ‘¥2ğ‘›+2,andthecoefficientofğ‘¥2ğ‘› wouldbesomething
else, depending on
on the other hand, ğ‘¥2ğ‘›+2 has leading term ğ‘¥2ğ‘›+2, so ğ‘Ÿ(ğ‘¥) âˆ’ ğ‘¥2ğ‘›+2 has leading term
âˆ’ğ‘¥2ğ‘›+2.
now,theright-handside,(ğ‘¥2âˆ’1)(ğ‘¥2âˆ’4)Â·Â·Â·(ğ‘¥2âˆ’ğ‘›2)(ğ‘¥2âˆ’ğ‘),expandstoğ‘¥2ğ‘›+2âˆ’(1+
4+Â·Â·Â·+ğ‘›2+ ğ‘)ğ‘¥2ğ‘›+Â·Â·Â· .
wait,actually,theproduct(ğ‘¥2âˆ’1)(ğ‘¥2âˆ’4)Â·Â·Â·(ğ‘¥2âˆ’ğ‘›2)(ğ‘¥2âˆ’ğ‘)wouldhaveleadingterm
ğ‘¥2ğ‘›+2, and the coefficient of ğ‘¥2ğ‘› would be âˆ’(1+4+Â·Â·Â·+ğ‘›2+ğ‘).
butğ‘Ÿ(ğ‘¥)âˆ’ğ‘¥2ğ‘›+2 hasleadingtermâˆ’ğ‘¥2ğ‘›+2,andthecoefficientofğ‘¥2ğ‘› wouldbesomething
else, depending on the expansion of ğ‘Ÿ(ğ‘¥).
therefore, to match the leading coefficients, i must have:
âˆ’ğ‘¥2ğ‘›+2 = ğ‘¥2ğ‘›+2âˆ’(1+4+Â·Â·Â·+ğ‘›2+ğ‘)ğ‘¥2ğ‘›+Â·Â·Â·
this implies that:
âˆ’ğ‘¥2ğ‘›+2 = ğ‘¥2ğ‘›+2âˆ’(1+4+Â·Â·Â·+ğ‘›2+ğ‘)ğ‘¥2ğ‘›+Â·Â·Â·
comparing coefficients, the coefficient of ğ‘¥2ğ‘›+2 on the left is âˆ’1, and on the right is 1.
this suggests that thereâ€™s an inconsistency unless i made a mistake in my earlier steps.
wait, perhaps i need to reconsider the factorization.
given that ğ‘Ÿ(ğ‘¥)âˆ’ğ‘¥2ğ‘›+2 = (ğ‘¥2âˆ’1)(ğ‘¥2âˆ’4)Â·Â·Â·(ğ‘¥2âˆ’ğ‘›2)(ğ‘¥2âˆ’ğ‘), and the leading term
on the left is âˆ’ğ‘¥2ğ‘›+2, while on the right, itâ€™s ğ‘¥2ğ‘›+2, this suggests that there is a sign
discrepancy.
therefore, perhaps the correct factorization should include a negative sign:
ğ‘Ÿ(ğ‘¥)âˆ’ğ‘¥2ğ‘›+2 = âˆ’(ğ‘¥2âˆ’1)(ğ‘¥2âˆ’4)Â·Â·Â·(ğ‘¥2âˆ’ğ‘›2)(ğ‘¥2âˆ’ğ‘)
this way, the leading coefficient matches: âˆ’ğ‘¥2ğ‘›+2 = âˆ’ğ‘¥2ğ‘›+2.
now, to find ğ‘, perhaps i can consider the constant term.
the constant term of ğ‘Ÿ(ğ‘¥)âˆ’ğ‘¥2ğ‘›+2 is ğ‘Ÿ(0)âˆ’0 = ğ‘Ÿ(0).
97

<!-- Page 98 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
but ğ‘Ÿ(ğ‘¥) = ğ‘¥2ğ‘›ğ‘ (ï¸€1)ï¸€ , so ğ‘Ÿ(0) = lim ğ‘¥2ğ‘›ğ‘ (ï¸€1)ï¸€ .
ğ‘¥ ğ‘¥â†’0 ğ‘¥
but ğ‘ (ï¸€1)ï¸€ behaves like (ï¸€1)ï¸€2ğ‘› as ğ‘¥ â†’ 0, so ğ‘¥2ğ‘›Â· (ï¸€1)ï¸€2ğ‘› = 1, so ğ‘Ÿ(0) = 1.
ğ‘¥ ğ‘¥ ğ‘¥
on the other hand, the constant term of âˆ’(ğ‘¥2 âˆ’ 1)(ğ‘¥2 âˆ’ 4)Â·Â·Â·(ğ‘¥2 âˆ’ ğ‘›2)(ğ‘¥2 âˆ’ ğ‘) is
âˆ’(âˆ’1)ğ‘›Â·1Â·4Â·9Â·Â·Â·ğ‘›2Â·ğ‘.
wait, actually, expanding the product:
(ğ‘¥2âˆ’1)(ğ‘¥2âˆ’4)Â·Â·Â·(ğ‘¥2âˆ’ğ‘›2)(ğ‘¥2âˆ’ğ‘) = ğ‘¥2ğ‘›+2âˆ’(1+4+Â·Â·Â·+ğ‘›2+ğ‘)ğ‘¥2ğ‘›+Â·Â·Â·+(âˆ’1)ğ‘›+1(1Â·
4Â·9Â·Â·Â·ğ‘›2Â·ğ‘)
therefore, the constant term is (âˆ’1)ğ‘›+1(1Â·4Â·9Â·Â·Â·ğ‘›2Â·ğ‘).
but since ğ‘Ÿ(ğ‘¥)âˆ’ğ‘¥2ğ‘›+2 has constant term ğ‘Ÿ(0) = 1, and the right-hand side has constant
term âˆ’(âˆ’1)ğ‘›+1(1Â· 4Â·9Â·Â·Â·ğ‘›2Â·ğ‘) = (âˆ’1)ğ‘›(1Â·4Â·9Â·Â·Â·ğ‘›2Â·ğ‘), we have:
(âˆ’1)ğ‘›(1Â·4Â·9Â·Â·Â·ğ‘›2Â·ğ‘) = 1
therefore:
1
ğ‘ =
(âˆ’1)ğ‘›(1Â·4Â·9Â·Â·Â·ğ‘›2)
but 1Â·4Â·9Â·Â·Â·ğ‘›2 = (ğ‘›!)2, since 1 = 12,4 = 22,9 = 32, etc., up to ğ‘›2.
therefore:
1
ğ‘ =
(âˆ’1)ğ‘›(ğ‘›!)2
but since ğ‘ is a real number, and ğ‘› is even (as given in the problem), because ğ‘› is an
even positive integer, (âˆ’1)ğ‘› = 1, so:
1
ğ‘ =
(ğ‘›!)2
therefore, the factorization is:
(ï¸‚ )ï¸‚
ğ‘Ÿ(ğ‘¥)âˆ’ğ‘¥2ğ‘›+2 = âˆ’ (ï¸€ ğ‘¥2âˆ’1 )ï¸€(ï¸€ ğ‘¥2âˆ’4 )ï¸€ Â·Â·Â· (ï¸€ ğ‘¥2âˆ’ğ‘›2)ï¸€ ğ‘¥2âˆ’ 1
(ğ‘›!)2
now, since ğ‘Ÿ(ğ‘¥) = ğ‘¥2ğ‘›ğ‘ (ï¸€1)ï¸€ , and ğ‘Ÿ(ğ‘¥)âˆ’ğ‘¥2ğ‘›+2 = 0 when ğ‘ (ï¸€1)ï¸€ = ğ‘¥2, the roots of this
ğ‘¥ ğ‘¥
equation are ğ‘¥ = Â±1,Â±2,...,Â±ğ‘›,Â±1.
ğ‘›!
wait,no. actually,fromthefactorization,therootsareğ‘¥ = Â±1,Â±2,...,Â±ğ‘›,Â±1 because
ğ‘›!
ğ‘¥2âˆ’ 1 = 0 gives
(ğ‘›!)2
1
ğ‘¥ = Â± .
ğ‘›!
therefore, the solutions to ğ‘ (ï¸€1)ï¸€ = ğ‘¥2 are ğ‘¥ = Â±1,Â±2,...,Â±ğ‘›,Â±1.
ğ‘¥ ğ‘›!
but the problem states that ğ‘ (ï¸€1)ï¸€ = ğ‘˜2 for 1 â‰¤ |ğ‘˜| â‰¤ ğ‘›, so these ğ‘¥ = Â±1,Â±2,...,Â±ğ‘› are
ğ‘˜
already known solutions.
therefore, the "other real numbers" ğ‘¥ that satisfy ğ‘ (ï¸€1)ï¸€ = ğ‘¥2 are ğ‘¥ = Â±1.
ğ‘¥ ğ‘›!
wait, but in the factorization, i have
1
ğ‘¥2âˆ’ ,
(ğ‘›!)2
which gives ğ‘¥ = Â±1, assuming thatâ€™s correct.
ğ‘›!
but i need to verify this conclusion.
98

<!-- Page 99 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
given that ğ‘ = 1 and the factorization is
(ğ‘›!)2
(ï¸‚ )ï¸‚
ğ‘Ÿ(ğ‘¥)âˆ’ğ‘¥2ğ‘›+2 = âˆ’ (ï¸€ ğ‘¥2âˆ’1 )ï¸€(ï¸€ ğ‘¥2âˆ’4 )ï¸€ Â·Â·Â· (ï¸€ ğ‘¥2âˆ’ğ‘›2)ï¸€ ğ‘¥2âˆ’ 1 ,
(ğ‘›!)2
then indeed, the roots are ğ‘¥ = Â±1,Â±2,...,Â±ğ‘›,Â±1.
ğ‘›!
therefore, the additional solutions beyond ğ‘¥ = Â±1,Â±2,...,Â±ğ‘› are
1
ğ‘¥ = Â± .
ğ‘›!
but wait, is ğ‘›! correct here? let me double-check the calculation of ğ‘.
earlier, i had:
1 1
ğ‘ = =
(âˆ’1)ğ‘›(1Â·4Â·9Â·Â·Â·ğ‘›2) (ğ‘›!)2
since ğ‘› is even, (âˆ’1)ğ‘› = 1, so indeed, ğ‘ = 1 .
(ğ‘›!)2
therefore, the additional roots are ğ‘¥ = Â±1.
ğ‘›!
hence, the other real numbers ğ‘¥ that satisfy ğ‘ (ï¸€1)ï¸€ = ğ‘¥2 are ğ‘¥ = Â±1.
ğ‘¥ ğ‘›!
**final answer**
1
ğ‘¥ = Â±
ğ‘›!
99

<!-- Page 100 -->

TowardsSystem2ReasoninginLLMs:LearningHowtoThinkWithMetaChain-of-Thought
Table 5: Regret Statistics by Model and Prompting Strategy
Model Prompting Strategy Regret Count Regret %
Baseline 6 0.24

### CoT 8 0.32

Llama 3.1 405B Think 87 3.49

### Think (3-shot) 216 8.69

Think & Verify 391 15.67
Baseline 7 0.30

### CoT 12 0.50

Llama 3.1 8B Think 103 4.51

### Think (3-shot) 103 4.28

Think & Verify 454 20.87
Baseline 5 0.20

### CoT 6 0.25

Llama 3.1 70B Think 313 12.65

### Think (3-shot) 231 9.33

Think & Verify 638 25.67
Baseline 1 0.04

### CoT 2 0.08

GPT-4o-mini Think 66 2.64
Think (3-shot) 53 2.12
Think & Verify 62 2.48
Baseline 0 0.00

### CoT 0 0.00


### GPT-4o Think 34 1.36

Think (3-shot) 28 1.12
Think & Verify 38 1.52
Baseline 1 0.04

### CoT 2 0.08

GPT-3.5-turbo Think 24 0.96
Think (3-shot) 25 1.00
Think & Verify 12 0.48
Baseline 0 0.00

### CoT 3 0.12

Claude 3.5 Sonnet Think 81 3.24

### Think (3-shot) 99 3.96

Think & Verify 111 4.44
Baseline 0 0.00

### CoT 1 0.04

Claude 3.5 Haiku Think 16 0.64
Think (3-shot) 16 0.64
Think & Verify 4 0.16
100

## Tables

**Table (Page 10):**

|  |  |  |  |  |  |  |  |  |  |
|---|---|---|---|---|---|---|---|---|---|
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  | M M |  |  |  |
|  |  |  |  |  |  |  | M M | ajority@6 ajority@8 | 4 |
|  |  |  |  |  |  |  | Gr Ba Or | eedy se Filter iginal |  |


**Table (Page 10):**

|  |  |  |  |  |  |  |  |  |  |
|---|---|---|---|---|---|---|---|---|---|
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  | Pass@ Pass@ | 2 4 | Pass@1 Pass@3 | 6 2 |
|  |  |  |  |  |  | Pass@ | 8 | Pass@6 | 4 |


**Table (Page 14):**

| Policy | Value |
|---|---|
| ğœ‹ ğœƒ0 ğœ‹ ğœƒ1 | - - |
| ğœ‹ ğœƒ0 ğœ‹ ğœƒ0 ğœ‹ ğœƒ1 ğœ‹ ğœƒ1 | {ğ‘£,ğ‘ŸË†} ğœ‘0 {ğ‘£,ğ‘ŸË†} ğœ‘1 {ğ‘£,ğ‘ŸË†} ğœ‘0 {ğ‘£,ğ‘ŸË†} ğœ‘1 |


**Table (Page 20):**

|  |  |  |  |  |  |  |  |
|---|---|---|---|---|---|---|---|
|  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |


**Table (Page 20):**

|  | Sequential | Best-of-N Weig | hted |  |  |  |  |  |
|---|---|---|---|---|---|---|---|---|
|  | Parallel Be Sequential Parallel Ma | st-of-N Weighte Majority jority | d |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |


**Table (Page 28):**

| % pf Training Epoch # of Unique Problems ~0.05 ~0.3 1 (#Training Trajectories) 500 (7.44k) 3000 (41.7k) 7086 (97k) |  |  |
|---|---|---|
|  |  |  |
|  |  |  |
|  |  |  |


**Table (Page 28):**

|  |
|---|
|  |
|  |


**Table (Page 28):**

|  |  |  |  |  |  |  |
|---|---|---|---|---|---|---|
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |


**Table (Page 28):**

|  |  |  |  |  |  |  |
|---|---|---|---|---|---|---|
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |


**Table (Page 28):**

|  |  |  |  |  |  |  |
|---|---|---|---|---|---|---|
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |


**Table (Page 28):**

|  |  |  |  |  |  |  |
|---|---|---|---|---|---|---|
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |


**Table (Page 28):**

|  |  |  |  |  |  |  |
|---|---|---|---|---|---|---|
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |


**Table (Page 28):**

|  |  |  |  |  |  |  |
|---|---|---|---|---|---|---|
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |


**Table (Page 28):**

|  |  |  |  |  |  |  |
|---|---|---|---|---|---|---|
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |


**Table (Page 28):**

|  |  |  |  |  |  |  |
|---|---|---|---|---|---|---|
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |


**Table (Page 28):**

|  |  |  |  |  |  |  |
|---|---|---|---|---|---|---|
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |


**Table (Page 29):**

| Oracle 0.6 Majority ycaruccA 0.5 0.4 0.3 1 2 4 8 16 32 64 Number of Generations per Problem | Oracle Majority |
|---|---|
|  |  |
|  |  |


**Table (Page 29):**

| hcraeS ycaruccA snekoT ni fo desU # Process Reward Models |  |  |  |  |  |  |  |
|---|---|---|---|---|---|---|---|
|  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |


**Table (Page 29):**

|  |  |  |  |  |  |  |
|---|---|---|---|---|---|---|
|  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |


**Table (Page 43):**

|  |  | PRM-Q7086 PRM-Q3000 PRM-Q500 Oracle Majority ğ›¾=1 ğ›¾=0.995 ğ›¾=0.99 |
|---|---|---|
|  |  |  |
|  |  |  |
|  |  |  |
|  |  |  |
|  |  |  |


**Table (Page 46):**

| Data Source | Original | Base Filter | Strict Filter |
|---|---|---|---|
| HARP (Yue et al., 2024) NuminaMath (LI et al., 2024) Omni-MATH (Gao et al., 2024) OpenMathInstruct-2 (Toshniwal et al., 2024) | 4,780 859,608 4,428 607,324 | 3,691 452,820 3,660 600,191 | 2,996 231,887 2,478 496,331 |
| Total | 1,476,140 | 1,060,362 | 733,692 |


**Table (Page 50):**

| Majority Vote (TIR) Best of N (TIR) |  |
|---|---|
| Majority Vote (CoT) Best of N (CoT) |  |
|  |  |
|  |  |
|  |  |
|  |  |
|  |  |
|  |  |
