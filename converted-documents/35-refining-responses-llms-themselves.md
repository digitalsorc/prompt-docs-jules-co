---
title: "Refining Responses LLMs Themselves"
original_file: "./35_Refining_Responses_LLMs_Themselves.pdf"
document_type: "research"
conversion_date: "2025-11-29"
topics: ["prompt-engineering", "llm", "rag", "chain-of-thought", "agents"]
keywords: ["gpt", "model", "optimization", "response", "refinedgpt", "llms", "arxiv", "language", "user", "page"]
summary: "<!-- Page 1 -->

Refining the Responses of LLMs by Themselves

### TianqiangYana;* andTianshengXub

aFutureNetworkofIntelligenceInstitute,TheChineseUniversityofHongKong,Shenzhen518172,P.R.China
bDepartmentofMathematics,FacultyofNatureScience,ImperialCollege,LondonSW72AZ,UnitedKingdom
Abstract. Inthepastfewyears,LargeLanguageModels(LLMs) innumerousinterdisciplinarytasks,whileofferingsupportformulhave generated unprecedented enthusiasm, with models like GPT timodaldataandexpandingtherangeofapplica"
related_documents: []
---

# Refining Responses LLMs Themselves

<!-- Page 1 -->

Refining the Responses of LLMs by Themselves

### TianqiangYana;* andTianshengXub

aFutureNetworkofIntelligenceInstitute,TheChineseUniversityofHongKong,Shenzhen518172,P.R.China
bDepartmentofMathematics,FacultyofNatureScience,ImperialCollege,LondonSW72AZ,UnitedKingdom
Abstract. Inthepastfewyears,LargeLanguageModels(LLMs) innumerousinterdisciplinarytasks,whileofferingsupportformulhave generated unprecedented enthusiasm, with models like GPT timodaldataandexpandingtherangeofapplicationsforlargelanproviding human-like responses for a wide range of inquiries in guagemodelstoapreviouslyunattainedscale.Atpresent,themulnearlyalldomains.However,thesemodelsoftenfailtodeliversat- timodal features of GPT-4’s ChatGPT remain inaccessible to most
isfactoryanswersalignedwithusers’specificneedsontheirfirstat- users. Nonetheless, in regular conversations, the latest version of
tempt,necessitatingmultipleiterationsandadditionaluserinputto ChatGPThasexhibitedconsiderableenhancementsinunderstanding
refinetheresponses.Thiscanleadtoanunnecessaryinvestmentof andresponsecapabilitiescomparedtoitsearlieriterations.Inadditime and effort from users. In this paper, we propose a simple yet tion,thewell-developedcommercializationofChatGPT,exemplified
efficient approach based on prompt engineering that leverages the byMicrosoft’sGPT-4-basedChatwithBingandOfficeCopilot,as
largelanguagemodelitselftooptimizeitsanswerswithoutrelying wellasamultitudeofthird-partyapplicationsusingGPTAPIs,has
onauxiliarymodels.Weintroduceaniterativeself-evaluatingopti- facilitatedthegradualpermeationofLLMconceptsandapplications
mizationmechanism,withthepotentialforimprovedoutputquality across diverse fields and demographics. This has established a sigas iterations progress, removing the need for manual intervention. nificantmilestoneintherealmofcomputerscience.
Theexperiment’sfindingsindicatethatutilizingourresponserefine- ThesuperiorityofLLMsrepresentapivotaladvancementinNLP
mentframeworkontheGPT-3.5modelyieldsresultsthatareonpar research, offering new prospects for language generation, dialogue
with, or even surpass, those generated by the cutting-edge GPT-4 systems,andcreativewriting.AsLLMscontinuetoevolve,theyare
model.Detailedimplementationstrategiesandillustrativeexamples expectedtoplayanincreasinglycriticalroleindictatingthedirection
areprovidedtodemonstratethesuperiorityofourproposedsolution. ofnaturallanguageprocessingandmachinelearningresearch.
1 Introduction 1.2 StudiesonrefiningtheresponsesofLLMs
1.1 RevisitingLargeLanguageModels
Despite their impressive capabilities, these models are not without
limitations: obtaining a user’s desired answer in a single attempt
LargeLanguageModels(LLMs)havebecomeasignificantdevelopremains a challenging endeavor. Various factors contribute to this
mentindeeplearningtechniques,whichallowsthemtounderstand
issue, such as biases inherent in training data and model architec- and generate natural language using vast amounts of textual data.
tures,whichcanresultinincorrectorcontextuallyinappropriatere-
LLMs have shown exceptional potential and flexibility in various
sponses[7].Moreover,thelackofexplainabilityandtransparencyin
Natural Language Processing (NLP) and Natural Language Generthedecision-makingprocessoftheseblack-boxmodelsfurtherexacation(NLG)tasks,suchastextsummary,machinetranslation,sentierbatesthedifficultyinoptimizingmodeloutputsforuserneeds[8].
mentanalysis,contentcreation,andconversationalAI.Thesemod-

### Thisphenomenonisprimarilyattributedtothechallengesfacedby

elsaretrainedusingaself-supervisedlearningapproachwherethey
thesemodelsincomprehendingnuancedandhighlyspecializedconlearnfromunlabeleddatabypredictingthefollowingwordortoken
textsoradheringtospecificwritingstylesandformatswhilegenerinsequentialdata.ThisprocessenablesLLMstodecipherthesyntax,
atingresponses,whichoftenleadstoinconsistenciesanddeviations
semantics,andgeneralknowledgeofhumanlanguagewhilealsorefromthedesiredoutput.[9,10].
tainingsignificantamountsoffactualinformationretrievedfromthe
The Reinforcement Learning with Human Feedback (RLHF)
trainingdataset.
mechanism is a recent advancement in the field of language learn-
TheemergenceandevolutionofLLMswereduetotheadvanceingmodels(LLMs)thataimstooptimizetheirinteractiveresponses
mentsintransformermodels,atypeofneuralnetworkthatutilizes
tohumanusers.Thisinnovativeapproachisdesignedtoincorporate
attentionmechanismstoencodeanddecodesequentialdata.Vaswani
human feedback in training LLMs to generate more effective, acet al. first proposed transformer models [1], and since then, many
curate,andcontextuallyrelevantresponses,whilemitigatingpotenvariations, including BERT [2], GPT-3 [3], XLNet [4], and XLM-
tialpitfallsassociatedwithtraditionalreinforcement-learning-based
RoBERTa[5],havebeendeveloped,demonstratingunrivaledperformethods[3].OnesignificantadvantageoftheRLHFmechanismlies
manceinseveralNLPbenchmarksandtasks,furtherhighlightingthe
initsabilitytoleveragebothexpertdemonstrationsandpreference
potencyandversatilityofLLMs.
comparisons to build a reward model for the LLM, which enables

### OpenAI’sChatGPTstandsasoneofthemostrenownedLLMsin

the model to adapt and improve its response generation based on
thefield,ofwhichthelatestversionisbasedonGPT-4[6].GPT-4
human-providedfeedback[11].InordertotraintheRLHFmechahasdemonstratedperformancethatrivalsorexceedshumanexperts
nismtooptimizetheLLM’sresponsestohumanusers,aninitialset
∗TianqiangYan.Email:222010015@link.cuhk.edu.cn of demonstrations is provided by human experts who interact with
3202
yaM
6
]LC.sc[
1v93040.5032:viXra

<!-- Page 2 -->

theLLM,generatinghigh-qualityresponses.Thesedemonstrations nismthatisinspiredtheideasofconversationalreinforcementlearnare then utilized to perform supervised fine-tuning [12]. Building ingandchain-of-thought.Ourcontributionscanbeconcludedas:
uponthisfoundation,themechanismfurtherincorporatesuserpref-
• We provide a novel paradigm to refine LLMs’ responses in an
erence comparisons through a process wherein the LLM generates
independent,application-level,andfullyautomaticway.
multiple candidate responses, and users are asked to rank or rate
• Ourparadigm,alongwiththeimplementationonitsbasis,allows
these responses according to their relevance, usefulness, and qualinstantdeploymentwithanyavailableLLMAPIs,whilerequiring
ity. To adjust and update the reward model accordingly, the RLHF
nearlyzerodevelopmentknowledge.
mechanismemploysanalgorithmthatcomputesthegradientsofthe
• Theimplementationisexaminedwithpossibledailyinquiries,and
rewardmodelbasedontheaggregateduserfeedback[11].
thejointoptimizationschemeachievesoverallthebestoutcome.
DespiteRLHF’snumerousadvantagesintrainingLLMs,onemajor concern is the possibility of negative side effects from over-
Thefollowingcontentofthisreportisarrangedasfollows:InSecoptimizingtohumanfeedback,potentiallyleadingtheLLMtogenertionII,theproposedoptimizationschemeisintroduced,andthethree
ateuninformativeorexcessivelyverboseresponsesinordertomaxiderivedsolutionsaredescribed.InSectionIII,thedetailedsettings
mizeitsperceivedreward[12].Additionally,therelianceonhumanandtheresultsofourtestingaredemonstrated.Eventually,wecongenerateddemonstrationsandfeedbackinherentlyintroducesthepocludeourstudyinSectionIV.
tentialforbiasorinconsistencyintothetrainingprocess,whichmay
influencetheperformanceandbehavioroftheresultingLLM.Inthe
2 Theadaptiveoptimizationparadigm
meantime,theintegrationofRLHFinLLMsdemandshighquality
volunteeroruserresponses,leadingtoincreasedtimeandmonetary
In this paper, we design a highly-efficient, fully-automatic interaccosts.
tionmechanismgroundedintheapplicationlayeroflargelanguage
Another extensively examined research study elucidated that
models(LLMs).OurapproachenablesadaptiveoptimizationofAI-
LLMsexhibitachain-of-thoughtcognition,andconcurrentlyemphageneratedresponseswithoutnecessitatinghumaninvolvement,while
sizedthatdeconstructingtheinferentialprocedureofaproblemvia
simultaneously eschewing the need for fine-tuning the underlying
chain-of-thoughtpromptingmayservetoaugmentthemodel’sprolanguagemodelorintroducingsupplementarymodels.Additionally,
ficiencyinaddressingintricatechallenges[13].Fundamentally,this
theframework’sexclusivefocusontheapplicationlayermakesitreconceptnecessitatesuserstometiculouslydissecttheirqueriesprior
markablyconvenientforuserstointegrateitwithLLMAPIs.Inthis
toinputtingthemintocomprehensivelanguagemodelsortoencoursection,wewillprovideathoroughexplanationoftheoptimization
agethemodeltodeliverresponsesaccompaniedbyanelaboratereaprocess.
soningprocess.IftheRLHFoptimizationprocedureisencapsulated
into four stages: “User formulates a query, model proffers a resolution, user evaluates the quality of the solution, model fine-tunes
based on assessment,” then chain-of-thoughts prompting embodies
amoreefficaciousapplication-tierfeedbackoptimizationtechnique.
This is attributed to its regardlessness of whether the model itself
has undergone optimization, and its potential capacity to facilitate
the model in producing dependable replies in a singular endeavor.
Nonetheless,thechallengewiththechain-of-thoughttheoryisthatif
auser’squeryismanuallydissected,themodel’sabilitytogenerate
a reliable response hinges on whether the user has accurately providedthedecomposedversionofthecorrespondingquestion.Onthe
otherhand,iftheuserrequeststhemodeltodeliverstep-by-stepanswers,butthereisadiscrepancybetweenthemodel’sinterpretation
oftheproblemandtheuser’soriginalpurpose,theultimatesolution
canalsobeentirelyoff-course.Insummary,bothRLHFandchainof-thoughtpromptingoffervaluableadvantages.Assuch,ouraimis
to integrate the strengths of these two methods and develop a fast
to deploy, fully automated strategy to improve the performance of
LLMs.
In this study, we concentrate on the exploration of application Figure1:Thisisanillustrationofusingouriterativeoptimization
mechanisms employed by prevalent LLMs. Our approach is predi- frameworktoenhancetheanswersprovidedbyLLMinresponseto
catedexclusivelyuponuserinquiries,LLMresponses,andjudicious userqueries.Thediagramdepictsoneiterationoftheoptimization
supplementaryprompts,aimingtoenhancethequalityofLLMfeed- process,involvingthreeagents:theuser(depictedasanavatar),a
back.It’simportanttonotethattheterm“quality”usedhere(includ- remoteLLMserver(representedbyarobot),andaterminal
ing the same expression that appears later) encompasses multiple (symbolizedbyacomputer).Theautomationloopisenclosed
metrics, including but not limited to the accuracy, comprehensive- withinadashedbox.
ness,andconcisenessoftheanswer.Explicitly,weproposeafeasible
methodologythatobviatesthenecessityforancillarymodelsorsupplementarymanualinterventions,enablinganLLMtoautonomously
2.1 Theoverallframework
refineitsresponsetoaquerythroughaprompt-driven,adaptivelyiterativeself-assessmentandoptimizationprocess.Withthisparadigm

### Theproposedoptimizationprocesscanbeoutlinedinthefollowing

asourgoal,wedesignaviable,general-purposeoptimizationmechasteps:

<!-- Page 3 -->


## Userstep:Theuserinputsaqueryintotheterminalandsendsit engineeringallowsforthespecificationofhuman-readableprompts

viaanAPItotheremoteLLMserver,withadesignatedmaximum tailoredtotaskobjectives,thusempoweringthemodeltodeliverdenumberofoptimizationiterations. siredoutputs.Tofacilitateanexaminationofpotentialdeficienciesin

## Automaticstep1:TheremoteLLMserverinitiallyrespondstothe anLLM’sreponsetoagivenquery,allthatisrequiredisthetransquerybyproducingamodel-generatedanswerandreturningitto missionofapre-designedprompttoaremoteserverwhichiscapable

theterminal. ofconveyingsucharequestandprovidingthedesiredfeedback.To

## Automaticstep2:Theterminalintegratestheuserqueryandthe constructaproficientprompt,threecriticalconstituentsmustbeinmodel’spreviousresponsetoformaprompt,instructingtheLLM tegrated: the initial inquiry posed by the user, the present response

model to analyze its initial answer, identify any limitations, and generatedbytheLLMinreferencetotheinquiry,andpromptsthat
providefeedbackaccordingly. steerthemodeltowardsaccuratecomprehension,analysis,andfeed-

## Automaticstep3:Theterminalgeneratesanoptimizationprompt, backalignedwiththedesiredobjectives.Moreover,inanattemptto

combining the LLM’s response, the user’s query, and the defi- mitigate the potential inclusion of extraneous information in LLM
ciencyanalysis,andsendsittotheremoteserverforimprovement. outputs, prompts can be augmented with limiting cues. For exam-
TheLLMmodelinfersanupdatedanswerandsendsitbacktothe ple,thepromptmayconcludewithadirectivesuchas“providethe
terminal. analysisresultonly”tofocusthemodel’soutputonspecificaspects

## Automatic step 4: The terminal receives the optimized response relevanttothequery.

andgeneratesapromptthatcombinestheoptimizedanswerwith Asouroptimizationstrategyreliesonaniterativeprocess,anesthe previous response and the user’s query, asking the model to sentialelementofoursolutionisaniterativeself-terminationmechdetermineiftheoptimizedanswerisanimprovement.Thecom- anismbasedonavotingmethodutilizingLLM.Whentheoptimizaparisonresultisthenreturnedtotheterminal. tionprocessadvancestothisstage,theterminalhasalreadycached

## Automatic step 5: If the comparison result shows that the opti- theuser’squestion,thepre-optimizedLLMresponse,andthepostmizedanswerisbetter,theterminalutilizesagreedystrategyto optimized response. The essence of this iterative self-termination

repeattheautomaticoptimizationprocess(fromstep3,automatic strategyistocomparethemodel’sanswersbeforeandafteroptimizastep2)untilthemaximumiterationshavebeenreached.Ifthere- tionbasedontheuser’squestionandselecttheoneitconsiderssupesult is not improved, the terminal ends the optimization process rior.Thereasonforsuchajudgmentmechanism,insteadofsimply
andreturnsthepreviousresponse. iteratingoptimizationuntiltheuser’smaximumoptimizationtimes
arereached,isthatwecannotguaranteethattheLLMoutputis100%
Figure 1 offers an intuitive portrayal of our response refinement reliable. In simple terms, whether the optimized answer generated
strategy,exemplifiedthroughasimulatedoptimizationprocess.The bythemodeltrulyachievesthegoalof“optimization”isnotentirely
optimizationprocessasawholeismainlyautomatedthroughtheter- certain,dependingonmanyfactors,includingwhetherthemodelcorminalinteractionswiththemodelAPI,withtheexceptionoftheuser rectlyunderstandstheuser’sintent,andwhetheritaccuratelydigests
settinganupperlimitforthenumberofoptimizationloops.Inpracti- thedefectsinthepreviousanswerandmakestargetedimprovements.
calapplications,thespecificoptimizationprocessishiddenfromthe Asthesolutionoperatesattheapplicationlevel,andthemodelreuser,andtheywillreceivearefinedresponsedirectlyafterinputting mains a black box, this self-termination strategy supports adaptive
theirquestion. iterationswhileavoidinganypotentialnegativeconsequencesfrom
Itshouldbenotedthattheinteractionlogicchainthroughoutthe further optimization beyond the optimal point. To summarize, this
entireenhanceprocessisfirst-order,meaningthattheschemedoes iterativeoptimizationself-terminationmechanismcontributestothe
not require the LLM to remember the entire previous optimization stabilityoftheoutputwhilesupportingtheprocess’sadaptability.
process.Thisisdonetopreventlargetokencostsandprematurede- Thismechanismisalsoprompt-driven.Theterminalintegratesthe
pletionoftokens.Furthermore,theoptimizationprocesswehavede- user’squery,thecurrentandpreviousanswersfromthemodel,and
signedcanensurethereliabilityofthisfirst-orderoptimizationmode. then employs a voting prompt to send the combined input to a re-
Thespecificreasonsforthiswillbeexplainedin2.2. mote server. A newly initialized model returns its judgment result.
To facilitate the terminal in producing appropriate responses based
onthevotingresult,itiscriticaltoaddanlimitinginstruction.As-
2.2 RefiningtheresponsesofLLMsbythemselves
sumingthatthelabelsoftheresponsesbeforeandafteroptimization
are“1”and“2”respectively,thepurposeofthelimitinginstruction
We have proposed an iterative optimization paradigm that intetoallowtheremotemodeltoonlyreturncontentlimitedtobeoneof
grates ideas from self-supervised reinforcement learning and chain
thetwolabels.Whenthemodeldeterminesthatthecurrentresponse
ofthought.
wouldbetteranswertheuser’squery,itemploysagreedystrategyby

### UponrevisitingtheoptimizationstrategyofRLHF,itisapparent

repeatingthepreviouslymentionedoptimizationsteps.Ontheother
thatthetwokeystepsinourrefinementmechanism–namely,feedhand,ifthemodeldeterminesthatthepreviousanswerisstillthebest
backdefectanalysisanddefect-guidingoptimizationlogic–arefunresponse,itreturnsthatanswertotheuser.
damentallyakintothoseofRLHF.However,therearenotabledis-
Anothercrucialdesignaspectofourapproachisthattheresponse
tinctionsbetweenthetwo.WhereasRLHFisgearedtowardsLLMs
optimizationmechanismofthisprocessisnotblind,whichattributes
stillintheirdevelopmentanddebuggingphasewithafocuson“hutothedefect-guidedenhancement.Theinspirationforthisoptimizamanfeedback,”ouroptimizationapproachleveragesself-evaluation
tion method comes from our understanding of the concept of the
andself-optimization(SESO)throughconversationalself-interaction
chainofthought[13].Theideafundamentallyinvolvesusingexplicit
processes that predominantly rely on prompt engineering. Prompt
guidanceorrequiringthemodeltooutputthereasoningprocessinorengineering represents a milestone in the field of natural language
dertoimprovetherobustnessoftheoutputoflargelanguagemodels
processingthathasemergedconcomitantlywiththeriseoflargelanasmuchaspossible.Thecorepurposeofthisideaistopreventthe
guage models. Enabled by the powerful contextual comprehension
modelfromblindlysearchingandgeneratinganswers.IntheappliandreasoningcapabilitiesinherentincontemporaryLLMs,prompt

<!-- Page 4 -->

Figure2:Theflowchartdepictingtheentireprocessofanadaptiveiterativeoptimizationmechanism.Thedeepblueroundedrectangles
representvariablesgeneratedbytheuser,theblackonesrepresentvariablesgeneratedbytheterminal,andthelightblueonesrepresent
variablesgeneratedbytheremotemodel.A"⊕"denotesthepromptcombinationoperation.
Table1:Settingsoftheimplementedstructureoftheresponseoptimization

### LLM Module Promptused

Pleaselistthedefectsofansweratothequestionq.Listthedefectsinonesentence

### Defectanalysis

insteadofalistwithlinebreaks!
Theansweratothequestionqisnotoptimalbecausethatd.Pleaserefinetheanswer
Guidedoptimisation providingabetteroneregardingtheaforementionedflaw.Youshouldprovidenothing
GPT-3.5-Turbo
buttheanswer.
Thequestionisq,towhichtherearetwooptimalanswers,oneisa,theotheroneisa∗.
Votingforbetter Pleaseanswereither"1"or"2"ifyouthinkoneofthemisbetter,or"0"ifyouthink
they’reequallygood.Donotreplyanythingelsethananumber!
cationscenariotargetedbyoursolution,skippingthedefectanalysis ingtheresponsesgeneratedbylargelanguagemodels.
and guided optimization steps would lead to a completely random
optimizationpath,whichwillfurtherresultintheinstabilityofthe
3 Testingandresultanalysis
optimizationprocess.Forexample,assumingtheuser’squestionis
“Wherewerethe2012Olympicsheld?”andthemodel’sinitialre-
3.1 Theimplementationandexperimentdesign
sponseis“The2012OlympicstookplaceinLondon,UK,opening
onJuly27thandclosingonAugust12th.”Itcanbechallengingtoop- We have publicly released the source code of an intellitimizethisanswer,evenforahuman.However,byprovidingsome gent conversational programme on GitHub, implemented
guidance information to the model, such as “The original question based on the reponse refinement paradigm discussed in this
only asks about the location of the Olympics, while the previous article, while following a flexible modular design as Figanswer includes irrelevant time information,” the model can focus ure 2. The project is available through the following link:
itsoptimizationeffortandremovetheunnecessarytimeinformation https://github.com/henryyantq/OptimaLLM. Here,
fromtherefinedanswerwithhighprobability. weprovidedetailsofthemodelconfigurationsandpromptsused,as
We can summarize the entire process into a flow structure, as listedinTable1.
shown in Figure 2, based on the various optimization nodes men- Todate,publicAPIsforotherlargelanguagemodelsarenotyet
tionedabove.Theframeworkoperateswithfirst-ordermemorysince available.Thus,inthissection,wewillonlyuseOpenAI’sGPTas
itensuresthateachiterationproducesaresultsthatistheoretically the target model for our optimization framework. As demonstrated
betterthanallpreviousoptimizationresults.Thus,theprocedureef- inTable1,theschemeisappliedtotheGPT-3.5-Turbomodel(herefectivelyavoidstheaccumulationofpreviousoutcomes,whichcould afterreferredtoastherefinedGPT-3.5),whichpresentsapromising
leadtoanincreaseintokenconsumptionasthenumberofiterations opportunity for improvement due to its status as an earlier version
increases.It’snoteworthythatsincepromptsarebasedonnaturallan- oftheGPTfamilyofferingthechatcompletioninterface.Additionguage,thedesignofpromptsinvolvedineachmoduleinthefigure ally,theGPT-3.5-Turboboastsadvantagesintermsoffasterresponse
mayvary.Ourfocusinthispaperistoprovidesuchacomprehensive generationandsmallercomputationaloverheads,makingitafeasible
responseoptimizationframework.InSection3,wepresentaseries choiceevenifmultipleiterationsarerequiredtoimprovethequality
ofexperimentresultstodemonstratetheeffectivenessofourscheme. oftheoriginalmodel’sresponses.Theconsumptionofcomputational
Theseexperimentsshowcasethesuperiorityofourapproachinrefin- resourcesandtimeisfullymanageableunderthesecircumstances.

<!-- Page 5 -->

Table2:Listoftheselectedquestionsandthecorrespondingreferenceanswers.Thereferenceanswersaregivenbythehumanexpert.

### Question ReferenceAnswer

Howtoreplacethememoryona2020AppleM1processorversion Infact,thememoryofthisMacbookisNOTupgradable.

### MacBookAir?

Howtousethefournumbers2,2,8,and8alongwithbasicarith- Theanswervaries.Anyanswerthatmeetstherequirementsisacmeticoperationstoobtain24,witheachnumberusedexactly ceptable.
once?
Thefirstfivenumbersinasequenceare2,3,6,15,and45,respec- Theanswercanbe157.5,ormorerigorously,multiplesincethisis
tively.Ifthesixthnumberhasonlyonedecimalplaceandthese- anopenquestionwithlimitedconditionsgiven.
quenceisincremented,whatmaybethesixthnumber?
WhowasthefatherofShinkansen? ShinjiSogo¯iscreditedwiththecreationofthefirst"bullettrain",
theTo¯kaido¯Shinkansen.
WhyhaveFormula1racingcarsadoptedthedesignofhalosince F1officiallyadoptedthedesignin2018,NOT2016.Itisforpro-
2016? tectingthedriversfrompotentialheaddamage.
Figure3:AnswersofthefiveselectedquestionsgeneratedbytheoriginalGPT-3.5-Turbo,theoriginalGPT-4,andtherefinedGPT-3.5
(horizontalcomparison)
Wehavechosenfiveproblemscommonlyfacedindailyhumanlife modeltoidentifyerroneousinformationwithinthequestion.Asfor
toevaluatethemodel,aspresentedinTable2.Outofthefiveques- theinferentialquestions,questions2and3havemultiplecorrectantions,questions1,4,and5arefactualandquestions2and3arein- swers.Therefore,themodelisconsideredcorrectifitrecognizesand
ferential.Ofthethreefactualquestions,questions1and5themselves providesanyorallofthesolutionsthatmeetthecriteria,withquesaresomewhatmisleading,particularlyquestion5,whichrequiresthe tion2requiringthemodeltoprovideatleastoneaccurateanswer.

<!-- Page 6 -->

Table3:Thecomprehensiveassessmentresultsprovidedbythehumanexpert.Themodelnameindicatedinthecorrespondingrowand
columnreferstothemodelwhosegeneratedanswerisacknowledgedashavingthebestperformanceonthatspecificquestionandquota.The
evaluationtargetsonlyincludethethreemodels(GPT-3.5-Turbo,GPT-4,andrefinedGPT-3.5)inFigure3.

### Quota Q1 Q2 Q3 Q4 Q5

Accuracy refinedGPT-3.5= refinedGPT-3.5 All refinedGPT-3.5 refinedGPT-3.5=

## Gpt-4 Gpt-4

Conciseness refinedGPT-3.5 refinedGPT-3.5 hardtodecide refinedGPT-3.5 refinedGPT-3.5
Completeness GPT-4 refinedGPT-3.5 GPT-3.5-Turbo refinedGPT-3.5 GPT-4
It’scrucialtoelaborateonthemeasuringapproachwehavetaken.
Tobeginwith,allthequestionsweselectedforcomparativetesting
havecorrectanswers,whichminimizestheuncertaintyofresponse
output,reducesthesubjectivityoftheevaluationprocess,andensures
thereliabilityoftheresultspresented.Inaddition,sincehumansstill
possessasuperiorunderstandingandevaluativeabilityinlanguage
andquestion-answering,thefinalevaluationofresponsequalityfor
eachquestionfromdifferentmodelsiscarriedoutbyahumanexpert.
Theassessmentofresponsequalityisbasedonthreecriteria:accuracy, conciseness, and completeness. Accuracy measures how correcttheresponsesare.Concisenessreferstowhetherthemodel’sanswerscontainasignificantamountofunnecessaryinformation.Completenessmeasureswhetherthemodelcanaddressallthekeypoints
raisedinthequestion.Thehumanexperttakesallthreeaspectsinto
considerationandprovidesacomprehensiveevaluationaccordingly.
Overall,theaforementionedexperimentscanintuitivelyillustrates
the comprehensiveness of our response optimization process, and

### Figure4:Answersofthefiveselectedquestionsgeneratedbythe

highlightsitspotentialforenhancingreal-worldlargelanguagemodtwoprunedvariationsoftherefinedGPT-3.5,i.e.theblind
elsaswellascompetingagainsttheverybest.
refinementversionandtherecklessrefinementversion(longitudinal
comparison).“NR”referstothattheprocessisessentially
equivalenttothecompleterefinedGPT-3.5framework,sothere’sno 3.2 Experimentresultsandanalysis
needforaduplicateexperiment.

### Theoutcomesfrombothphase1horizontalcomparativetestingand

phase 2 longitudinal comparative testing are presented in Figure 3
Theentireexperimentprocedureconsistsoftwomainstages.In andFigure4respectively,meanwhileTable5presentsacomprehenthe first stage (the horizontal comparison testing), we compare the sive assessment of all the response outcomes to each question, as
responsesprovidedbytheprimaryGPT-3.5-Turbo,therefinedGPT- evaluatedbythehumanexpert.
3.5 and the GPT-4 when addressing identical questions. The GPT- Tofacilitateamoreintuitivecomparisonforreaders,wehavenot
4’sresponseprocesswillremainunoptimized,enablingustoassess includedtheintermediateresultsgeneratedduringtheiterativeoptiwhetheranearlierLLMwithourenhancementprocessappliedcan mizationprocess.Ifitisnecessarytorefertotheintermediateresults
rivalitsoriginalselfaswellasthecurrentindustry-leadingmodelin during subsequent analysis, we will enumerate them at the corretermsofresponsequality.Besides,wecomparedthecomputational spondinglocation.
overheadincurredbythenativeGPT-4modelandtherefinedGPT- Basedonthecomprehensiveassessmentresultsofthehorizontal
3.5modelwhenaddressingthesamequestions,emphasizingthecost comparisontest(seeTable3),ourrefinedGPT-3.5modelachieved
benefitsofapplyingourperipheraloptimizationschemeoverusinga anastonishing100%accuracyinansweringthesequestions,aclear
moreadvancedmodelforrefinedfeedback.Inthesecondstage(the advantageoverboththeoriginalGPT-4andGPT-3.5-Turbomodels
longitudinalcomparisontesting),wecarryoutacomparativeassess- withoutanyresponseoptimizationstrategies.Inaddition,therefined
mentoftheinfluenceoftheoptimizationframework’scompleteness GPT-3.5 also surpassed all competitors regarding the answer conontheresultingresponsequality,usingthesamesetofquestions.In ciseness.Yetintermsofanswercompleteness,althoughGPT-4anspecificterms,wedesigntwosimplifiedversionsbasedontheorig- swered one question incorrectly, our refined GPT-3.5 model could
inal optimization framework. The first simplified version is called notsurpassitsadvantageindeliveringmorethoroughexplanations.
“blind refinement”, which simplifies the original guided optimiza- Notably,theanswersobtainedfromtherefinedGPT-3.5areresults
tion mechanism by allowing the model to optimize without refer- of the iterative optimization of the response refinement framework
encing any prompts in each loop. The second simplified version is proposed in this paper, built upon the initial answers provided by
named“recklessrefinement”,wherebythevotingmechanismisre- thenativeGPT-3.5-Turbo.AsitisobviousthatGPT-3.5-Turbopermoved, ensuring that the optimization process never automatically formed the worst on account of the results, this demonstrates that
stopsbeforereachingthepredeterminedmaximumnumberofitera- ourparadigmiscapableofrefiningtheaccuracyandredundancyof
tions.Thegoalofthisstageistoobserveandanalyzetheinfluence theinitialanswerstoproducenewoneswithhigherquality.Furtherofvariousmoduleswithintheframeworkonoptimizationoutcomes, more,althoughrefinedGPT-3.5mayrequiremoretimeforiteration,
underscoringthesignificanceofmoduleintegration. but its total computational cost is much lower than that of GPT-4.

<!-- Page 7 -->

Evenwhenusingthesamenumberoftokens,refinedGPT-3.5with 3.5-Turbo.Ourexperimentsshowthatouroptimizationmechanism
amaximumiterationlimitof3cansave5to10timestheAPIusage enablesaless-capablemodeltoachieveresponsequalitybetterthan
consumption, thus greatly reducing the economic burden on users. itsoriginalself,andevenonparwithoneofthebestcurrentmod-
The reason for the lower computational cost is not only due to the elswhilereducingresourceconsumption.Throughthisscheme,we
useofamoreeconomicalmodel,butalsobecauseofthecontribution demonstratethatinmanysituations,theexistingquestion-answering
ofthefirst-ordermemoryoftheoptimizationframeworkmentioned interactionparadigmmaynotfullyharnessthepotentialofgeneraearlier,sincesuchoperationmodedoesnotexponentiallyaccumulate tivelanguagemodels.Appropriatelydesigningpromptsandplanning
thenumberoftokens,andtheresourcecostofeachiterationcanbe responseinteractionlogicisacrucialapproachtofurtherunleasha
consideredalmostconstant. model’spotential.

### The organic combination of each module in the optimization

framework is also essential for obtaining high-quality responses.

### References


### FromtheanswersprovidedbyvariantsofrefinedGPT-3.5thathave

removedonekeymoduleforeachquestion(asshowninFigure4),it [1] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit,
LlionJones,AidanN.Gomez,LukaszKaiser,andIlliaPolosukhin.Atcanbeseenthatbothweakeningthepurposeofoptimization(guided
tentionisallyouneed.ArXiv,abs/1706.03762,2017.
optimizationmechanism)andremovingtheabilityofself-reviewand
[2] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.
self-stop(votingmechanism)significantlyreduceoptimizationcapa- Bert:Pre-trainingofdeepbidirectionaltransformersforlanguageunbilitiesandevenresultinnegativeoptimization. derstanding.ArXiv,abs/1810.04805,2019.
Wefoundsomepivotalcluesbasedontheintermediateresultsto [3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,

### JaredKaplan,PrafullaDhariwal,ArvindNeelakantan,PranavShyam,

explain the reasons for the shortcomings. As an example of the it-
GirishSastry,AmandaAskell,SandhiniAgarwal,ArielHerbert-Voss,
erativeoptimizationprocessforthesecondquestionusingtheblind Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh,
refinementframework,theinitialresponsegivenbythenativeGPT- Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse,
3.5-Turbocontainednumerousincorrectstepsandanincorrectequa- MarkChen,EricSigler,MateuszLitwin,ScottGray,BenjaminChess,
JackClark,ChristopherBerner,SamMcCandlish,AlecRadford,Ilya
tion:"(8×8)+2÷2−9=24".Thisanswerisproblematicbecause
Sutskever,andDarioAmodei.Languagemodelsarefew-shotlearners.
theoriginalquestiondoesnotincludethenumber9,doesnotallow
ArXiv,abs/2005.14165,2020.
foranynumbersotherthan8,8,2,2tobeusedinthecalculation,and [4] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan
theequationonbothsidesoftheequalssignisactuallynotequiva- Salakhutdinov,andQuocV.Le.Xlnet:Generalizedautoregressiveprelent. After the first round of iteration, the blind refinement mech- trainingforlanguageunderstanding.InNeuralInformationProcessing
Systems,2019.
anism removed only the cumbersome steps, keeping the erroneous
[5] AlexisConneau,KartikayKhandelwal,NamanGoyal,VishravChaudequationasthenewresponse,whichwasstillclearlyincorrect.Even hary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle
afterthesecondandthirdroundsofiteration,thisincorrectequation Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised crossremainedamajorpartoftheanswer,resultinginafinaloptimizedso- lingualrepresentationlearningatscale.InAnnualMeetingoftheAssociationforComputationalLinguistics,2019.
lutionthatwasstillincorrect.Thissuggeststhatallowingthemodel
[6] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774,
tooptimizewithoutaclearpurposecanresultinthemodelrepeat-
2023.
edlyrewritingtheanswerwithoutactuallyimprovingit.Atthesame [7] Alec Radford and Karthik Narasimhan. Improving language undertime, in the example of the reckless refinement framework that re- standingbygenerativepre-training.2018.
movesthevotingmoduletodealwiththesamequestion,themodel [8] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and
ShmargaretShmitchell. Onthedangersofstochasticparrots:Canlanprovidedacorrectanswerbytheendoftheseconditeration,butunguagemodelsbetoobig?Proceedingsofthe2021ACMConferenceon
expectedlyproducedanincorrectanswerbythethirditeration.The Fairness,Accountability,andTransparency,2021.
lackofself-reviewandself-stopmechanismsmeansthatthemodel [9] YizheZhang,SiqiSun,MichelGalley,Yen-ChunChen,ChrisBrockcontinuedtooptimizebeyondthepointwhereithadalreadyobtained ett,XiangGao,JianfengGao,JingjingLiu,andWilliamB.Dolan. Dialogpt:Large-scalegenerativepre-trainingforconversationalresponse
thecorrectanswer,andasaresult,endedupoutputtinganincorrect
generation. InAnnualMeetingoftheAssociationforComputational
answerthatwasgeneratedinthefinalroundofoptimization.Bothof
Linguistics,2019.
thesetypicalexamplesservetounderscoretheintegralrolesplayed [10] AlexWang,YadaPruksachatkun,NikitaNangia,AmanpreetSingh,Jubythesetwomodulesthroughouttheoptimizationprocess. lianMichael,FelixHill,OmerLevy,andSamuelR.Bowman. Super-
Tosumup,Wehavesubstantiatedthroughtheconclusionsofthe glue:Astickierbenchmarkforgeneral-purposelanguageunderstanding
systems.ArXiv,abs/1905.00537,2019.
above experiments that using the iterative optimization framework
[11] Paul Francis Christiano, Jan Leike, Tom B. Brown, Miljan Martic,
proposed in this paper on a large language model can significantly Shane Legg, and Dario Amodei. Deep reinforcement learning from
enhancethequalityofthegeneratedanswersatalowercostFurther- humanpreferences.ArXiv,abs/1706.03741,2017.
more,ithighlightsthecontributionsmadebytheguidedoptimization [12] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina
mechanismandthevotingmechanisminimprovingoptimizationca-
Slama,AlexRay,JohnSchulman,JacobHilton,FraserKelton,LukeE.
pabilities.
Miller,MaddieSimens,AmandaAskell,PeterWelinder,PaulFrancis

### Christiano,JanLeike,andRyanJ.Lowe. Traininglanguagemodels

tofollowinstructionswithhumanfeedback. ArXiv,abs/2203.02155,
4 Conclusion
2022.
[13] JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,EdHuai
Inthispaper,weintroduceafully-autonomousadaptiveiterativere- hsinChi,F.Xia,QuocLe,andDennyZhou.Chainofthoughtpromptsponseoptimizationparadigm,inspiredbyconceptsfromRLHFand ingelicitsreasoninginlargelanguagemodels.ArXiv,abs/2201.11903,
thechainofthought.Thisapproachreliessolelyonsimpleprompt 2022.
engineeringandtheLLMAPI,withouttheneedformanualintervention,auxiliarymodels,oraccesstointernalstructuresandparameters
oflanguagemodels.Specifically,wepresentadetailedoptimization
frameworkutilizinganefficientmodulardesign,appliedtotheGPT-