---
title: "Advanced RAG Techniques"
original_file: "./Advanced_RAG_Techniques.pdf"
document_type: "research"
conversion_date: "2025-11-29"
topics: ["prompt-engineering", "llm", "rag", "chain-of-thought", "agents"]
keywords: ["cid", "retrieval", "vol", "gpt", "page", "article", "publicationdate", "rag", "arxivabs", "associationforcomputationallinguistics"]
summary: "<!-- Page 1 -->

The Survey of Retrieval-Augmented Text Generation in Large

### Language Models


### YIZHENGHUANGandJIMMYX.HUANG,YorkUniversity,Canada

Retrieval-AugmentedGeneration(RAG)mergesretrievalmethodswithdeeplearningadvancementstoaddress
thestaticlimitationsoflargelanguagemodels(LLMs)byenablingthedynamicintegrationofup-to-date
externalinformation.Thismethodology,focusingprimarilyonthetextdomain,providesacost-effective
solutiontothegenerationofplausiblebutpossiblyincorrectresponsesbyLLM"
related_documents: []
---

# Advanced RAG Techniques

<!-- Page 1 -->

The Survey of Retrieval-Augmented Text Generation in Large

### Language Models


### YIZHENGHUANGandJIMMYX.HUANG,YorkUniversity,Canada

Retrieval-AugmentedGeneration(RAG)mergesretrievalmethodswithdeeplearningadvancementstoaddress
thestaticlimitationsoflargelanguagemodels(LLMs)byenablingthedynamicintegrationofup-to-date
externalinformation.Thismethodology,focusingprimarilyonthetextdomain,providesacost-effective
solutiontothegenerationofplausiblebutpossiblyincorrectresponsesbyLLMs,therebyenhancingthe
accuracyandreliabilityoftheiroutputsthroughtheuseofreal-worlddata.AsRAGgrowsincomplexityand
incorporatesmultipleconceptsthatcaninfluenceitsperformance,thispaperorganizestheRAGparadigminto
fourcategories:pre-retrieval,retrieval,post-retrieval,andgeneration,offeringadetailedperspectivefromthe
retrievalviewpoint.ItoutlinesRAGâ€™smechanicsanddiscussesthefieldâ€™sprogressionthroughtheanalysisof
significantstudies.Additionally,thepaperintroducesevaluationmethodsforRAG,addressingthechallenges
facedandproposingfutureresearchdirections.Byofferinganorganizedframeworkandcategorization,the
studyaimstoconsolidateexistingresearchonRAG,clarifyitstechnologicalunderpinnings,andhighlightits
potentialtobroadentheadaptabilityandapplicationsofLLMs.
CCSConcepts:â€¢Computingmethodologiesâ†’Naturallanguagegeneration;â€¢Informationsystems
â†’Informationretrieval.
AdditionalKeyWordsandPhrases:retrieval-augmentedgeneration,informationretrieval,largelanguage
model

### ACMReferenceFormat:

YizhengHuangandJimmyX.Huang.2018.TheSurveyofRetrieval-AugmentedTextGenerationinLarge
LanguageModels.InProceedingsofMakesuretoenterthecorrectconferencetitlefromyourrightsconfirmation
emai(Conferenceacronymâ€™XX).ACM,NewYork,NY,USA,37pages.https://doi.org/XXXXXXX.XXXXXXX
1 Introduction
TheadventofChatGPThassignificantlyimpactedbothacademiaandindustryduetoitsinteractive
capabilitiesandwidespreadapplication,establishingitselfasaleadingartificialintelligencetool
[55, 60, 77]. At the core of ChatGPT is the large language model (LLM) GPT-4, as detailed by
[1],whichhasseennumerousenhancementstoitspredecessors,showcasingexceptionalabilities
inavarietyofNaturalLanguageProcessing(NLP)tasks[78].Despitetheseadvancements,the
adoptionofLLMshashighlightedseveralcriticalissuesprimarilyduetotheirrelianceonextensive
datasets.Thisreliancerestrictstheirabilitytoincorporatenewinformationpost-training,leading
tothreeprimarychallenges.First,thefocusonbroadandgeneraldatatomaximizeaccessibilityand
applicabilityresultsinsubparperformanceinspecializedareas.Second,therapidcreationofonline
data,combinedwiththesignificantresourcesrequiredfordataannotationandmodeltraining,
Authorsâ€™ContactInformation:YizhengHuang,hyz@yorku.ca;JimmyX.Huang,jhuang@yorku.ca,YorkUniversity,Toronto,
Ontario,Canada.
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalorclassroomuseisgrantedwithoutfee
providedthatcopiesarenotmadeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeandthe
fullcitationonthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthantheauthor(s)mustbehonored.
Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,requires
priorspecificpermissionand/orafee.Requestpermissionsfrompermissions@acm.org.

### Conferenceacronymâ€™XX,June03â€“05,2018,Woodstock,NY

Â©2018Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.

## Acmisbn978-1-4503-Xxxx-X/18/06

https://doi.org/XXXXXXX.XXXXXXX
,Vol.1,No.1,Article.Publicationdate:August2018.
4202
guA
32
]RI.sc[
2v18901.4042:viXra

<!-- Page 2 -->

2 Huangetal.
Fig.1. AnexampleofRAGbenefitsChatGPTresolvesquestionsthatcannotbeansweredbeyondthescope
ofthetrainingdataandgeneratescorrectresults.
hindersLLMsâ€™abilitytostayupdated.Third,LLMsaresusceptibletogeneratingconvincingyet
inaccurateresponses,knownasâ€œhallucinationsâ€,whichcanmisleadusers.
AddressingthesechallengesiscrucialforLLMstobeeffectivelyutilizedacrossvariousdomains.
ApromisingsolutionistheintegrationofRetrieval-AugmentedGeneration(RAG)technology,
whichsupplementsmodelsbyfetchingexternaldatainresponsetoqueries,thusensuringmore
accurateandcurrentoutputs.Figure1illustrateshowRAGcanenableChatGPTtoprovideprecise
answersbeyonditsinitialtrainingdata.
SinceitsintroductionbyLewisetal.[83]in2020,RAGhasseenrapiddevelopment,especially
withtheriseofmodelslikeChatGPT.Despitetheseadvancements,thereremainsanoticeablegap
intheliteratureregardingacomprehensiveanalysisofthemechanismsunderlyingRAGandthe
progressachievedbysubsequentstudies.Moreover,thefieldsuffersfromfragmentedresearch
focusesandinconsistentterminologyforsimilarmethods,leadingtoconfusion.Thissurveyseeks
tobridgethisgapbyofferingastructuredoverviewofRAG,categorizingvariousapproaches,and
providinganin-depthunderstandingofthecurrentresearchlandscape,withafocusontextual
applicationsgiventheirprominenceinrecentresearch.
To provide clarity and structure, this paper is organized as follows: Section 2 outlines the
overallRAGworkflow,dividingthemethodologiesintopre-retrieval,retrieval,post-retrieval,and
generationphases.Sections3through6explorethecoretechniqueswithineachphase.Section
7focusesontheevaluationmethodologiesforRAG.Section8summarizesthereviewedstudies,
detailingtheretrieversandgeneratorsused,whileSection9discusseschallengesandfutureresearch
directions,extendingbeyondtext-basedstudiestoincludemultimodaldataapplications.Thepaper
concludeswithSection10.
OtherrelatedsurveysprovidevaluableinsightsintotheevolvingRAGlandscapefromdifferent
angles.Gaoetal.[38]identifiedthreekeystagesinRAGdevelopment:pre-trainingenhancement,
inference,andfine-tuning.Zhaoetal.[162]focusedonthediverseapplicationsofRAG,including
text,code,image,andvideogeneration,emphasizingaugmentedintelligenceingenerativetasks.
Meanwhile,Huetal.[48]exploredRetrieval-AugmentedLanguageModels(RALMs),examininghow
interactionsbetweenretrievers,languagemodels,andaugmentationsinfluencemodelarchitectures
andapplications.
Inthispaper,weaimtoofferacomprehensiveandunifiedframeworkforunderstandingRAGfrom
aninformationretrieval(IR)perspective,identifyingkeychallengesandareasforimprovement.We
delveintothecoretechnologiesthatdriveRAG,assessingtheireffectivenessinaddressingretrieval
andgenerationtasks.Additionally,thissurveyintroducestheevaluationmethodsemployedinRAG
research,highlightscurrentlimitations,andproposespromisingavenuesforfutureexploration.
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 3 -->

TheSurveyofRetrieval-AugmentedTextGenerationinLargeLanguageModels 3
Fig.2. TheunifiedRAGcoreconceptswithbasicworkflow.
2 RAGFramework
ThehallucinationsarelargelyattributedtoLLMsâ€™inabilitytoaccessup-to-dateinformation.This
limitationstemsfromthemodelsâ€™relianceontheirtrainingdatasets.RAGproposesasolutionto
thisissuebysupplementingtheLLMâ€™strainingdatawithcurrentinformationfromexternalsources
througharetrievalmodel,therebyenablingthegenerationofaccurateresponses.RAGpresentsa
morecost-effectivealternativetotheextensivetrainingandfine-tuningprocessestypicallyrequired
forLLMs.Itallowsforthedynamicincorporationoffreshinformationviatraditionalretrieval
methodsorpre-trainedLMs,withouttheneedtodirectlyintegratethisnewdataintotheLLM.This
featuremakesRAGbothflexibleandscalable,facilitatingitsapplicationacrossdifferentLLMsfor
variouspurposes.TheinformationretrievedthroughRAGisderivedfromreal-worlddata,authored
byhumans,whichnotonlysimplifiesthegenerationprocessbutalsoincreasesthereliabilityof
thegeneratedresponses.
ResearchbyKhandelwaletal.[72]demonstratesthataccessingrelevantinformationfromthe
trainingdatasetitselfcansignificantlyimproveLLMperformance,highlightingtheeffectivenessof
RAG.Overtime,RAGhasevolvedfromameansofprovidingsupplementaryinformationtoenabling
multipleinteractionsbetweentheretrievalandgenerationcomponents.Thisinvolvesconducting
several rounds of retrieval to refine the accuracy of the information retrieved and iteratively
improvethequalityofthegeneratedoutput.ToolkitssuchasLangChain1andLlamaIndex2have
modularizedtheRAGapproach,enhancingitsadaptabilityandexpandingitsrangeofapplications.
DespitethesetoolkitsemployingdiversemethodologiestotackledifferentaspectsofRAGâ€”from
multiplesearchiterationstoiterativegenerationâ€”theymaintainadherencetothefundamental
RAG workflow. This consistency is crucial for understanding their operation and pinpointing
opportunitiesforfurtherdevelopment.
2.1 BasicRAGWorkflow
Figure2representstheunifiedRAGcoreconceptswithbasicworkflow.TheworkflowofRAG
beginswiththecreationofanindexcomprisingexternalsources.Thisindexservesasthebasisfor
retrievingrelevantinformationthrougharetrievermodelbasedonaspecificquery.Thefinalstep
1https://www.langchain.com
2https://www.llamaindex.ai
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 4 -->

4 Huangetal.
involvesageneratormodel,whichcombinestheretrievedinformationwiththequerytoproduce
thedesiredoutput.
2.1.1 Indexing. Efficientretrievalbeginswithcomprehensiveindexing,wheredatapreparation
is key. This stage involves text normalization processes such as tokenization, stemming, and
theremovalofstopwordstoenhancethetextâ€™ssuitabilityforindexing[98].Textsegmentsare
then organized into sentences or paragraphs to facilitate more focused searches, allowing for
the pinpointing of segments containing pertinent keywords. The integration of deep learning
hasrevolutionizedindexingthroughtheuseofpretrainedLMsforgeneratingsemanticvector
representationsoftexts.Thesevectorsarestored,enablingrapidandpreciseretrievalfromextensive
datacollections,significantlyenhancingretrievalefficiency.
2.1.2 Retrieval. Whiletraditionalretrievalmethods,suchastheBM25algorithm[44],focuson
termfrequencyandpresencefordocumentranking,theyoftenoverlookthesemanticinformation
ofqueries.CurrentstrategiesleveragepretrainedLMslikeBERT[29],whichcapturethesemantic
essenceofqueriesmoreeffectively.Thesemodelsimprovesearchaccuracybyconsideringsynonyms
andthestructureofphrases,therebyrefiningdocumentrankingthroughthedetectionofsemantic
similarities. This is typically achieved by measuring vector distances between documents and
queries,combiningtraditionalretrievalmetricswithsemanticunderstandingtoyieldsearchresults
thatarebothrelevantandalignedwithuserintent.
2.1.3 Generation. Thegenerationphaseistaskedwithproducingtextthatisbothrelevantto
thequeryandreflectiveoftheinformationfoundintheretrieveddocuments.Theusualmethod
involvesconcatenatingthequerywiththeretrievedinformation,whichisthenfedintoanLLM
for text generation [85]. Although ensuring the generated textâ€™s alignment and accuracy with
theretrievedcontentpresentschallenges,itisalsoessentialtostrikeabalancebetweenadhering
closelytothesourcematerialandinfusingtheoutputwithcreativity.Thegeneratedtextshould
accuratelyconveytheinformationfromtheretrieveddocumentsandalignwiththequeryâ€™sintent,
whilealsoofferingtheflexibilitytointroducenewinsightsorperspectivesnotexplicitlycontained
withintheretrieveddata.
2.2 RAGParadigm
TheRAGparadigmorganizesresearchwithinthedomain,offeringastraightforwardyetrobust
framework to enhance LLM performance. Central to RAG is its search mechanism, crucial for
generatinghigh-qualityoutcomes.Therefore,thisparadigmisstructuredintofourmainphases
fromaretrievalperspective:pre-retrieval,retrieval,post-retrieval,andgeneration.Bothsingle-hop
andmulti-hopretrievalapproaches,encompassingiterativeretrieve-generatecycles,followthis
four-phasestructure.Figure3isthetaxonomytreeofRAGâ€™scoretechniques.
2.2.1 Pre-Retrieval. Thepre-retrievalphaseofretrieval-augmentedgenerationlaysthefoundation
for successful data and query preparation, ensuring efficient information retrieval. This phase
includesessentialtaskstoprepareforeffectivedataaccess.
Indexing. Theprocessstartswithindexing,whichestablishesanorganizedsystemtoenablefast
andaccurateretrievalofinformation.Thespecificityofindexingdependsonthetaskanddata
type.Forexample,sentence-levelindexingisbeneficialforquestion-answeringsystemstoprecisely
locateanswers,whiledocument-levelindexingismoreappropriateforsummarizingdocumentsto
understandtheirmainconceptsandideas.
QueryManipulation. Afterindexing,querymanipulationisperformedtoadjustuserqueriesfor
abettermatchwiththeindexeddata.Thisinvolvesqueryreformulation[61,155],whichrewrites
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 5 -->

TheSurveyofRetrieval-AugmentedTextGenerationinLargeLanguageModels 5
thequerytoalignmorecloselywiththeuserâ€™sintention;queryexpansion[51],whichextendsthe
querytocapturemorerelevantresultsthroughsynonymsorrelatedterms;andquerynormalization,
whichresolvesdifferencesinspellingorterminologyforconsistentquerymatching.
DataModification. Datamodificationisalsocriticalinenhancingretrievalefficiency.Thisstep
includespreprocessingtechniqueslikeremovingirrelevantorredundantinformationtoimprove
thequalityofresultsandenrichingthedatawithadditionalinformationsuchasmetadatatoboost
therelevanceanddiversityoftheretrievedcontent[6].
2.2.2 Retrieval.
Search&Ranking. Theretrievalstageisthecombinationofsearchandranking.Itfocuseson
selecting and prioritizing documents from a dataset to enhance the quality of the generation
modelâ€™s outputs. This stage employs search algorithms to navigate through the indexed data,
findingdocumentsthatmatchauserâ€™squery.Afteridentifyingrelevantdocuments,theprocessof
initiallyrankingthesedocumentsstartstosortthemaccordingtotheirrelevancetothequery.
2.2.3 Post-Retrieval. Thepost-retrievalphaseservestorefinetheinitiallyretrieveddocumentsto
improvethequalityoftextgeneration.Thisphaseconsistsofre-rankingandfiltering,eachaimed
atoptimizingthedocumentselectionforthefinalgenerationtask.
Re-Ranking. Inthere-rankingstep,thedocumentspreviouslyretrievedarereassessed,scored,
andreorganized.Theobjectiveistomoreaccuratelyhighlightthedocumentsmostrelevantto
thequeryanddiminishtheimportanceofthelessrelevantones.Thisstepinvolvesincorporating
additionalmetricsandexternalknowledgesourcestoenhanceprecision.Inthiscontext,pre-trained
modelswithsuperioraccuracybutlowerefficiencycanbeeffectivelyemployedduetothelimited
setofcandidatedocumentsavailable[54].
Filtering. Filteringaimstoremovedocumentsthatfailtomeetspecifiedqualityorrelevance
standards[56,74].Thiscanbedonethroughseveralapproaches,suchasestablishingaminimum
relevancescorethresholdtoexcludedocumentsbelowacertainrelevancelevel.Furthermore,the
useoffeedbackfromusersorpriorrelevanceevaluationsassistsinadjustingthefilteringprocess,
guaranteeingthatonlythemostrelevantdocumentsareretainedfortextgeneration.
2.2.4 Generation. The generation stage is a crucial component of the RAG process, responsibleforleveragingretrievedinformationtoenhancethequalityofthegeneratedresponse.This
stageencompassesseveralsub-stepsaimedatproducingcontentthatisreadable,engaging,and
informative.
Enhancing. Attheheartofthegenerationphaseistheenhancementstep,wheretheobjectiveis
tomergetheretrievedinformationwiththeuserâ€™squerytocreateacoherentandrelevantresponse.
Thisincludestheprocessofelaboration,addingextradetailstotheretrievedcontenttoenrich
it.Effortsarefocusedonimprovingtheoutputâ€™squalitybyincreasingitsclarity,coherence,and
stylisticappealthroughmethodssuchasrephrasingandrestructuring.Informationfromvarious
sourcesiscombinedtoofferacomprehensiveperspective,andverificationisconductedtoensure
theaccuracyandrelevanceofthecontent.
Customization. Customizationisuser-centric.Itencompassestailoringcontentintwoprimary
ways.First,italignsthegeneratedoutputwithrelevantinformationretrievedinearlierstages,
ensuringconsistencyandaccuracybyincorporatingkeyknowledge.Second,itadaptsthecontent
tosuituser-specificfactorssuchasintendedaudience,situationalcontext,andpersonalpreferences,shapingtheresponsetobebothcontextuallyrelevantanduser-centric.Thisdualfocuson
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 6 -->

6 Huangetal.

## Rag


### Pre-Retrieval Retrieval Post-Retrieval Generation

Indexing QueryMa- DataModification Search&Ranking Re-Ranking Filtering Enhancing Customization
nipulation
REALM[42]; Webgpt[100]; RA-DIT[89]; REALM[42]; Re2G[40]; Webgpt[100]; FiD[58]; LAPDOG[52];
kNN-LMs[72]; DSP[73]; RECITE[125]; kNN-LMs[72]; DSP[73]; Self-RAG[4]; Webgpt[100]; PersonaRAG[160];
RAG[83]; CoK[86]; UPRISE[20]; RAG[83]; CoK[86]; FiD-TF[5]; DSP[73]; ERAGent[123];
Webgpt[100]; IRCOT[131]; GENREAD[156]; FiD[58]; FiD-TF[5]; PROMPTAGATOR[27]; IRCOT[131]; ROPG[118]
RETRO[9]; Query2doc[137]; KnowledGPT[140]; Webgpt[100]; ITER-RETGEN[121]; RECOMP[147]; ITRG[34];
MEMWALKER[13]; Step-Back[163]; Selfmem[21]; RETRO[9]; PROMPTAGATOR[27]; DKS-RAC[53]; RA-DIT[89];
Atlas[94]; PROMPTAGATOR[27]; RARG[159] ITRG[34]; Selfmem[21]; CoK[86]; PRCA[151];
Chameleon[63]; KnowledGPT[140]; RA-DIT[89]; DKS-RAC[53]; FILCO[141]; RECITE[125];
AiSAQ[126]; Rewrite-Retrieve- SURGE[70]; In-Context BlendFilter[135]; UPRISE[20];
PipeRAG[64]; Read[94]; PRCA[151]; RALM[112]; CRAG[149] GENREAD[156];
LRUS-CoverTree[93] FLARE[65]; AAR[157]; Fid-light[47]; Selfmem[21];
RQ-RAG[12]; ITER-RETGEN[121]; GenRT[148] MEMWALKER[13];
RARG[159]; UPRISE[20]; Atlas[94]

## Dragin[124] Memwalker[13];

Atlas[94];

## Flare[65];


### PlanRAG[81]


### Fig.3. TaxonomytreeofRAGâ€™scoretechniques

integratingrelevantknowledgeandadjustingtodiversecontextualdemandsformsthebasisof
effectivecustomizationinRAG.
3 Pre-Retrieval
3.1 Indexing
Oneofthemostcommonlyusedindexingstructuresintraditionalinformationretrievalsystems
istheinvertedindex.Thisstructureassociatesdocumentswithwordstoformavocabularylist,
allowinguserstoquicklylocatereferenceswhereaspecificwordappearswithinacollectionof
documents.Thevocabularylistherereferstothesetofalluniquewordspresentinthedocument
collection,whilethereferenceincludesthedocumentswherethewordappears,alongwiththe
wordâ€™s position and weight within those documents. However, traditional indexing structures
struggletoretrievedocumentsthataresemanticallyrelatedtoauserâ€™squerybutdonotcontain
theexactqueryterms.
Toaddressthislimitation,retrievalmethodsusingdensevectorsgeneratedbydeeplearning
models have become the preferred choice. These vectors, also known as embeddings, capture
thesemanticmeaningofwordsanddocuments,allowingformoreflexibleandaccurateretrieval.
Densevector-basedindexingmethodscanbecategorizedintothreemaintypes:graphs,product
quantization(PQ)[62],andlocality-sensitivehashing(LSH)[28].Sincegeneratingdensevectors
withlargelanguagemodelsrequiressubstantialresources,andthedocumentcollectionstobe
searchedaretypicallyvast,thecorestrategyoftheseindexingmethodsisbasedonapproximate
nearestneighborsearch(ANNS)[3].Thisapproachsignificantlyspeedsupthesearchprocessat
thecostofaslightreductioninsearchaccuracy.
Graph. UsinggraphstobuildindexesisacommonpracticeinRAG.Byindexingvectorswith
a graph structure, the range of nodes where distances need to be computed during retrieval
canbelimitedtoalocalsubgraph,therebyenhancingsearchspeed.Severalprominentmethods
andtoolshavebeendevelopedusingthisapproach.Forexample,k-nearestneighborlanguage
modelskNN-LMs[72],asdemonstratedbyKhandelwaletal.,integratethekNNalgorithmwith
pre-trainedlanguagemodels.Thismethodemploysadatastorecreatedfromcollectionsoftexts
todynamicallyretrievecontextuallyrelevantexamples,enhancingmodelperformancewithout
requiringadditionaltraining.FAISS[68],atoolwidelyadoptedforindexinginmanystudies[72,73,
83],integratesenhancementsliketheHierarchicalNavigableSmallWorld(HNSW)approximation
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 7 -->

TheSurveyofRetrieval-AugmentedTextGenerationinLargeLanguageModels 7
[97]tofurtherspeedupretrieval[83].WebGPT[100]showcasesanotherpracticalapplicationby
utilizingtheBingAPI3 forindexingbasedonactualusersearchhistories,whichillustratesthe
potentialofintegratingreal-worlduserdataintotheretrievalprocess.Additionally,othermethods
likeMEMWALKER[13]introducesinnovativeapproachestoovercomelimitationssuchascontext
windowsizeinlargelanguagemodels.Itcreatesamemorytreefrominputtext,segmentingthe
textintosmallerpiecesandsummarizingthesesegmentsintoahierarchicalstructure.Moreover,
LRUS-CoverTree method [93] designed another tree structure for k-Maximum Inner-Product
Search(k-MIPS)andachievesperformancecomparablewithsignificantlylowerindexconstruction
time.Thesetechniquesfacilitateefficientindexingandmanagementoflargeinformationvolumes,
demonstratingtheversatilityandeffectivenessofgraph-basedapproaches.
ProductQuantization. PQisoneofthemostrepresentativemethodsforhandlinglarge-scaledata.
Itacceleratessearchesbysegmentingvectorsandthenclusteringeachpartforquantization.Unlike
graph-basedmethods,whichspeedupsearchesbyreducingthenumberofvectorsfordistance
calculation,PQachievesfastersearchesbyreducingthetimespentoncalculatingworddistances.
SeveralimplementationsofPQhaveemergedinRAG,eachimprovingitsefficiencyandscalability
indifferentways.PipeRAG[64]integratesPQwithinapipeline-parallelismframeworktoenhance
retrieval-augmentedgenerationbyoptimizingretrievalintervals.Chameleonsystem[63]leverages
PQinadisaggregatedacceleratorenvironmenttobalancememoryusageandretrievalspeedinRAG
tasks.AiSAQ[126]introducesanall-in-storageANNSmethodthatoffloadsPQvectorsfromDRAM
tostorage,drasticallyreducingmemoryusagewhilemaintaininghighrecall.Itdemonstratesthat
evenwithbillion-scaledatasets,memoryusagecanbeminimizedtoaround10MBwithonlyminor
latencyincreases,makingitahighlyscalablesolutionforRAGsystems.
Locality-sensitiveHashing. ThecoreideaofLSHistoplacesimilarvectorsintothesamehash
bucketwithhighprobability.LSHuseshashfunctionsthatmapsimilarvectorstothesameor
nearbyhashvalues,makingiteasiertofindapproximatenearestneighbours.InLSH,whenaquery
vectorishashed,thesystemquicklyretrievescandidatevectorsthatsharethesamehashvalue.
Thismethodreducesthedimensionalityoftheproblemandcanbeimplementedefficiently,butit
mayintroducesomeinaccuraciesduetothehashingprocessitself.WhileLSHisrarelyusedinRAG
systemscomparedtograph-basedandPQmethods,itstilloffersausefulapproachinscenarios
wherespeedisprioritizedovertheslightlossinaccuracy.
3.2 QueryManipulation
QuerymanipulationispivotalinenhancingtheeffectivenessandaccuracyofmodernIRsystems.By
refiningusersâ€™originalqueries,itaddresseschallengessuchasambiguousphrasingandvocabulary
mismatchesbetweenthequeryandtargetdocuments.Thisprocessinvolvesmorethanmerely
replacingwordswithsynonyms;itrequiresadeepunderstandingofuserintentandthecontext
ofthequery,particularlyincomplextaskslikeRAG.Effectivequerymanipulationsignificantly
boostsretrievalperformance,whichinturncangreatlyimpactthequalityofgeneratedoutputs.
Thethreeprimaryapproachestoquerymanipulationarequeryexpansion,queryreformulation,
andprompt-basedrewriting.
Query Expansion. Query expansion involves augmenting the original query with additional
terms or phrases that are related to or synonymous with the query terms. In the context of
LLMs,queryexpansioncanbemoresophisticated,utilizingthemodelâ€™sextensiveknowledgeto
generatecontextuallyrelevantexpansions.Thistechniqueaimstoimproverecallbyensuringthat
theretrievalprocesscapturesabroaderrangeofrelevantdocuments,accommodatingdifferent
3https://www.microsoft.com/en-us/bing/apis/bing-web-search-api
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 8 -->

8 Huangetal.
terminologies or expressions. Techniques such as synonym expansion, semantic similarity, or
leveragingexternalknowledgebasesarecommonlyemployedinqueryexpansion.Forexample,
the method described in FiD [58] expands the query by retrieving a wider range of passages
usingbothsparseanddenseretrievaltechniques,enablingthemodeltoaggregateevidencefrom
multiplesourcesandtherebyimprovingtheaccuracyandrobustnessofgeneratedanswers.Amore
advancedformofqueryexpansionisdemonstratedinQuery2doc[137],wherepseudo-documents
generatedbyLLMsenhancetheoriginalquery,effectivelybridgingthegapbetweentheuserâ€™s
inputandthecorpusinformation,whichbenefitsbothsparseanddenseretrievalsystems.Similarly,
KnowledGPT [140] broadens the scope of information accessed during retrieval by leveraging
externalknowledgebases,furtherrefiningtheretrievalprocess.TheRARG[159]frameworkuses
anevidence-drivenqueryexpansionapproach,incorporatingawidearrayofsupportingdocuments
togenerateinformedandaccuratecounter-misinformationresponses.
QueryReformulation. Queryreformulationinvolvesrephrasingorrestructuringtheoriginal
querytoenhanceitseffectiveness.Thismightincludemakingthewordingmorespecific,removing
vague terms, or adjusting the syntax to better align with the retrieval systemâ€™s requirements.
WithLLMs,queryreformulationcanbedynamicallydrivenbyunderstandingtheuserâ€™sintent
andcontext,allowingformoreprecisemodificationsthatleadtoimprovedretrievalresults.This
reformulationprocesscanalsobeinformedbypastqueriesoruserinteractions,adaptingthequery
tobetterfitthespecificretrievaltask.Forinstance,theRQ-RAG[12]modelrepresentsanadvanced
formofqueryreformulationbyrewriting,decomposing,anddisambiguatingqueries,makingit
particularlyeffectiveinscenariosthatdemandcomplexqueryhandling.Thisapproachensures
thattherefinedquerybettermatchestheneededcontext,improvingtherelevanceofretrieved
information. Rewrite-Retrieve-Read framework [94] adjusts the original query to optimize the
retrievalprocess,allowingthesystemtomoreeffectivelyleverageretrieveddataforgenerating
accurateresponses.Additionally,FLARE[65]exemplifiesqueryreformulationthroughitsactive
retrieval-augmentedgenerationapproach,whichiterativelyrefinesthequerybasedonafeedback
loop between retrieval and generation, thereby enhancing the accuracy and relevance of the
retrievedinformation.
Prompt-basedRewriting. Prompt-basedrewriting,particularlyinthecontextofLLMs,represents
aninnovativeapproachwheretheoriginalqueryisembeddedwithinalargerpromptorcontext
to guide the LLMâ€™s response. This technique harnesses the modelâ€™s ability to understand and
generatelanguagewithinaspecificcontext,effectivelyrewritingthequerytoalignwiththedesired
output.Prompt-basedrewritingisespeciallypowerfulinscenarioswheretheretrievalprocessis
integratedintoagenerativeworkflow,allowingthesystemtoadaptthequerytovariousstagesof
retrievalandgeneration.Thisapproachmayalsoinvolvedynamicpromptsthatevolvebasedon
interaction,furtherrefiningtheretrievalprocess.Forexample,Step-Back[163]refinesthequery
contextthroughcarefullycraftedpromptsthatguidetheLLMâ€™sreasoningprocess,ensuringthatthe
outputsaremorealignedwiththeuserâ€™sintent,particularlyincomplexreasoningtasks.TheCoK
[86]methodfocusesondynamicallyadaptingtheknowledgesourceandusingpromptstorewrite
thecontextinwhichaqueryisinterpreted.Thisapproachleveragesprompt-basedrewritingto
enabletheLLMtoeffectivelyintegrateandgrounditsresponsesbasedonvariousheterogeneous
knowledgesources.Additionally,Promptagator[27]discussesusingprompt-basedtechniquesto
adaptandrewritethequerytobetteralignwiththeretrievalsystemâ€™sexpectations,particularlyin
few-shotlearningscenarios.Thesepromptsguidethemodelingeneratingorrefiningthequeryto
optimizeretrievalresults.
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 9 -->

TheSurveyofRetrieval-AugmentedTextGenerationinLargeLanguageModels 9
3.3 DataModification
Documentmodificationtechniquesplayacriticalroleinenhancingretrievalperformance,particularlywhenintegratedwithLLMs.ThesetechniquescanbebroadlycategorizedintoInternalData
AugmentationandExternalDataEnrichment.InternalDataAugmentationfocusesonmaximizing
thevalueofexistinginformationwithindocumentsormodels,whileExternalDataEnrichment
introducessupplementarydatafromoutsidesourcestofillgaps,provideadditionalcontext,or
broadenthescopeofthecontent.
InternalDataAugmentation. InternalDataAugmentationleveragesinformationalreadypresent
withindocumentsortapsintotheinherentknowledgeembeddedinLLMs.Techniqueslikeparaphrasing,wherecontentisrewrittenforimprovedreadabilityormultipleperspectives,andsummarization,whichcondensesinformationwhileretainingcorecontent,arecommonlyemployed.Other
methodsinvolvegeneratingsupplementarycontentorexplanationsthatarecontextuallyrelated
withoutintroducingexternaldata.Forinstance,RECITE[125]utilizesamodelâ€™sinternalmemory
toreciterelevantinformationbeforegeneratingresponses,thusenhancingperformanceintasks
likeclosed-bookquestionansweringwithoutexternaldata.KnowledGPT[140]similarlyrefinesthe
internalknowledgeembeddedwithinLLMs,optimizingitsuseduringgeneration.GENREAD[156]
furtherdemonstrateshowpre-existingknowledgewithinLLMscanbeusedtogeneratecontext
that enhances task performance, bypassing the need for external sources. In another example,
theSelfmem[21]frameworkallowsthemodeltoiterativelyuseitsownoutputsasmemoryin
subsequentgenerationtasks.Byselectingandutilizingthebestinternaloutputsasmemory,this
approachboostsmodelperformancewithoutdependingonexternalmemoryresources.
ExternalDataEnrichment. ExternalDataEnrichmentenhancesdocumentcontentbyincorporatingnewinformationfromexternalsources,enrichingtheoverallcontextandaccuracy.This
processcaninvolveintegratingfacts,data,orcontextualknowledgefromexternaldatasetsorknowledgebases.Forexample,RA-DIT[89]augmentsinputpromptsduringfine-tuningbyleveraging
largedatasetslikeWikipediaandCommonCrawl,enhancingthemodelâ€™scapabilityinknowledgeintensivetasks.Thedualinstructiontuningtechniqueoptimizesboththelanguagemodelandthe
retrievertomoreeffectivelyincorporateretrievedinformation.UPRISE[20]demonstrateshow
retrievingpromptsfromdiversetaskdatasetsimprovesmodelgeneralizationinzero-shotscenarios
byenrichingthecontextduringinference.Additionally,RARG[159]exemplifiesexternaldata
enrichmentbyintegratingscientificevidencefromacademicdatabasestostrengthenresponses
counteringmisinformation.Thismethodinvolvesatwo-stageretrievalpipelinethatidentifiesand
ranksrelevantdocuments,whicharethenusedtosupportandenhancethefactualaccuracyof
generatedresponses.
4 Retrieval
4.1 Search&Ranking
ThesearchandrankingprocesswithinRAGiscrucialforimprovingtherelevanceandaccuracy
of generated outputs. Several methodologies have been developed to refine this process, each
contributinguniquestrategiesforenhancingretrievalandranking.Forexample,Atlas[59]and
AAR [157] both aim to improve the relevance of retrieved documents, but they approach this
challenge differently. Atlas focuses on optimizing the retrieverâ€™s ability to select contextually
relevantdocuments,especiallyinnewdomainswithlimiteddata,byemployingfew-shotlearning
techniques such as Attention Distillation and Perplexity Distillation. AAR, on the other hand,
adaptsretrievalpreferencestobetteralignwiththerequirementsofLLMs,enhancingretrieval
generalizationacrosstasksbytrainingasmallersourcemodel.
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 10 -->

10 Huangetal.
Fig.4. AnexampleofatypicalRAGframeworkwithinterativeretrievalstrategy.
Additionally,IRCOT[131]andFLARE[65]introducedynamicinteractionswithintheretrieval
process,albeitwithdistinctgoals.IRCOTintegratesretrievalwithchain-of-thought(CoT)reasoning,
interleavingtheseprocessestoensurethateachretrievalstepsupportstheongoingreasoningtask.
FLARE,incontrast,adoptsaconfidence-basedactiveretrievalmechanism,dynamicallytriggering
retrievalwhenthemodelgenerateslow-confidencetokens.Thisapproachisparticularlyusefulin
scenarioswheremodelconfidencevaries,asitallowsthesystemtofetchadditionalinformationto
resolveuncertaintiesduringthegenerationprocess.
Whenaddressingdomain-specificretrievalchallenges,SURGE[70]andPRCA[151]offerdifferent
solutions.SURGEusesasubgraphretrievertoextractrelevantsubgraphsfromknowledgegraphs,
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 11 -->

TheSurveyofRetrieval-AugmentedTextGenerationinLargeLanguageModels 11
integratingstructureddataintotheretrievalprocesstoimprovethecontextualunderstandingof
generatedresponses.Therelationalstructureofknowledgegraphsallowsformoreaccurateand
informedretrieval.PRCA,incontrast,focusesondomain-specificabstractivesummarization,using
areward-drivenapproachtorefinetheretrievedcontent.Thisstrategyisdesignedtooptimize
contentforthegenerator,particularlyinscenarioswherethegeneratorfunctionsasablackbox,
therebyenhancingalignmentbetweenretrievalandgeneration.
MEMWALKER[13]presentsauniqueapproachtohandlinglong-contextquestionansweringby
incorporatinganinternalsearchandrankingmechanismwithinamemorytreestructure.This
methodnavigatesextensivememorystores,ensuringthatthemostrelevantinformationisretrieved
andusedforcomplexqueries.Unlikeothermethods,MEMWALKERemphasizesefficientprocessing
oflongtextsthroughiterativenavigationandsummarization,ratherthansolelyoptimizingthe
initialretrievalphase.
4.2 RetrievalStrategy
TheretrievalstrategieswithinRAGarevitalforcustomizingtheretrievalprocesstospecificapplicationneeds,witheachstrategyofferingdistinctadvantagesandaddressingparticularchallenges.
InRAG,itismostlytheutilizationofretrievaltechniquesratherthantheexplorationofretrieval
algorithmsthatisinvolved,soitisthestrategyofretrievalthatisusuallyconsideredinsearching
andranking.WhilebasicRAGsareusuallysingle-hopsearches,i.e.,theyareretrievedonlyonce
asgeneratedsupplementarymaterial,todayâ€™sRAGsaremostlymulti-hopsearches,i.e.,theyare
searched several times through different search strategies until they are satisfied. In terms of
practicalapplicationsthesestrategiesbelongtothedesignontheengineeringpipeline.Figure4
showsatypicalcaseoftheRAGframeworkwithiterativeretrievalstrategy.Therearefivemain
retrievalstrategiesinRAG:
BasicRetrievalStrategy. Basicretrievalstrategiestypicallyfollowalinearworkflow,moving
sequentiallythroughpre-retrieval,retrieval,post-retrieval,andgenerationphases.TheAtlas[94]
frameworkexemplifiesthisstraightforwardapproach,guidingtheretrievalprocessefficientlyfrom
starttofinishwithoutiterationsorcomplexconditionalmodifications.REPLUG[122]similarly
follows this basic strategy, augmenting black-box language models with retrieval in a simple
manner,wheretheretrievedinformationisdirectlyusedtoenhancethegenerationprocess.
IterativeRetrievalStrategy. Formorecomplexscenarios,iterativeretrievalstrategies(Algorithm
1)areemployed,whereinformationisretrievedinmultiplesteps,eachinformedbypreviousresults.
IRCOT [131] exemplifies this by integrating retrieval with chain-of-thought reasoning, where
theretrievalprocessissequentialandcloselytiedtoreasoningsteps.Thismethodisparticularly
effectiveinscenariosrequiringmulti-stepproblem-solving,suchasresearchassistanceorcomplex
queriesthatbenefitfromdetailedexploration.ITER-RETGEN[121]alsoemploysiterativeretrieval,
refining the process based on generated responses, allowing for continuous improvement and
closeralignmentbetweenretrievalandgeneration.RQ-RAG[12]advancesthisapproachbyusing
techniqueslikequeryrewriting,decomposition,anddisambiguation,refiningtheretrievalstep-bysteptoenhancethefinaloutput.PlanRAG[81]alsofitswithinthisstrategy,iterativelyrefining
theretrievalprocessbasedongeneratedcontentandfeedback,ensuringthateachstepisbetter
informedthanthelast.
RecursiveRetrievalStrategy. Recursiveretrieval(Algorithm2)involvesretrievalthatcancall
itself,creatingahierarchyortreeofretrievals.Thismethodeffectivelyhandleshierarchicalor
layeredinformationbybreakingdowncomplexqueriesintosimplersub-queries.Itisparticularly
usefulforhierarchicaldataexploration,knowledgebaseconstruction,anddetailedinformation
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 12 -->

12 Huangetal.

### Algorithm1IterativeRetrievalStrategyinRAG

Require: Queryğ‘,Documentsğ·,MaximumIterationsğ‘,Retrieverğ‘…,Generatorğº,Pre-retrieval
Functionğ¹ ğ‘ğ‘Ÿğ‘’,Post-retrievalFunctionğ¹
ğ‘ğ‘œğ‘ ğ‘¡
Ensure: FinalOutputğ‘¦
ğ‘“ğ‘–ğ‘›ğ‘ğ‘™
1: Initializeğ‘– â†1 //Startiterationcounter
2: whileğ‘– â‰¤ ğ‘ do
Pre-retrievalPhase
3:
4:
ğ‘â€² â†ğ¹ ğ‘ğ‘Ÿğ‘’(ğ‘) //Indexing,QueryManipulation,DataModification
RetrievalPhase
5:
6:

## ğ·

ğ‘–
â†ğ‘…(ğ‘â€²,ğ·) //Searchandinitialrankingofdocuments
Post-retrievalPhase
7:
8: ğ· ğ‘– â€² â†ğ¹ ğ‘ğ‘œğ‘ ğ‘¡(ğ‘â€²,ğ· ğ‘–) //Re-rankingandfilteringtorefinedocuments
GenerationPhase
9:
10:
ğ‘¦
ğ‘–
â†ğº(ğ‘â€²,ğ·
ğ‘–
â€²) //Generateoutputbasedonrefineddocuments
11:
if stoppingconditionmetbasedonğ‘¦
ğ‘–
then
BREAK //Stopiterationsifoutputissatisfactory
12:
endif
13:
14: Updateğ‘â€² â†UpdateQuery(ğ‘,ğ‘¦ ğ‘–) //Refinequerybasedonthegeneratedoutput
15:
ğ‘– â†ğ‘–+1 //Incrementiterationcounter
endwhile
16:
FinalSynthesis
17:
18: ğ‘¦ ğ‘“ğ‘–ğ‘›ğ‘ğ‘™ â†SynthesizeResults({ğ‘¦ 1 ,ğ‘¦ 2 ,...,ğ‘¦ ğ‘–}) //Mergeresults
return ğ‘¦
19: ğ‘“ğ‘–ğ‘›ğ‘ğ‘™
retrieval.SURGE[70]leveragesthisstrategythroughknowledgegraphs,whererelevantsubgraphs
areextractedtoenhancecontextualunderstanding.Therelationalstructureofknowledgegraphs
facilitatesnavigatingmultiplelayersofinformation,ensuringaccurateandcontextuallyrelevant
retrieval. MEMWALKER [13] similarly adopts a recursive approach, processing long texts by
constructing a memory tree of summaries. The system navigates through this tree to retrieve
relevantinformation,effectivelybreakingdowncomplexqueriesintomanageablesegments,which
isparticularlyusefulforhandlinglong-contextquestionanswering.IMRAG[150]introducesa
multi-roundretrievalmechanism,whereeachroundofretrievalisbasedonthemodelâ€™sinternal
monologues,progressivelyrefiningthesearchwitheachiteration.Selfmem[21]employsaselfmemorymodule,enablingthesystemtostoreandretrieveinformationrecursively,buildingupon
previouslyretrievedknowledgeinahierarchicalmanner.Thisrecursivestrategyenhancesthe
systemâ€™sabilitytomanageandintegratevastamountsofinformationacrossmultipleretrieval
iterations.
ConditionalRetrievalStrategy. Conditionalretrievalstrategies(Algorithm3)aregovernedby
specificconditionsorrules,whichmaybepredefinedordynamicallydeterminedduringtheprocess.
Thismethodensuresthatretrievalalignswithspecificconstraintsorcriteria,enhancingrelevance
and specificity. It is particularly useful for compliance checking, rule-based recommendation
systems,andcontext-sensitiveinformationretrieval.PRCA[151]isaprimeexample,whereretrieval
strategies are adapted based on reward-driven adjustments, refining the context used by large
languagemodelstoenhanceprecisionandrelevance.RARG[159]similarlyemphasizesretrieval
basedonspecificevidenceconditions,ensuringthattheretrievalprocessalignswithpredefined
requirements,whichiscriticalforgeneratingfactualandpoliteresponses.CRAG[149]addsanother
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 13 -->

TheSurveyofRetrieval-AugmentedTextGenerationinLargeLanguageModels 13

### Algorithm2RecursiveRetrievalStrategyinRAG

Require: InitialQueryğ‘,Documentsğ·,MaximumDepthğ¿,Retrieverğ‘…,Generatorğº,Pre-retrieval
Functionğ¹ ğ‘ğ‘Ÿğ‘’,Post-retrievalFunctionğ¹ ğ‘ğ‘œğ‘ ğ‘¡,Sub-queryGenerationFunctionğ¹ ğ‘ ğ‘¢ğ‘ğ‘,Hierarchical
LayerBuildingFunctionğ¹ ğ‘ğ‘¢ğ‘–ğ‘™ğ‘‘,HierarchicalInformationOperatingFunctionğ¹
â„ğ‘–ğ‘’ğ‘Ÿ
Ensure: FinalOutputğ‘¦
ğ‘“ğ‘–ğ‘›ğ‘ğ‘™
BuildHierarchicalLayers(Pre-retrieval)
1:
2:
ğ»ğ‘–ğ‘’ğ‘Ÿğ‘ğ‘Ÿğ‘â„ğ‘¦ â†ğ¹ ğ‘ğ‘¢ğ‘–ğ‘™ğ‘‘(ğ‘) //Buildhierarchicallayersbasedontheinitialquery
3: Initializeğ‘™ â†0 //Startdepthcounterfrom0
4: Initializeğ‘†ğ‘¢ğ‘_ğ‘ğ‘¢ğ‘’ğ‘Ÿğ‘–ğ‘’ğ‘  â† [ğ‘] //Initializelistofsub-queries
5:
whileğ‘™ â‰¤ğ¿do
6:
foreachğ‘
ğ‘™
â€² âˆˆğ‘†ğ‘¢ğ‘_ğ‘ğ‘¢ğ‘’ğ‘Ÿğ‘–ğ‘’ğ‘  do
Pre-retrievalPhase
7:
8:
ğ‘
ğ‘™
â€² â†ğ¹ â„ğ‘–ğ‘’ğ‘Ÿ(ğ»ğ‘–ğ‘’ğ‘Ÿğ‘ğ‘Ÿğ‘â„ğ‘¦,ğ‘
ğ‘™
â€²) //Adjustquerybasedonhierarchicallayers
9:
ğ‘
ğ‘™
â€² â†ğ¹ ğ‘ğ‘Ÿğ‘’(ğ‘
ğ‘™
â€²) //QueryManipulation,Indexing,andDataModification
RetrievalPhase
10:
11:

## ğ·

ğ‘™
â†ğ‘…(ğ‘
ğ‘™
â€²,ğ·) //Retrievedocumentsforthecurrentsub-query
Post-retrievalPhase
12:
13: ğ· ğ‘™ â€² â†ğ¹ ğ‘ğ‘œğ‘ ğ‘¡(ğ‘ ğ‘™ â€²,ğ· ğ‘™) //Re-rankingandfilteringtorefinedocuments
GenerationPhase
14:
15:
ğ‘¦
ğ‘™
â†ğº(ğ‘
ğ‘™

## â€²,ğ·

ğ‘™
â€²) //Generateoutputbasedonrefineddocuments
Sub-queryGeneration(ifneeded)
16:
17:
if additionalrefinementneededbasedonğ‘¦
ğ‘™
then
18: ğ‘†ğ‘¢ğ‘_ğ‘ğ‘¢ğ‘’ğ‘Ÿğ‘–ğ‘’ğ‘  â†ğ¹ ğ‘ ğ‘¢ğ‘ğ‘(ğ‘¦ ğ‘™) //Generatenewsub-queriesbasedoncurrentoutput
endif
19:
endfor
20:
21: ğ‘™ â†ğ‘™ +1 //Incrementdepthcounter
endwhile
22:
FinalSynthesis
23:
24: ğ‘¦ ğ‘“ğ‘–ğ‘›ğ‘ğ‘™ â†SynthesizeResults({ğ‘¦ 0 ,ğ‘¦ 1 ,...,ğ‘¦ ğ‘™}) //Mergeresults
return ğ‘¦
25: ğ‘“ğ‘–ğ‘›ğ‘ğ‘™
layertothisapproachbyincorporatingaretrievalevaluatorthatassessesthequalityofretrieved
documentsandtriggersdifferentactionsbasedonconfidencethresholds,ensuringthatonlythe
mostrelevantandaccurateinformationisusedinthegenerationprocess.
AdaptiveRetrievalStrategy. Adaptiveretrieval(Algorithm4)dynamicallyadjuststheretrieval
strategybasedonthecontextandnatureofthequeryorthedataretrievedsofar.Thishighly
flexible method tailors retrieval approaches on-the-fly to optimize for relevance and precision,
makingitidealforpersonalizedsearchengines,adaptivelearningsystems,andreal-timedecision
support.AAR[157]exemplifiesadaptiveretrievalbyadjustingitsstrategybasedonthepreferencesofLLMs,learningfromasmallsourcemodelandgeneralizingtounseentasks.FLARE[65]
takes a similar adaptive approach but focuses on dynamically fetching additional information
whenmodelconfidenceislow,therebyimprovingtherelevanceofgeneratedresponses.SelfRAG
[4]goesfurtherbyincorporatingself-reflectiveprocesses,wheretheretrievalstrategyevolves
basedoncritiquesofthegeneratedcontent.CoK[86],ontheotherhand,implementsadynamic
mechanismthatadjustsretrievalstrategiesbasedontheevolvingneedsofthetask.Theretrieval
processinCoKisnotstaticbutadaptsaccordingtothespecificscenarioandthenatureofthe
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 14 -->

14 Huangetal.

### Algorithm3ConditionalRetrievalStrategyinRAG

Require: Queryğ‘,Documentsğ·,MaximumIterationsğ‘,Retrieverğ‘…,Generatorğº,Pre-retrieval
Functionğ¹ ğ‘ğ‘Ÿğ‘’,Post-retrievalFunctionğ¹ ğ‘ğ‘œğ‘ ğ‘¡,ConditionEvaluationFunctionğ¹
ğ‘ğ‘œğ‘›ğ‘‘
Ensure: FinalOutputğ‘¦
ğ‘“ğ‘–ğ‘›ğ‘ğ‘™
1: Initializeğ‘– â†1 //Startiterationcounter
2:
ğ‘â€² â†ğ‘ //Initializequery
3: whileğ‘– â‰¤ ğ‘ do
Pre-retrievalPhase
4:
5:
ğ‘â€² â†ğ¹ ğ‘ğ‘Ÿğ‘’(ğ‘â€²) //Performquerymanipulationanddatamodification
RetrievalPhase
6:
7:

## ğ·

ğ‘–
â†ğ‘…(ğ‘â€²,ğ·) //Retrievedocumentsbasedonthecurrentquery
Post-retrievalPhase
8:
9: ğ· ğ‘– â€² â†ğ¹ ğ‘ğ‘œğ‘ ğ‘¡(ğ‘â€²,ğ· ğ‘–) //Re-rankandfilterdocumentsbasedonconditions
GenerationPhase
10:
11:
ğ‘¦
ğ‘–
â†ğº(ğ‘â€²,ğ·
ğ‘–
â€²) //Generateoutputusingtherefineddocuments
ConditionalBranching
12:
13:
if ğ¹ ğ‘ğ‘œğ‘›ğ‘‘(ğ‘¦
ğ‘–

## ,ğ·

ğ‘–
â€²)isConditionAthen
14: ApplyStrategyA //e.g.,refinethequerybasedonfeedback
15:
elseif ğ¹ ğ‘ğ‘œğ‘›ğ‘‘(ğ‘¦
ğ‘–

## ,ğ·

ğ‘–
â€²)isConditionBthen
16: ApplyStrategyB //e.g.,expandthescopeoradjustparameters
17:
elseif ğ¹ ğ‘ğ‘œğ‘›ğ‘‘(ğ‘¦
ğ‘–

## ,ğ·

ğ‘–
â€²)isConditionCthen
18: ApplyStrategyC //e.g.,modifyretrievalstrategyoroutputprocessing
else
19:
20: Continuewithoutchanges //Ifnoconditionsaremet,proceedwithoutadjustments
endif
21:
CheckTerminationCondition
22:
23:
if ğ¹ ğ‘ğ‘œğ‘›ğ‘‘(ğ‘¦
ğ‘–

## ,ğ·

ğ‘–
â€²)meetsstoppingcriteriathen
BREAK //Exittheloopifthestoppingconditionismet
24:
endif
25:
26:
ğ‘– â†ğ‘–+1 //Incrementiterationcounter
endwhile
27:
FinalSynthesis
28:
29: ğ‘¦ ğ‘“ğ‘–ğ‘›ğ‘ğ‘™ â†SynthesizeResults({ğ‘¦ 1 ,ğ‘¦ 2 ,...,ğ‘¦ ğ‘–}) //Mergeresults
return ğ‘¦
30: ğ‘“ğ‘–ğ‘›ğ‘ğ‘™
informationbeingaccessed,makingithighlyeffectiveforcontext-sensitiveapplications.DRAGIN
[124]discussesareal-timedynamicretrievalmechanismthatadaptstotheevolvingneedsofthe
languagemodel,ensuringthattheretrievalstrategyremainsresponsiveandalignedwiththeimmediatetaskrequirements,thusoptimizingtherelevanceandprecisionoftheretrievedinformation.
Insummary,thechoiceofretrievalstrategywithinRAGdependsonthespecificrequirementsof
theapplicationathand.Whilebasicretrievalstrategiesoffersimplicityandefficiency,iterative
retrievaliswell-suitedfortasksrequiringdetailedexplorationandrefinement.Recursiveretrieval
excelsinmanaginghierarchicalinformation,whileadaptiveretrievalprovidesflexibilityindynamic
environments. Conditional retrieval ensures strict adherence to predefined criteria, making it
indispensableinapplicationswherecomplianceandspecificconstraintsarecritical.Bycarefully
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 15 -->

TheSurveyofRetrieval-AugmentedTextGenerationinLargeLanguageModels 15

### Algorithm4AdaptiveRetrievalStrategyinRAG

Require: Queryğ‘,Documentsğ·,MaximumIterationsğ‘,Retrieverğ‘…,Generatorğº,Pre-retrieval
Functionğ¹ ğ‘ğ‘Ÿğ‘’,Post-retrievalFunctionğ¹ ğ‘ğ‘œğ‘ ğ‘¡,AdaptiveAdjustmentFunctionğ¹ ğ‘ğ‘‘ğ‘ğ‘ğ‘¡,Feedback
Functionğ¹
ğ‘“ğ‘’ğ‘’ğ‘‘ğ‘ğ‘ğ‘ğ‘˜
Ensure: FinalOutputğ‘¦
ğ‘“ğ‘–ğ‘›ğ‘ğ‘™
1: Initializeğ‘– â†1 //Startiterationcounter
2:
ğ‘â€²,ğ¶ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ â†ğ‘,âˆ… //Initializequeryandcontext
3: whileğ‘– â‰¤ ğ‘ do
Pre-retrievalPhase
4:
5:
ğ‘â€²,ğ¶ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ â†ğ¹ ğ‘ğ‘Ÿğ‘’(ğ‘â€²,ğ¶ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡) //QueryManipulation,Indexing,DataModification,and

### ContextSetup

DynamicRetrievalPhase
6:
7:

## ğ·

ğ‘–
â†ğ‘…(ğ‘â€²,ğ·,ğ¶ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡) //Retrievedocumentsbasedonthecurrentqueryandcontext
AdaptivePost-retrievalPhase
8:
9:

## ğ·

ğ‘–
â€² â†ğ¹ ğ‘ğ‘œğ‘ ğ‘¡(ğ‘â€²,ğ·
ğ‘–
,ğ¶ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡) //Re-rankingandfilteringbasedonadaptivecriteria
GenerationPhase
10:
11:
ğ‘¦
ğ‘–
â†ğº(ğ‘â€²,ğ·
ğ‘–
â€²,ğ¶ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡) //Generateoutputusingtherefineddocuments
AdaptiveAdjustment
12:
13:
if ğ¹ ğ‘“ğ‘’ğ‘’ğ‘‘ğ‘ğ‘ğ‘ğ‘˜(ğ‘¦
ğ‘–

## ,ğ·

ğ‘–
â€²)isnegativethen
14:
ğ‘â€²,ğ¶ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡ â†ğ¹ ğ‘ğ‘‘ğ‘ğ‘ğ‘¡(ğ‘â€²,ğ¶ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡,ğ‘¦
ğ‘–

## ,ğ·

ğ‘–
â€²) //Dynamicallyadjustthequeryandcontext
endif
15:
FeedbackIntegration
16:
17:
if ğ¹ ğ‘“ğ‘’ğ‘’ğ‘‘ğ‘ğ‘ğ‘ğ‘˜(ğ‘¦ ğ‘–)ispositivethen
BREAK //Stopiterationsifoutputissatisfactory
18:
endif
19:
20:
ğ‘– â†ğ‘–+1 //Incrementiterationcounter
endwhile
21:
FinalSynthesis
22:
23: ğ‘¦ ğ‘“ğ‘–ğ‘›ğ‘ğ‘™ â†SynthesizeResults({ğ‘¦ 1 ,ğ‘¦ 2 ,...,ğ‘¦ ğ‘–}) //Mergeresults
return ğ‘¦
24: ğ‘“ğ‘–ğ‘›ğ‘ğ‘™
selectingandcombiningthesestrategies,RAGsystemscanbetailoredtoeffectivelyhandleawide
rangeofinformationretrievalscenarios,leveragingthestrengthsofeachapproachtodeliverrobust
andpreciseresults.
5 Post-Retrieval
5.1 Re-Ranking
Asretrievalmechanismsoftenreturnalargenumberofpotentiallyrelevantdocuments,re-ranking
methodsareemployedtoreorderthesedocuments,prioritizingthosemostlikelytocontribute
meaningfullytothefinaloutput.Byleveragingvariousstrategies,includingunsupervisedtechniques,supervisedlearning,anddataaugmentation,re-rankingaimstooptimizethealignment
betweentheretrievedcontentandthedesiredresponse,therebyimprovingtheoveralleffectiveness
ofRAGsystems[165].
UnsupervisedRe-ranking. Unsupervisedre-rankersdonotrelyonlabeleddatafortraining.They
usestrategiessuchaspointwise,listwise,orpairwisemethodstorankdocumentsbasedonLLM
outputswithouttheneedforsupervisedfine-tuning.Forexample,In-ContextRALM[112]employs
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 16 -->

16 Huangetal.
azero-shotapproachwhereanoff-the-shelflanguagemodelisusedtore-rankthetop-kdocuments
retrievedbyaBM25retriever.Thisprocessinvolvesselectingthedocumentthatmaximizesthe
likelihoodofthegeneratedtext,effectivelyusingtheLMâ€™ssemanticunderstandingtoimprove
documentrelevancewithoutrequiringadditionalsupervisedtraining.Thepaperalsoexplores
trainingadedicatedre-rankerusingself-supervisedlearningtofurtherenhancetheselectionof
relevantdocuments,demonstratingthattrainingare-rankerwithdomain-specificdatacanbe
moreeffectivethanzero-shotre-ranking.
Supervised Re-ranking. Supervised re-rankers involve fine-tuning LLMs on specific ranking
datasets.ThiscategorycanbefurtherdividedintomodelslikeBERTthatprocessquery-document
pairs to compute relevance scores, models like T5 that treat ranking as a generation task and
usegenerated tokenstodetermine relevance, andmodelslike RankLLaMA[95]that employa
prompt-basedapproach,focusingonthelasttokenâ€™srepresentationforrelevancecalculation[165].
Forinstance,there-rankerinRe2G[40]isbasedonaBERTmodeltrainedonlabeleddata(suchas
MSMARCO)andfine-tunedtoimprovetherelevancerankingofretrieveddocuments.FiD-Light
[47]employsasupervisedapproachwherethemodelisfine-tunedonspecificdatasetstolearn
howtore-rankpassageseffectivelyusingsourcepointersduringautoregressivetextgeneration.
Themodelusesalistwiseauto-regressivere-rankingmechanism,trainedtoidentifyandre-rank
relevantpassagesbasedontheoutputgeneratedduringthetextgenerationprocess.GenRT[148]
utilizesacombinationofanencodertocapturegloballist-levelfeaturesandasequentialdecoder
toreorderdocumentsbasedonrelevance.Themodelistrainedtolearnrelevancescoresthrough
supervisedlearning,guidedbylabeledrelevancedata,ensuringthatthemostpertinentdocuments
areprioritizedinthefinalrerankedlist.Furthermore,ITER-RETGEN[121]proposesusingamore
capablere-ranker,whichhasaccesstomodelgenerations,todistillknowledgeintoadenseretriever.
Thisknowledgedistillationprocessoptimizesthequeryencoderofthedenseretriever,enablingit
tobettercapturethesemanticrelevanceofdocumentsrelativetothetaskinput.
DataAugmentationforRe-ranking. Dataaugmentationforre-rankersfocusesonenhancingthe
trainingprocessbygeneratingadditionaltrainingdata,suchaspseudo-relevancelabels,using
LLMs.Thisdataaugmentationprovidesmorevariedtrainingexamples,whichhelpsimprovethe
performanceofre-rankingmodels.Forexample,DKS-RAC[53]introducesmethodslikeDense
KnowledgeSimilarity(DKS)andRetrieverasAnswerClassifier(RAC),whichfocusonimproving
theretrievalprocessbyincorporatingrichanswerencodings.Thesemethodsinvolvegenerating
additionaltrainingsignalsorutilizingenricheddatarepresentationstoimprovetheretrievaland
rankingofdocuments.Additionally,thePROMPTAGATOR[27]frameworkutilizessyntheticdata
generatedthroughLLM-basedquerygenerationtoenhancethetrainingofthereranker.Thisdata
augmentationapproachallowsthere-rankertorefinecandidatepassagesmoreeffectively,usinga
cross-attentionmodeltrainedontheseadditionalexamplestoboostretrievalaccuracy.
5.2 Filtering
Filteringandre-rankingaredistinctprocessesinthepost-retrievalstageofRAGsystems.Filtering
focusesoneliminatingirrelevantorlow-qualitydocumentsfromtheretrievedset,therebyreducing
the document set sizeand improving efficiencyand effectivenessin subsequentprocessing. In
contrast,re-rankingorderstheremainingdocumentsbasedontheirrelevanceorutilityforthetask,
oftenprioritizingthosethatenhancethequalityofthegeneratedoutput,especiallyinresponseawarescenarios.
SeveralfilteringmethodshavebeendevelopedtorefinedocumentsetsinRAGsystems,each
withuniquemechanismsbutsharingcommongoalsofimprovingrelevanceandreducingcomputationalload.Self-RAG[4]employsaself-reflectionmechanism,utilizingspecialâ€œreflectiontokensâ€
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 17 -->

TheSurveyofRetrieval-AugmentedTextGenerationinLargeLanguageModels 17
generatedbythemodeltoevaluatetherelevanceandqualityofretrievedpassagesandthemodelâ€™s
owngeneratedoutputs.Thisself-reflectionensuresthatonlythemostpertinentdocumentsare
retained,leveragingthemodelâ€™sinternalcapabilitieswithoutrelyingonexternalmodelsduring
inference.Similarly,BlendFilter[135]utilizestheLLMitselfasthefilter,assessingandremoving
irrelevantorlessusefuldocumentsbyapplyingfilteringseparatelytoknowledgeretrievedfrom
original,externallyaugmented,andinternallyaugmentedqueries.BothSelf-RAGandBlendFilter
highlightthemodelâ€™sintrinsicabilitytoperformfiltering,reducingtheneedforadditionalmodels
andenhancingcomputationalefficiency.
In contrast, RECOMP [147] and CRAG [149] employ more external or structural strategies.
RECOMPfocusesonselectiveaugmentation,wheresummariesgeneratedfromretrieveddocuments
areselectivelyprependedtotheinputforthelanguagemodel.Iftheretrieveddocumentsaredeemed
irrelevant,thecompressorcangenerateanemptysummary,effectivelyfilteringoutunnecessary
information.Thismethodallowsforadynamicapproachtofiltering,whereonlyhelpfulcontentis
retained.CRAG,ontheotherhand,usesadecompose-then-recomposeapproach,whereretrieved
documentsaresplitintofinerknowledgestrips.ThesestripsareevaluatedforrelevanceusingafinetunedT5model,andonlytherelevantstripsarerecomposedtoformarefinedsetofinformation
forthegenerationtask.Thisgranularfilteringprocessensuresthatthefinaldocumentsetisboth
relevantandconcise,tailoredspecificallytothegenerationtask.
Dynamic filtering techniques are also employed in methods like FiD-TF [5] and CoK [86].
FiD-TFintroducesTokenFilteringduringthedecodingprocess,wherelessrelevanttokensare
dynamicallyfilteredoutbasedoncross-attentionscores.Thisapproachreducesthecomputational
load by eliminating tokens deemed uninformative for generating the final answer, enhancing
efficiency with minimal impact on performance. CoK employs a filtering technique based on
self-consistency,identifyingandprocessingonlythosequestionswithâ€œuncertainâ€answers.This
methodworksbysamplingvariousreasoningpathsandanswers,preservingonlypredictionswith
highconsistency.Questionsthatdonotmeetthespecifiedconsistencythresholdundergofurther
processing,effectivelypreventingthepropagationoferrorsinthegenerationprocess.
Finally,FILCO[141]implementsacomprehensivefilteringapproachusingthreedistinctstrategies:StringInclusion(STRINC)tomatchexactoutputs,LexicalOverlaptomeasureword-level
similarity,andConditionalCross-MutualInformation(CXMI)toassesshowmuchthecontext
improvesoutputlikelihood.FILCOappliesthesefilteringstrategiesatthesentencelevel,refining
theretrievedcontentforbetterrelevance.Additionally,FILCOtrainsacontextfilteringmodel
usingthesestrategies,whichpredictsthemostusefulcontextatinferencetime,therebyenhancing
theaccuracyandrelevanceofthegenerationmodelâ€™soutput.
6 Generation
6.1 Enhancing
Enhancing methods are strategies aimed at improving the quality and relevance of generated
outputs by integrating retrieved content in various ways. These methods differ in how they
combine,aggregate,orrefineretrievedinformation,offeringmultipleapproachestoenrichthefinal
output.Broadly,thesetechniquescanbegroupedintothreecategories:enhancingwithqueries,
enhancingwithensembleapproaches,andenhancingwithfeedbackloops.
EnhancewithQuery. Thisapproachintegratestheretrieveddocumentswiththeoriginalquery,
enablingthegeneratortoleveragebothsourcesinproducingthefinaloutput.Bycombiningthe
querywiththeretrievedcontent,thegenerationprocessensuresthattheresponseremainsclosely
alignedwiththeuserâ€™sintentwhilebeingenrichedbyrelevantinformation.Thefocushereisonthe
seamlessfusionofthequeryandcontext,allowingthegeneratedoutputtomaintainbothrelevance
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 18 -->

18 Huangetal.
andcompleteness.Forinstance,theRETRO[9]modelenhancesgenerationbyintegratingretrieved
textchunkswiththeuserâ€™squeryusingachunkedcross-attentionmechanism,whererelevant
informationfromtheretrievedneighborsisdirectlyinjectedintothegenerationprocess.This
methodinvolvesfirstretrievingsimilardocumentchunksbasedonthequeryandthenusingacrossattentionmoduletoalignandcombinethesechunkswiththeinputsequenceduringgeneration.
In-ContextRALM[112]takesacomparableapproach,directlyprependingtheretrieveddocuments
totheinputquery.Inthisway,thelanguagemodelcangenerateresponsesconditionedonboth
thequeryandtheretrievedcontentwithoutrequiringchangestothemodelâ€™sarchitecture.Both
examplesillustrateastraightforwardyeteffectivemethod:concatenatingthequeryandretrieved
documentsintoasingleinputsequencethattheLLMsprocesstogether,yieldingoutputsthatare
contextuallyenhanced.
Enhance with Ensemble. When multiple sources are synthesized, the generation process can
achieveamorecoherentandwell-roundedresponse.Ratherthanrelyingsolelyonasinglesource,
thisapproachaggregatesinformationfromvariousdocuments,allowingthegeneratortoreconcile
conflictingdetails,blenddiverseperspectives,andselectthemostreliableorcomprehensiveoutput.
The ensemble process can manifest in different ways: it may involve combining insights from
severalsourcesintoaunifiednarrative,orgeneratingmultiplecandidateoutputsandchoosing
the best one based on criteria like consistency, relevance, or factual accuracy. An instance of
thisstrategyisseeninFiD[58],whichencodesmultipleretrievedpassagesindependentlybefore
fusingtheminthedecodertocreateacoherentanswer.Bytreatingeachpassageseparatelyduring
encodingandthenmergingthemduringdecoding,themodeleffectivelycombinesevidencefrom
multiple sources. Meanwhile, in REPLUG [122], an ensemble approach is adopted where each
retrieveddocumentisindependentlyprependedtothequeryandprocessedseparately.Theoutputs
arethenaggregated,withrelevancescoresguidingtheweightingofeachdocumentâ€™scontribution.
Throughthisprocess,themodelcapitalizesondiverseinformationacrossseveralsources,leading
toimprovementsinansweraccuracy,coverage,andscalabilityasmoredatabecomesavailable.
EnhancewithFeedback. Incontrasttoapproachesthatprocessretrievedinformationinasingle
pass,thismethodintroducesiterativerefinementintothegenerationprocessbyincorporating
feedbackloops.Initially,thegeneratorproducesadraftresponse,whichisthenevaluatedand
adjustedbasedonfeedbackmechanisms,suchasself-reflectionorpredefinedcriteriafocusedon
factualaccuracyandfluency.Thisiterativeapproachaimstoincrementallyimprovetheoutput
byidentifyingandcorrectingerrorsorfine-tuningcontenttobetteralignwithqualitystandards,
ultimatelyproducingapolishedandreliableresponse.PRCA[151]offersanexamplebypositioning
itselfbetweentheretrieverandgenerator,distillingretrievedinformationbasedonfeedbackfrom
thegenerator.Thisdistilledinformationservesasarewardmodeltoguidecontextoptimization,
leveragingreinforcementlearningandmetricslikeROUGE-Lscorestoiterativelyrefinewhich
detailsshouldbeemphasizedordownplayed.DSP[73],ontheotherhand,refinesbothqueries
andretrievedpassagesthroughamulti-hopretrievalprocessthatincorporatesprogrammatically
bootstrappedfeedback.Here,thelanguagemodelgeneratesintermediatequeries,retrievesrelevant
passages,andupdatesthecontextinsubsequentstepsâ€”eachstagebuildingonthelasttorefinethe
finaloutput.Feedback-drivenenhancementsarealsoevidentinmodelslikeSelfmem[21],which
focusongeneratingself-memory.Themodelfirstproducesanunboundedpoolofoutputsand
thenselectsthemostrelevantoneasmemoryforthenextgeneration,guidedbymetricslikeBLEU
or ROUGE. Finally, RECITE [122] integrates feedback by generating multiple recitations from
themodelâ€™sinternalknowledgeandusingself-consistencytechniquestoaggregatetheoutputs.
Byintroducingdiversityintherecitationsandleveragingpassagehintsduringgeneration,this
approachselectsthebestcontentthroughmajorityvoting.Together,thesemethodsdemonstrate
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 19 -->

TheSurveyofRetrieval-AugmentedTextGenerationinLargeLanguageModels 19
howfeedbackloopsanditerativerefinementscanleadtooutputsthatarenotonlymoreaccurate
butalsoincreasinglycoherentandcontextuallygroundedastheyevolve.
6.2 Customization
Customizationfocusesontailoringcontenttotheuserâ€™spersonalityandneeds.Itinvolvesadjusting
theoutputeithertoalignwithspecificknowledgeretrievedduringearlierstages(contentalignment)
or to adapt the generated response to meet the userâ€™s preferences, context, or audience needs
(contextualadaptation).
InLAPDOG[52],customizationisachievedprimarilythroughcontentalignmentbyintegrating
personaprofileswithexternalstoriestoenrichthecontextusedforgeneration.Thestoryretriever
identifiesrelevantnarrativesbasedonthepersona,expandingthelimitedprofileswithadditional
information. The generator then combines this enriched knowledge with the dialogue history,
ensuringthatresponsesaligncloselywiththepersonaâ€™straitsandbackground.Thisapproach
allowsforanuancedunderstandingoftheuserâ€™spersonality,makingtheoutputmoreengaging
andcontextuallyappropriate.
Ontheotherhand,PersonaRAG[160]emphasizesreal-timeadaptationbycustomizinggenerated
contentbasedondynamicuserprofiles,sessionbehavior,andongoingfeedback.Amulti-agent
system continuously analyzes user interactions to refine responses, ensuring alignment with
theuserâ€™spreferencesandcontext.Byintegratingpersonalizedinsightsateachstep,thesystem
canadjustitsoutputtosuitspecificinformationalneedsandsituationalcontexts.Thislevelof
responsivenessallowsthesystemtoevolveinlinewiththeuserâ€™schangingrequirements,creating
morerelevantandtargetedresponses.
ERAGent[123]alsofocusesoncustomizationbutthroughtheuseofaPersonalizedLLMReader,
whichadaptsresponsesusinguser-specificprofiles.Thismoduleintegratesrewrittenquestions,
filteredknowledge,anduserpreferencestotailorresponsesaccordingtobothcontentrelevance
anduserneeds.Forinstance,ittakesintoaccountpreferenceslikeenvironmentalconsciousness
or dietary restrictions, ensuring that the generated content is not only aligned with retrieved
knowledgebutalsopersonalizedtotheuserâ€™sparticularvaluesandrequirements.Thisdeeplevel
ofcustomizationensuresthattheoutputisbothrelevantandpersonallymeaningful,enhancing
userengagement.
ROPG[118]proposesadynamicpre-andpost-generationretrieverselectionmodel,enhancing
personalizationbyaligningtheretrievalprocesswithboththeinputcontextandtheuserâ€™spreferences.Thepre-generationmodeldetermineswhichretrievalstrategyâ€”suchasrecency-based,
keywordmatching,orsemanticretrievalâ€”ismostappropriatebeforegenerationbegins.Bytailoring
theretrievalprocessinthisway,themodelensuresthatthedocumentsretrievedfromtheuser
profilecloselymatchthecurrentinput,therebyaligningthecontentwithrelevantuser-specific
knowledge.Followingthis,thepost-generationmodelevaluatestheoutputsgeneratedbydifferent
retrievalstrategiesandselectsthemostpersonalizedresult.Thisselectionisguidedbyfeedback
fromthegeneratedcontent,whichisthenusedtoadjustfutureretrievals.Bycombiningcontent
alignment(throughpre-generationretrieval)withcontextualadaptation(throughpost-generation
evaluation),thisapproachoffersacomprehensivesolutionforcustomizationwithinRAG.
7 EvaluationinRAG
To assess how effectively language models can generate more accurate, relevant, and robust
responsesbyleveragingexternalknowledge,theevaluationofRAGsystemshasemergedasa
crucialresearchfocus.Giventherisingpopularityofdialogue-basedinteractions,muchrecent
workhasconcentratedonevaluatingRAGmodelsâ€™performanceonsuchdownstreamtasksusing
establishedmetricslikeExactMatch(EM)andF1scores.Thesemetricshavebeenappliedacrossa
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 20 -->

20 Huangetal.
EvaluationFramework Aspects Methods Metrics Datasets

### ContextRelevance ExtractedSentences/TotalSentences

RAGAS[33] QualityofRAGSystems AnswerRelevance AverageCosineSimilarity WikiEval7
Faithfulness SupportedStatements/TotalStatements
ContextRelevance

## Kilt[109]

ARES[117] ImprovingRAGAS AnswerRelevance ConfidenceIntervals

### SuperGLUE[134]

AnswerFaithfulness

### Accuracy(QA)


### ResponseQuality

BLEU,ROUGE-L(Generation) EventKG[41]
RECALL[91] CounterfactualRobustness
MisleadingRate(QA) UJ[50]

### Robustness

MistakeReappearanceRate(Generation)

### NoiseRobustness Accuracy


### NegativeRejection RejectionRate

RGB[14] ImpactofRAGonLLMs InformationIntegration Accuracy Synthetic

### ErrorDetectionRate

CounterfactualRobustness

### ErrorCorrectionRate


### Zero-ShotLearning MMLU-Med[45]


### Multi-ChoiceEvaluation MedQA-US[66]

MIRAGE[144] RAGinMedicalQA Accuracy MedMCQA[105]
Retrieval-AugmentedGeneration PubMedQA[67]
Question-OnlyRetrieval BioASQ-Y/N[132]

### DownstreamTask Accuracy,ROUGE

eRAG[119] RetrievalQualityinRAG Set-based Precision,Recall,HitRate KILT

### Ranking MAP,MRR,NDCG


### Surface-Based EM,F1,Precision,Recall

BERGEN[114] StandardizingRAGExperiments QADatasets[69,76]

### Semantic BEM[11],LLMeval[114]

Table1. TheComparisonofDifferentRAGEvaluationFrameworks.
widearrayofdatasets,includingTriviaQA[69],HotpotQA[153],FEVER[129],NaturalQuestions
(NQ)[76],WizardofWikipedia(WoW)[30],andT-REX[32],whichareoftenusedtobenchmark
theeffectivenessofretrievalandgenerationcomponentsinknowledge-intensivetasks.
Whiledownstreamtaskevaluationsprovidevaluableinsights,theyfailtoaddressthemultifaceted
challenges that arise as RAG systems continue to evolve. To fill this gap, recent research has
proposedvariousframeworksandbenchmarksthataimtoevaluatethesesystemsfrommultiple
perspectives,consideringnotonlythequalityofthegeneratedtextbutalsotherelevanceofretrieved
documentsandthesystemâ€™sresiliencetomisinformation,asshowninTable1.Theseevaluations
includemetricsthatassessnoiserobustness,negativeprompting,informationintegration,and
counterfactualrobustness,allofwhichreflectthecomplexchallengesRAGsystemsfaceinrealworldapplications.Theongoingdevelopmentofcomprehensiveevaluationframeworksandmetrics
isessentialforadvancingthefield,broadeningtheapplicabilityofRAGsystems,andensuringthat
theymeetthedemandsofanincreasinglydynamicandcomplexinformationlandscape[154].
7.1 Retrieval-basedAspect
Ininformationretrieval,standardmetricssuchasMeanAveragePrecision(MAP),Precision,ReciprocalRank,andNormalizedDiscountedCumulativeGain(NDCG)[103,110,115]havetraditionally
beenusedtoevaluatetherelevanceofretrieveddocumentstoagivenquery.Thesemetricsare
essential in assessing the effectiveness of traditional information retrieval systems, where the
primarygoalistomeasurehowwelltheretrieveddocumentsmatchtheuserâ€™squery.
When applied to RAG systems, these retrieval-based metrics extend their focus to consider
howtheretrievedinformationcontributestothequalityofthegeneratedoutput.Inthiscontext,
Accuracy becomes a crucial metric, assessing how precisely the retrieved documents provide
correctinformationforansweringqueries.Additionally,RejectionRate[14],whichmeasuresthe
systemâ€™sabilitytodeclineansweringwhennorelevantinformationisavailable,hasemergedasa
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 21 -->

TheSurveyofRetrieval-AugmentedTextGenerationinLargeLanguageModels 21
keyindicatorofresponsibleoutputgeneration.Similarly,ErrorDetectionRate[14]evaluatesthe
modelâ€™scapabilitytoidentifyandfilteroutincorrectormisleadinginformation,ensuringthatthe
generationprocessisbasedontrustworthysources.
AnotherimportantconsiderationisContextRelevance,whichassessesthealignmentofretrieved
documents with the specific query, emphasizing the need for content directly relevant to the
generationtaskâ€™scontext.Faithfulness[33]isalsocriticalindeterminingwhetherthegenerated
textaccuratelyreflectstheinformationfoundintheretrieveddocuments,therebyminimizingthe
riskofgeneratingmisleadingorincorrectcontent.
TheeRAGframework[119]introducesamorerefinedapproachtoevaluatingretrievalquality
inRAGsystemsbyfocusingonindividualdocumentsratherthantheentireretrievalprocess.It
operates by feeding each document in the retrieval list into the LLM alongside the query and
evaluatingthegeneratedoutputagainstdownstreamtaskmetricssuchasAccuracy.ThedocumentlevelscoresarethenaggregatedusingrankingmetricslikeMAPtoproduceasingleevaluationscore.
Thisfocusondocument-levelcontributionsoffersamorepreciseassessmentofretrievalquality
whilebeingsignificantlymorecomputationallyefficientthantraditionalend-to-endevaluations.
Notably,eRAGdemonstratesthatitsdocument-levelevaluationcorrelatesmorestronglywith
downstreamRAGperformancecomparedtoconventionalmethodslikehumanannotationsor
provenancelabels.ThiscorrelationunderscoresthattheLLM,astheprimaryconsumerofthe
retrievedresults,isthemostreliablejudgeofretrievalperformance[119].Regardlessoftheretrieval
modelorthenumberofretrieveddocuments,eRAGconsistentlyoutperformsotherevaluation
approaches,indicatingthatdirectlyevaluatinghoweachdocumentsupportstheLLMâ€™soutputis
themosteffectivewaytomeasureretrievalqualityinRAGsystems.
7.2 Generation-basedAspect
Theevaluationoftextproducedbylargelanguagemodelsinvolvesanalyzingperformanceacrossa
rangeofdownstreamtasksusingstandardmetricsthatassesslinguisticquality,coherence,accuracy,
andalignmentwithground-truthdata.MetricslikeBLEU[107]andROUGE-L[87]areoftenused
tomeasurefluency,similaritytohuman-producedtext,andtheoverlapwithreferencesummaries,
respectively,providinginsightsintohowwellthegeneratedcontentcaptureskeyideasandphrases.
Inadditiontothesemetrics,whichfocusonthequalityoflinguisticoutput,Accuracyandoverlap
withground-truthdataareevaluatedusingEMandF1scores,whichrespectivelymeasurethe
percentageofcompletelycorrectanswersandofferabalancedviewofprecisionandrecall.This
ensuresthatrelevantanswersareretrievedwhileinaccuraciesareminimized.
Beyondthesestandardevaluationtechniques,morespecializedcriteriahavebeenintroduced
to assess RAG systems in specific contexts. For dialogue generation, for instance, metrics like
perplexityandentropyareemployedtoevaluateresponsediversityandnaturalness.Inscenarios
wheremisinformationisaconcern,metricslikeMisleadingRateandMistakeReappearanceRate
[91]havebeendevelopedtomeasureamodelâ€™sabilitytoavoidgeneratingincorrectormisleading
content.OtheradvancedmetricsincludeAnswerRelevance[33],whichassessestheprecisionof
responsestoqueries,Kendallâ€™stau[117],usedforevaluatingtheaccuracyofsystemrankings,and
Micro-F1[117],whichfine-tunesaccuracyevaluationintasksinvolvingmultiplecorrectanswers.
PredictionAccuracyfurthercomplementsthesebydirectlymeasuringhowcloselythegenerated
responsesalignwiththeexpectedanswers,offeringaclearmeasureofasystemâ€™seffectivenessin
producingaccuratecontent.
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 22 -->

22 Huangetal.
Research Year In R t e e t r r n i a e l val E S x o t u e r r c n e al Multi-hop Training Indexing QueryM P a r n e i - p R u e l t a r t i i e o v n al DataModification Searc R h et & ri R ev a a n l king Re-R P a o n s k t- i R ng etri F e i v l a te l ring Enhanci G ng ene C ra u t s i t o o n mization

### REALM[42] 2020 (cid:33) (cid:33) (cid:33) (cid:33)

kNN-LMs[72] 2020 (cid:33) (cid:33) (cid:33) (cid:33) (cid:33)
RAG[83] 2020 (cid:33) (cid:33) (cid:33) (cid:33)

### FiD[58] 2021 (cid:33) (cid:33) (cid:33)

Webgpt[100] 2021 (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33)

### Re2G[40] 2022 (cid:33) (cid:33) (cid:33) (cid:33)

RETRO[9] 2022 (cid:33) (cid:33) (cid:33) (cid:33) (cid:33)
DSP[73] 2022 (cid:33) (cid:33) (cid:33) (cid:33) (cid:33)

### CoK[86] 2023 (cid:33) (cid:33) (cid:33) (cid:33)


### IRCOT[131] 2023 (cid:33) (cid:33) (cid:33) (cid:33)

ITRG[34] 2023 (cid:33) (cid:33) (cid:33) (cid:33) (cid:33)

### PKG[92] 2023 (cid:33)

RA-DIT[89] 2023 (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33)
Self-RAG[4] 2023 (cid:33) (cid:33) (cid:33)

### SURGE[70] 2023 (cid:33) (cid:33)


### FiD-TF[5] 2023 (cid:33) (cid:33) (cid:33)

PRCA[151] 2023 (cid:33) (cid:33) (cid:33) (cid:33)
REPLUG[122] 2023 (cid:33) (cid:33) (cid:33)
AAR[157] 2023 (cid:33) (cid:33) (cid:33)

### Query2doc[137] 2023 (cid:33) (cid:33)


### Step-Back[163] 2023 (cid:33) (cid:33) (cid:33)


### ITER-RETGEN[121] 2023 (cid:33) (cid:33) (cid:33) (cid:33)

RECITE[125] 2023 (cid:33) (cid:33) (cid:33) (cid:33) (cid:33)
PROMPTAGATOR[27] 2023 (cid:33) (cid:33) (cid:33) (cid:33) (cid:33)
UPRISE[20] 2023 (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33)

### GENREAD[156] 2023 (cid:33) (cid:33) (cid:33)

LAPDOG[52] 2023 (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33)

### KnowledGPT[140] 2023 (cid:33) (cid:33) (cid:33) (cid:33)

Selfmem[21] 2023 (cid:33) (cid:33) (cid:33) (cid:33) (cid:33)
MEMWALKER[13] 2023 (cid:33) (cid:33) (cid:33) (cid:33)

### RECOMP[147] 2023 (cid:33) (cid:33) (cid:33)


### Rewrite-Retrieve-Read[94] 2023 (cid:33) (cid:33) (cid:33)

Atlas[94] 2023 (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33)
DKS-RAC[53] 2023 (cid:33) (cid:33) (cid:33) (cid:33) (cid:33)

### In-ContextRALM[112] 2023 (cid:33) (cid:33)

Fid-light[47] 2023 (cid:33) (cid:33) (cid:33)

### FLARE[65] 2023 (cid:33) (cid:33) (cid:33)


### Chameleon[63] 2023 (cid:33) (cid:33) (cid:33) (cid:33)

ERAGent[123] 2024 (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33)
PipeRAG[64] 2024 (cid:33) (cid:33) (cid:33) (cid:33) (cid:33)

### GenRT[148] 2024 (cid:33) (cid:33) (cid:33) (cid:33)

PersonaRAG[160] 2024 (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33)
CRAG[149] 2024 (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33)
IMRAG[150] 2024 (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33)

### AiSAQ[126] 2024 (cid:33) (cid:33) (cid:33) (cid:33)


### ROPG[118] 2024 (cid:33) (cid:33) (cid:33) (cid:33) (cid:33)

RQ-RAG[12] 2024 (cid:33) (cid:33) (cid:33) (cid:33) (cid:33)
PlanRAG[81] 2024 (cid:33) (cid:33) (cid:33) (cid:33) (cid:33)

### RARG[159] 2024 (cid:33) (cid:33) (cid:33) (cid:33) (cid:33)

DRAGIN[124] 2024 (cid:33) (cid:33) (cid:33) (cid:33) (cid:33)
LRUS-CoverTree[93] 2024 (cid:33) (cid:33) (cid:33) (cid:33) (cid:33)
Table2. ThecomprehensivesummaryofRAGstudies.A(cid:33)intheâ€œMulti-hopâ€columnsignifiesthatthe
researchinvolvesmultiplesearchrounds.Similarly,a(cid:33)intheâ€œTrainingâ€columnindicatesthatthestudy
includedtrainingphases.Itisimportanttonotethatinthiscontext,â€œTrainingâ€encompassesbothinitial
modeltrainingandfine-tuningprocesses.
8 ComparisonsofRAG
8.1 TheComprehensiveSummaryofRAG
Table2presentsadetailedanalysisoftheRAGstudiesdiscussedinthispaper.Theanalysisshows
thatthemajorityofthesestudieshaveutilizedexternaldatasourcestoenrichthecontentofLLMs.
Apreferenceformultiple-hopoversingle-hopretrievalwasnoted,indicatingthatiterativesearch
roundsgenerallyyieldsuperiorresults.Inotherwords,mostmethodsemploydenseretrievalto
securehigherqualitycandidatedocuments.Comparedtomodifyingdatasetsinthepre-retrieval
stage,morestudiesfocusonmanipulatingthequerytoimproveretrievalperformance.Additionally,
thereisasignificantemphasisonoptimizingtheretrievalphase,highlightingitscrucialrolein
theresearch.However,thereseemstobeascarcityofstudiesconcentratingoncustomizationin
thegenerationstage,pointingtothisasapotentialareaforfutureexploration.Overall,whilethe
goalofRAGistoenhancetheresponsequalityofLLMs,greatereffortshavebeendirectedtowards
improvingretrievalaspects.
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 23 -->

TheSurveyofRetrieval-AugmentedTextGenerationinLargeLanguageModels 23

### Research Year Retriever Generator

REALM[42] 2020 BERT[29] Transformers[133]
kNN-LMs[72] 2020 FAISS[68] Transformers
RAG[83] 2020 DPR[71] BART-Large[82]
FiD[58] 2021 BM25[116],DPR T5[111]
Webgpt[100] 2021 Bing GPT-3[10]

### Re2G[40] 2022 BM25,DPR BART


### RETRO[9] 2022 BERT Transformer


### DSP[73] 2022 ColBERTv2[74] GPT-3.5(text-davinci-002)

CoK[86] 2023 LLaMA2-7B[130],ChatGPT(gpt-3.5-turbo-0613) ChatGPT(gpt-3.5-turbo-0613)
IRCOT[131] 2023 BM25 GPT-3(code-davinci-002),Flan-T5[24]

### ITRG[34] 2023 Atlas[94] LLaMA-33B

PKG[92] 2023 LLaMA-7B InstructGPT-3.5(text-davinic-002)[104]

### RA-DIT[89] 2023 DRAGON+[88] LLaMA

Self-RAG[4] 2023 Contriever[57] LLaMA2(7Band13B),GPT-4[1]
SURGE[70] 2023 GraphNeuralNetworks(GNN)[43] Transformers

### FiD-TF[5] 2023 BM25,SBERT[115] T5

PRCA[151] 2023 BM25,DPR,Contriver,SimCSE[37],SBERT T5,Phoenix-7B[19],Vicuna-7B[22],ChatGLM[31],GPT-3.5

### REPLUG[122] 2023 Contriever GPT-3

AAR[157] 2023 ANCE[146],Contriever Flan-T5,InstructGPT
Query2doc[137] 2023 BM25,DPR GPT-3(text-davinci-003)

### Step-Back[163] 2023 PaLM-2L[23] PaLM-2L,GPT-4

ITER-RETGEN[121] 2023 Contriever InstructGPT(text-davinci-003),LLaMA2
RECITE[125] 2023 PaLM,UL2[127],OPT[161],Codex[16]

## Promptagator[27] 2023 T5 Flan

UPRISE[20] 2023 GPT-Neo-2.7B[8] BLOOM-7.1B[142],OPT-66B,GPT-3-175B
GENREAD[156] 2023 InstructGPT
LAPDOG[52] 2023 Contriever T5

### KnowledGPT[140] 2023 GPT-4

Selfmem[21] 2023 BM25 XGLM[90],XLM-Rbase[25]
MEMWALKER[13] 2023 LLaMA2 LLaMA2

### RECOMP[147] 2023 BM25 T5-Large

Rewrite-Retrieve-Read[94] 2023 Bing T5-Large,ChatGPT(gpt-3.5-turbo),Vicuna-13B
Atlas[94] 2023 Contriever T5

## Dks-Rac[53] 2023 Dpr Bart

In-ContextRALM[112] 2023 BM25,BERT-base,Contriever,Spider[113] GPT-2,GPT-Neo,GPT-J[35],OPT,andLLaMA

### Fid-light[47] 2023 GTR-Base[101] T5

FLARE[65] 2023 BM25,Bing GPT-3.5(text-davinci-003)

### Chameleon[63] 2023 ChamVS[63] ChamLM[63]

ERAGent[123] 2024 Bing GPT-3.5,Falcon1B[108]
PipeRAG[64] 2024 SBERT RETRO[9]

### GenRT[148] 2024 LambdaMart[2]

PersonaRAG[160] 2024 BM25 GPT-3.5
CRAG[149] 2024 Contriever LLaMA2
IMRAG[150] 2024 DPR Vicuna-7B

### AiSAQ[126] 2024 DiskANN[106]

ROPG[118] 2024 BM25,Contriever FlanT5-XXL
RQ-RAG[12] 2024 DuckDuckGo8 LLaMA2-7B

### PlanRAG[81] 2024 GPT-4 GPT-4


### RARG[159] 2024 BM25,E5[136] LLaMA2-7B

DRAGIN[124] 2024 BM25,SGPT[99] LLaMA2(7Band13B),Vicuna-13B

### LRUS-CoverTree[93] 2024 k-MIPS

Table3. ThesummaryofRetrieversandGenerators.Theretrievalmodelsandpre-trainedlanguagemodels
explicitlymentionedinthesestudieshavebeenrecorded.
8.2 RetrieverandGenerator
InRAG,theretrieverandgeneratorarecentralcomponents,eachplayingadistinctroleinthe
systemâ€™soverallperformance.Table3summarizestheretrieversandgeneratorsusedacrossthe
studiesdiscussedinthispaper.Thetablerevealsthatwhileawiderangeofadvancedlanguage
modelsareemployedasgenerators,manysystemsstillrelyontraditionalretrieverslikeBM25,
valuedfortheirefficiency.Thishighlightsthecontinuedimportanceofoptimizingretrievalmethods
whilebalancingcomputationaldemands.Interestingly,despitetheavailabilityofpowerfulmodels
suchasLLaMA2,GPT-3.5,andGPT-4,thesearenotwidelyadoptedasgenerators.Instead,models
likeT5remainprevalent,whilemorefoundationalretrievalapproaches,suchasthosebasedon
BERT,seelimiteduse.TherelativescarcityofIR-focusedLLMsinretrieverssuggestsapromising
avenueforfutureresearchanddevelopmentinthisdomain.
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 24 -->

24 Huangetal.

### MirageBenchmarkDataset


### Corpus Retriever Average


### MMLU-Med MedQA-US MedMCQA PubMedQA* BioASQ-Y/N

None None 72.91Â±1.35 65.04Â±1.34 55.25Â±0.77 36.00Â±2.15 74.27Â±1.76 60.69

## Bm25 72.27Â±1.36 63.71Â±1.35 55.49Â±0.77 66.20Â±2.12 88.51Â±1.28 69.23

Contriever 71.72Â±1.36 63.94Â±1.35 54.29Â±0.77 65.60Â±2.12 85.44Â±1.42 68.20
PubMed SPECTER 73.19Â±1.34 65.20Â±1.34 53.12Â±0.77 54.80Â±2.23 75.73Â±1.72 64.41
(23.9M) MedCPT 73.09Â±1.34 66.69Â±1.32 54.94Â±0.77 66.40Â±2.11 85.76Â±1.41 69.38

## Rrf-2 75.57Â±1.30 64.34Â±1.34 55.34Â±0.77 69.00Â±2.07 87.06Â±1.35 70.26


## Rrf-4 73.37Â±1.34 64.73Â±1.34 54.75Â±0.77 67.20Â±2.10 88.51Â±1.28 69.71


## Bm25 73.37Â±1.34 63.47Â±1.35 54.10Â±0.77 26.40Â±1.97 71.36Â±1.82 57.74

Contriever 74.10Â±1.33 65.99Â±1.33 54.03Â±0.77 26.40Â±1.97 69.90Â±1.85 58.08
Wikipedia SPECTER 72.18Â±1.36 63.63Â±1.35 52.71Â±0.77 22.20Â±1.86 66.83Â±1.89 55.51
(29.9M) MedCPT 71.99Â±1.36 65.12Â±1.34 55.15Â±0.77 29.00Â±2.03 73.46Â±1.78 58.95

## Rrf-2 74.20Â±1.33 64.57Â±1.34 54.72Â±0.77 31.00Â±2.07 76.21Â±1.71 60.14


## Rrf-4 73.19Â±1.34 64.96Â±1.34 54.53Â±0.77 31.00Â±2.07 72.01Â±1.81 59.14

Table4. PartresultsofAccuracy(%)ofGPT-3.5acrossdifferentcorporaandretrieversonMirage.Redand
greenhighlight declines and improvements comparedtoCoT(firstrow),withshadingintensityreflecting
thedegreeofchange.DatasourcedfromMirage[144].
ImpactoftheRetriever. TheresultsshowninTable4highlighttheaccuracyofGPT-3.5across
differentcorporaandretrieversontheMiragebenchmark[144].Thesefindingsunderscorehow
retriever performance closely depends on the alignment between training data and the target
corpus. For example, in the MEDRAG system, MedCPTâ€”trained specifically on PubMed user
logsâ€”significantlyimprovesretrievalperformancewhenaccessingthePubMedcorpus.Thisillustratesthebenefitsofusingdomain-specificretrieverstailoredtospecializeddatasets.Incontrast,
general-purposeretrieverslikeContriever,whichincorporateWikipediadataduringtraining,excel
inretrievinginformationfromWikipedia,especiallyfortaskslikeMMLU-MedandMedQA-US.On
theotherhand,SPECTER,whichfocusesmoreonregularizingpairwisearticledistancesthanoptimizingquery-to-articlerelevance,underperformsontheMedCorpcorpus.Thestudyalsoexplores
combiningmultipleretrieversusingReciprocalRankFusion(RRF).However,resultsshowthat
addingmoreretrieversdoesnotalwaysleadtobetteroutcomes;forinstance,excludingSPECTERin
RRF-2onWikipediayieldsbetterresultsthanRRF-4,indicatingthatsimplyincreasingthenumber
ofretrieversisnotbeneficialunlesstheirstrengthsalignwiththeretrievaltask.
Figure 5a illustrates how eRAG investigates the correlation between LLM performance and
retrievaleffectivenessontheNQdatasetusingthreeretrieverswithdifferentcharacteristics:BM25
(lexicalsparse),RetroMAE(dense)[143],andSPLADEv3(learnedsparse)[80].Theinitialretrievals
arere-rankedusingaDeBERTa-v3[79]cross-encoder.Theanalysisdemonstratesthatasretrieval
quality improves, LLM performance increases significantly across various models. Notably, rerankingwithSPLADEv3andDeBERTa-v3consistentlyachievesthebestresultsacrossdatasetsand
metrics.Thisunderscoresthecriticalrolethathigh-qualityretrievalplaysindeterminingoverall
RAGsystemeffectiveness,suggestingthatIR-focusedLLMscouldbeavaluableassetinenhancing
generationperformance.
Impact of the Generator. The BERGEN study [114] compares the performance of LLMs with
gold passages (Oracle) against closed-book settings without retrieval, as shown in Figure 5b.
Surprisingly,theexperimentsdonotrevealastraightforwardrelationshipbetweenmodelsizeand
theperformancegainsfromretrieval.Forinstance,smallermodelslikeLLaMA2-7Bbenefitmore
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 25 -->

TheSurveyofRetrieval-AugmentedTextGenerationinLargeLanguageModels 25
0.85
0.80
0.75
0.70
0.65
0.60
0.6 0.7 0.8 0.9 1.0
Retrieval Performance (Recall@5)
)laveMLL(
ecnamrofreP

## Gar

1.0

### Oracle 0.8


### SPLADE-v3+RR

RetroMAE+RR 0.6

### SPLADE-v3

RetroMAE 0.4

## Bm25+Rr

0.2
0.0

## Bm25

TinyLlama-1.1B SOLAR-10.7B Mixtral-8x7B Llama3-8B Llama2-7B Llama2-70B

## Llm

(a)ImpactofretrievalperformanceonRAGperformanceforSOLAR-10.7B[75]onNQwithdifferentrankingsystems.RRmeanswithadditional
re-rankingusingDeBERTa-v3.
)laveMLL(
ecnamrofreP

## Gar

+0.06 +0.15 +0.03 +0.17
+0.15
+0.09

### Retriever

Closed Book

### Oracle

(b)Performancegainsw/andw/ooracleretrieval
forLLMswithdifferentsizes.Comparingclosed
book vs oracle passages averaged over all QA
datasetsinKILT.
(c)ThecorrelationbetweeneRAGandthedownstreamperformanceofdifferentLLMsizes.Inthisexperiment,
T5small(60Mparameters)andT5-base(220Mparameters)withFiDareused.Thedocumentsareretrieved
usingBM25.
Fig.5. RetrieverandgeneratorexperimentresultssourcedfromeRAG[119]andBERGEN[114].
fromretrievalthanlargermodelslikeLLaMA2-70B.Infact,LLaMA2-7Bwithretrievaloutperforms
LLaMA2-70Binaclosed-booksetting,suggestingthatretrievalaugmentationcanmakesmaller
modelsmorecompetitive.Similarly,resultsfromtheeRAGexperimentsinFigure5cindicatethat
varyingLLMsizes(e.g.,T5-smallvs.T5-base)doesnotsignificantlyaffectthecorrelationbetween
eRAGanddownstreamperformance.Thesefindingshighlightthatretrievalqualityhasamore
substantialimpactonRAGperformancethanthechoiceofgenerator,reinforcingthenotionthat
investinginbetterretrievalstrategiesoftenyieldsmorebenefitsthanrelyingsolelyonlargerLLMs.
9 ChallengesandFutureDirections
TheevolvinglandscapeofRAGsystemsfacessignificantchallengesthatimpactthequalityof
generatedoutputs,systemefficiency,andtheintegrationofmultimodaldata.Asthesesystems
becomemoreprevalentacrossarangeofapplications,addressingthesechallengesisessentialfor
improvingtheireffectivenessandscalability.
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 26 -->

26 Huangetal.
9.1 RetrievalQuality
ThequalityofretrievalisfundamentaltoanyeffectiveRAGsystem,directlyinfluencingtherelevanceandaccuracyofthegeneratedcontent[46,119,138,164].Currentretrievalmethods,however,
frequentlystrugglewithissueslikenoise,irrelevantdocuments,andfragmentedinformation,allof
whichcompromisethegenerationprocess.
NoiseRobustness. Irrelevantormisleadingdocumentswithintheretrievedsetcanintroduce
noise,leadingtohallucinationsorunreliableanswers.Thischallengehighlightstheneedformore
sophisticatedfilteringandcontext-awareretrievalmethodsthatcanbetterdifferentiaterelevant
from irrelevant content. However, Cuconasu et al. [26] present an interesting perspective by
showingthat,undercertainconditions,theinclusionofirrelevantdocumentscanenhanceoverall
accuracy.Thisfindingchallengesconventionalretrievalstrategiesandsuggeststhepotentialfor
developingspecializedapproachesthatstrategicallyintegratenoisewithintheretrievalprocess.
Negative Rejection. When retrieval fails to return relevant results, models often attempt to
generateresponsesregardless,increasingtheriskofincorrectoutputs.Thisissueisparticularly
problematicwhenqueriesarepoorlyexpressedorlacksufficientcontext,makingitdifficultfor
retrievalmodelstosurfacerelevantdocuments.Techniqueslikegeneratingapseudo-document
thatcapturesthequeryâ€™sessence,asdemonstratedbyHyDE[36],canhelpbridgethisgap.By
allowingretrievalsystemstofindmorerelevantdocumentsevenfromsuboptimalqueries,HyDE
improvesretrievalaccuracy,albeitwithatrade-offincomputationalcost.Futureresearchcould
focusonoptimizingthisprocesstobalanceimprovedretrievalaccuracywithreducedlatency.
InformationIntegration. Complexqueriesoftenrequiresynthesizinginformationfrommultipledocuments,yetfragmentedorconflictinginformationcanresultinincoherentorincomplete
answers.Pre-andpost-retrievaltechniquesplayacriticalroleinaddressingthischallenge.Enhancingretrievalgranularityandincorporatingtechniqueslikeentity-levelretrievalandre-ranking
can improve the cohesiveness of retrieved documents. However, many post-retrieval methods,
as investigated by Zhu et al. [165], rely heavily on calling LLM APIs, which incurs significant
costs.Exploringalternativessuchasknowledgedistillationtolightweightmodelscouldoffermore
scalablesolutions,makingadvancedretrievalstrategiesmorepracticalinonlinesettings.
Recent research highlights the development of generative models for search as a promising
directionforimprovingretrievalquality.ModelslikeGERE[15]andPARADE[84]enhancedocumentre-rankingandfactverificationbydirectlygeneratingrelevantdocumenttitlesorevidence
sentences.Fine-tuningpre-trainedmodelslikeRankT5[166]forranking-specifictaskshasalso
demonstratedpotentialinboostingout-of-domainperformance,whichiscrucialforgeneralizing
RAGsystemsacrossdiversecontexts.
9.2 SystemEfficiency
Systemefficiencyremainsasignificantbottleneck,especiallyasRAGsystemsscaletohandlelarge
datasetsandreal-timeapplications.Themulti-stepnatureofRAGworkflowsâ€”includingquery
classification,retrieval,re-ranking,andgenerationâ€”addscomplexityandlatency,whichcanhinder
overallperformance.
LatencyinRetrievalProcesses. Asdocumentcollectionsgrow,retrievalandre-rankingprocesses
increasinglybecomesourcesoflatency.Lightweightsearchmethodsandhybridretrievalapproaches
thatcombinesparseanddensetechniquesofferpotentialsolutionsbybalancingspeedandaccuracy.
Forexample,indexing,atraditionallyresource-intensiveprocess,hasseeninnovationsthrough
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 27 -->

TheSurveyofRetrieval-AugmentedTextGenerationinLargeLanguageModels 27
differentiablesearchindicessuchasDSI[128]andSEAL[7].Thesemethodsintegrateretrieval
withinTransformermodels,enablingdirectmappingoftextqueriestodocumentidentifiersand
therebyimprovingbothperformanceandretrievalefficiency.
ComputationalCosts. Theintroductionofdeeplearning-basedre-rankingmodelslikemonoT5
[102]andRankLLaMA[96]bringssignificantcomputationaloverhead,particularlyinscenarios
requiringiterativereasoning.Futureresearchcouldfocusonoptimizingthesemodelsordeveloping
retrievalpruningtechniquesthatreducethenumberofdocumentspassedtothegenerationphase
withoutsacrificingperformance[145].
Modular Workflow Optimization. The complexity of RAG systems often stems from interdependencies between components like chunking strategies, embedding models, and re-ranking
algorithms.Modulardesignsthatenableindependentoptimizationofeachstepwhileaccounting
forcross-componentinteractionsarekeytoenhancingsystemthroughput[39].Advancedchunking
methodsandhybridsearchstrategiescouldoffertrade-offsthatmaximizebothretrievalprecision
andspeed.AnexampleistheHybridwithHyDE[139]approach,whichintegratesbothsparseand
denseretrievaltocapturerelevantdocumentsfrombothlexicalandsemanticperspectives.
9.3 MultimodalRAG
The expansion of RAG systems to support multimodal dataâ€”encompassing text, images, and
audioâ€”presentsnewchallenges.Integratingdiversemodalitiesrequiresnotonlyeffectiveretrieval
butalsoseamlessalignmentandgenerationacrossdifferentdatatypes.
Cross-Modal Alignment. Aligning multimodal documents with text-based queries remains a
corechallenge.Thecomplexityofmappingdiversedatatypesintoaunifiedretrievalframework
necessitatesimprovedcross-modalretrievalstrategiescapableofsimultaneouslyhandlingtext,
image,andpotentiallyvideooraudiodata.
CoherentMultimodalGeneration. Generatingresponsesthatmeaningfullyintegrateinformation
frommultiplemodalitiesisanotherdifficulttask.Advancedgenerationmodelscapableofreasoning
acrossdifferentmodalitiesarerequiredtoproduceoutputsthatarebothcontextuallyrelevantand
visuallycoherent.
RecentadvancementsinmultimodalRAG,suchasMuRAG[17],REVEAL[49],andRe-ViLM
[152],haveshownpotentialinincorporatingmultimodalretrievalandgenerationintoreal-world
applicationslikevisualquestionanswering[18],imagecaptioning[120],andtext-to-audiogeneration[158].Movingforward,researchwilllikelyfocusonrefiningthesetechniques,especially
inscalingmultimodalretrievaltohandlelargerdatasetsandmorecomplexqueries.Extending
retrievalcapabilitiestoincludemorediversemediatypes,suchasvideoandspeech,alsorepresents
apromisingdirectionforthecontinuedevolutionofRAGsystems.
10 Conclusions
Inthispaper,wehavepresentedacomprehensiveframeworkforunderstandingtheRAGdomain,
highlightingitssignificanceinenhancingthecapabilitiesofLLMs.Throughastructuredoverview
of RAG, categorizing various methods, and an in-depth analysis of its core technologies and
evaluationmethods,thisstudyilluminatesthepathforfutureresearch.Itidentifiescrucialareas
forimprovementandoutlinespotentialdirectionsforadvancingRAGapplications,especiallyin
textualcontexts.ThissurveyaimstoelucidatethecoreconceptsoftheRAGfieldfromaretrieval
perspective, and it is intended to facilitate further exploration and innovation in the accurate
retrievalandgenerationofinformation.
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 28 -->

28 Huangetal.

### Acknowledgments

ThisresearchissupportedbytheNaturalSciencesandEngineeringResearchCouncil(NSERC)of
Canada.

### References

[1] OpenAIJoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,andetc.2023.GPT-4TechnicalReport.arXiv
(2023).
[2] QingyaoAi,KepingBi,JiafengGuo,andW.BruceCroft.2018.LearningaDeepListwiseContextModelforRanking
Refinement.InThe41stInternationalACMSIGIRConferenceonResearch&DevelopmentinInformationRetrieval.

## Acm.

[3] SunilArya,DavidM.Mount,NathanS.Netanyahu,RuthSilverman,andAngelaY.Wu.1998.AnOptimalAlgorithm
forApproximateNearestNeighborSearchingFixedDimensions.J.ACM45,6(1998),891â€“923. https://doi.org/10.
1145/293347.293348
[4] AkariAsai,ZeqiuWu,YizhongWang,AvirupSil,andHannanehHajishirzi.2024.Self-RAG:LearningtoRetrieve,
Generate,andCritiquethroughSelf-Reflection.InTheTwelfthInternationalConferenceonLearningRepresentations,
Vol.abs/2310.11511.
[5] MosheBerchansky,PeterIzsak,AviCaciularu,IdoDagan,andMosheWasserblat.2023. OptimizingRetrievalaugmentedReaderModelsviaTokenElimination.InProceedingsoftheConferenceonEmpiricalMethodsinNatural
LanguageProcessing.AssociationforComputationalLinguistics,1506â€“1524.
[6] MicheleBevilacqua,GiuseppeOttaviano,PatrickS.H.Lewis,ScottYih,SebastianRiedel,andFabioPetroni.2022.
AutoregressiveSearchEngines:GeneratingSubstringsasDocumentIdentifiers.InAdvancesinNeuralInformation
ProcessingSystems35:AnnualConferenceonNeuralInformationProcessingSystems2022,NeurIPS2022,NewOrleans,
LA,USA,November28-December9,2022,SanmiKoyejo,S.Mohamed,A.Agarwal,DanielleBelgrave,K.Cho,
andA.Oh(Eds.). http://papers.nips.cc/paper_files/paper/2022/hash/cd88d62a2063fdaf7ce6f9068fb15dcd-Abstract-

### Conference.html

[7] MicheleBevilacqua,GiuseppeOttaviano,PatrickS.H.Lewis,ScottYih,SebastianRiedel,andFabioPetroni.2022.
AutoregressiveSearchEngines:GeneratingSubstringsasDocumentIdentifiers.InConferenceonNeuralInformation
ProcessingSystems(NeurIPS).
[8] SidneyBlack,StellaBiderman,EricHallahan,QuentinAnthony,LeoGao,LaurenceGolding,HoraceHe,Connor
Leahy,KyleMcDonell,JasonPhang,MichaelPieler,UsvsnSaiPrashanth,ShivanshuPurohit,LariaReynolds,Jonathan
Tow,BenWang,andSamuelWeinbach.2022. GPT-NeoX-20B:AnOpen-SourceAutoregressiveLanguageModel.
InProceedingsofBigScienceEpisode#5â€“WorkshoponChallenges&PerspectivesinCreatingLargeLanguageModels,
Vol.abs/2204.06745.AssociationforComputationalLinguistics.
[9] SebastianBorgeaud,ArthurMensch,JordanHoffmann,TrevorCai,ElizaRutherford,KatieMillican,Georgevanden
Driessche,Jean-BaptisteLespiau,BogdanDamoc,AidanClark,DiegodeLasCasas,AureliaGuy,JacobMenick,Roman
Ring,TomHennigan,SaffronHuang,LorenMaggiore,ChrisJones,AlbinCassirer,AndyBrock,MichelaPaganini,
GeoffreyIrving,OriolVinyals,SimonOsindero,KarenSimonyan,JackW.Rae,ErichElsen,andLaurentSifre.2022.
ImprovingLanguageModelsbyRetrievingfromTrillionsofTokens.InInternationalConferenceonMachineLearning

## (Icml).2206â€“2240.

[10] TomB.Brown,BenjaminMann,NickRyder,MelanieSubbiah,JaredKaplan,PrafullaDhariwal,ArvindNeelakantan,
PranavShyam,GirishSastry,AmandaAskell,SandhiniAgarwal,ArielHerbert-Voss,GretchenKrueger,TomHenighan,
RewonChild,AdityaRamesh,DanielM.Ziegler,JeffreyWu,ClemensWinter,ChristopherHesse,MarkChen,Eric
Sigler,MateuszLitwin,ScottGray,BenjaminChess,JackClark,ChristopherBerner,SamMcCandlish,AlecRadford,
IlyaSutskever,andDarioAmodei.2020.LanguageModelsareFew-ShotLearners.InConferenceonNeuralInformation
ProcessingSystems(NeurIPS),Vol.abs/2005.14165.
[11] JannisBulian,ChristianBuck,WojciechGajewski,BenjaminBÃ¶rschinger,andTalSchuster.2022.Tomayto,Tomahto.
BeyondToken-levelAnswerEquivalenceforQuestionAnsweringEvaluation..InConferenceonEmpiricalMethodsin
NaturalLanguageProcessing(EMNLP).291â€“305.
[12] Chi-MinChan,ChunpuXu,RuibinYuan,HongyinLuo,WeiXue,YikeGuo,andJieFu.2024.RQ-RAG:Learningto
RefineQueriesforRetrievalAugmentedGeneration.arXivabs/2404.00610(2024).
[13] HowardChen,RamakanthPasunuru,JasonWeston,andAsliCelikyilmaz.2023.WalkingDowntheMemoryMaze:
BeyondContextLimitthroughInteractiveReading.arXivabs/2310.05029(2023).
[14] JiaweiChen,HongyuLin,XianpeiHan,andLeSun.2024. BenchmarkingLargeLanguageModelsinRetrieval-
AugmentedGeneration.ProceedingsoftheAAAIConferenceonArtificialIntelligence38,16(2024),17754â€“17762.
[15] JianguiChen,RuqingZhang,JiafengGuo,YixingFan,andXueqiCheng.2022.GERE:GenerativeEvidenceRetrieval
forFactVerification.InProceedingsofthe45thInternationalACMSIGIRConferenceonResearchandDevelopmentin
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 29 -->

TheSurveyofRetrieval-AugmentedTextGenerationinLargeLanguageModels 29
InformationRetrieval.ACM.
[16] MarkChen,JerryTworek,HeewooJun,QimingYuan,HenriquePondÃ©deOliveiraPinto,JaredKaplan,Harrison
Edwards,YuriBurda,NicholasJoseph,GregBrockman,AlexRay,RaulPuri,GretchenKrueger,MichaelPetrov,
HeidyKhlaaf,GirishSastry,PamelaMishkin,BrookeChan,ScottGray,NickRyder,MikhailPavlov,AletheaPower,
LukaszKaiser,MohammadBavarian,ClemensWinter,PhilippeTillet,FelipePetroskiSuch,DaveCummings,Matthias
Plappert,FotiosChantzis,ElizabethBarnes,ArielHerbert-Voss,WilliamHebgenGuss,AlexNichol,AlexPaino,Nikolas
Tezak,JieTang,IgorBabuschkin,SuchirBalaji,ShantanuJain,WilliamSaunders,ChristopherHesse,AndrewN.
Carr,JanLeike,JoshuaAchiam,VedantMisra,EvanMorikawa,AlecRadford,MatthewKnight,MilesBrundage,Mira
Murati,KatieMayer,PeterWelinder,BobMcGrew,DarioAmodei,SamMcCandlish,IlyaSutskever,andWojciech
Zaremba.2021.EvaluatingLargeLanguageModelsTrainedonCode.arXivabs/2107.03374(2021).
[17] WenhuChen,HexiangHu,XiChen,PatVerga,andWilliamCohen.2022.MuRAG:MultimodalRetrieval-Augmented
GeneratorforOpenQuestionAnsweringoverImagesandText.InProceedingsoftheConferenceonEmpiricalMethods
inNaturalLanguageProcessing(EMNLP).
[18] WenhuChen,HexiangHu,ChitwanSaharia,andWilliamW.Cohen.2023.Re-Imagen:Retrieval-AugmentedText-to-
ImageGenerator.InInternationalConferenceonLearningRepresentations(ICLR).
[19] ZhihongChen,FengJiang,JunyingChen,TiannanWang,FeiYu,GuimingChen,HongboZhang,JuhaoLiang,Chen
Zhang,ZhiyiZhang,JianquanLi,XiangWan,BenyouWang,andHaizhouLi.2023.Phoenix:DemocratizingChatGPT
acrossLanguages.arXivabs/2304.10453(2023).
[20] DaixuanCheng,ShaohanHuang,JunyuBi,YuefengZhan,JianfengLiu,YujingWang,HaoSun,FuruWei,Weiwei
Deng,andQiZhang.2023.UPRISE:UniversalPromptRetrievalforImprovingZero-ShotEvaluation.InProceedings
oftheConferenceonEmpiricalMethodsinNaturalLanguageProcessing.AssociationforComputationalLinguistics,
12318â€“12337.
[21] XinCheng,DiLuo,XiuyingChen,LemaoLiu,DongyanZhao,andRuiYan.2023.LiftYourselfUp:Retrieval-augmented
TextGenerationwithSelf-Memory..InConferenceonNeuralInformationProcessingSystems(NeurIPS).
[22] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,LianminZheng,SiyuanZhuang,Yonghao
Zhuang,JosephE.Gonzalez,IonStoica,andEricP.Xing.2023.Vicuna:AnOpen-SourceChatbotImpressingGPT-4
with90%*ChatGPTQuality. https://lmsys.org/blog/2023-03-30-vicuna/
[23] AakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,AdamRoberts,PaulBarham,
HyungWonChung,CharlesSutton,SebastianGehrmann,ParkerSchuh,KensenShi,SashaTsvyashchenko,Joshua
Maynez,AbhishekRao,ParkerBarnes,YiTay,NoamShazeer,VinodkumarPrabhakaran,EmilyReif,NanDu,Ben
Hutchinson,ReinerPope,JamesBradbury,JacobAustin,MichaelIsard,GuyGur-Ari,PengchengYin,TojuDuke,
AnselmLevskaya,SanjayGhemawat,SunipaDev,HenrykMichalewski,XavierGarcia,VedantMisra,KevinRobinson,
LiamFedus,DennyZhou,DaphneIppolito,DavidLuan,HyeontaekLim,BarretZoph,AlexanderSpiridonov,Ryan
Sepassi,DavidDohan,ShivaniAgrawal,MarkOmernick,AndrewM.Dai,ThanumalayanSankaranarayanaPillai,
MariePellat,AitorLewkowycz,EricaMoreira,RewonChild,OleksandrPolozov,KatherineLee,ZongweiZhou,
XuezhiWang,BrennanSaeta,MarkDiaz,OrhanFirat,MicheleCatasta,JasonWei,KathyMeier-Hellstern,Douglas
Eck,JeffDean,SlavPetrov,andNoahFiedel.2023. PaLM:ScalingLanguageModelingwithPathways. Journalof
MachineLearningResearch(JMLR)24(2023),240:1â€“240:113.
[24] HyungWonChung,LeHou,ShayneLongpre,BarretZoph,YiTay,WilliamFedus,YunxuanLi,XuezhiWang,
MostafaDehghani,SiddharthaBrahma,AlbertWebson,ShixiangShaneGu,ZhuyunDai,MiracSuzgun,XinyunChen,
AakankshaChowdhery,AlexCastro-Ros,MariePellat,KevinRobinson,DashaValter,SharanNarang,GauravMishra,
AdamsYu,VincentZhao,YanpingHuang,AndrewDai,HongkunYu,SlavPetrov,EdH.Chi,JeffDean,JacobDevlin,
AdamRoberts,DennyZhou,QuocV.Le,andJasonWei.2022.ScalingInstruction-FinetunedLanguageModels.arXiv
abs/2210.11416(2022).
[25] AlexisConneau,KartikayKhandelwal,NamanGoyal,VishravChaudhary,GuillaumeWenzek,FranciscoGuzmÃ¡n,
EdouardGrave,MyleOtt,LukeZettlemoyer,andVeselinStoyanov.2020.UnsupervisedCross-lingualRepresentation
LearningatScale.InProceedingsofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics.Association
forComputationalLinguistics,8440â€“8451.
[26] FlorinCuconasu,GiovanniTrappolini,FedericoSiciliano,SimoneFilice,CesareCampagnano,YoelleMaarek,Nicola
Tonellotto,andFabrizioSilvestri.2024. ThePowerofNoise:RedefiningRetrievalforRAGSystems.InAnnual
InternationalACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval(SIGIR),Vol.abs/2401.14887.
[27] ZhuyunDai,VincentY.Zhao,JiMa,YiLuan,JianmoNi,JingLu,AntonBakalov,KelvinGuu,KeithB.Hall,and
Ming-WeiChang.2023.Promptagator:Few-shotDenseRetrievalFrom8Examples.InInternationalConferenceon
LearningRepresentations(ICLR).
[28] MayurDatar,NicoleImmorlica,PiotrIndyk,andVahabS.Mirrokni.2004.Locality-sensitivehashingschemebased
onp-stabledistributions..InInternationalSymposiumonComputationalGeometry(SoCG).253â€“262.
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 30 -->

30 Huangetal.
[29] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.BERT:Pre-trainingofDeepBidirectional
TransformersforLanguageUnderstanding.InProceedingsoftheConferenceoftheNorth.AssociationforComputational
Linguistics,4171â€“4186.
[30] EmilyDinan,StephenRoller,KurtShuster,AngelaFan,MichaelAuli,andJasonWeston.2019.WizardofWikipedia:
Knowledge-PoweredConversationalAgents.InInternationalConferenceonLearningRepresentations(ICLR).
[31] ZhengxiaoDu,YujieQian,XiaoLiu,MingDing,JiezhongQiu,ZhilinYang,andJieTang.2022. GLM:General
LanguageModelPretrainingwithAutoregressiveBlankInfilling.InProceedingsofthe60thAnnualMeetingofthe
AssociationforComputationalLinguistics(Volume1:LongPapers).AssociationforComputationalLinguistics.
[32] HadyElSahar,PavlosVougiouklis,ArslenRemaci,ChristopheGravier,JonathonS.Hare,FrÃ©dÃ©riqueLaforest,andElena
Simperl.2018.T-REx:ALargeScaleAlignmentofNaturalLanguagewithKnowledgeBaseTriples.InInternational
ConferenceonLanguageResourcesandEvaluation(LREC).
[33] ShahulES,JithinJames,LuisEspinosaAnke,andStevenSchockaert.2023. RAGAs:AutomatedEvaluationof
RetrievalAugmentedGeneration.ConferenceoftheEuropeanChapteroftheAssociationforComputationalLinguistics
abs/2309.15217(2023).
[34] ZhangyinFeng,XiaochengFeng,DezhiZhao,MaojinYang,andBingQin.2024. Retrieval-GenerationSynergy
AugmentedLargeLanguageModels.InIEEEInternationalConferenceonAcoustics,Speech,andSignalProcessing,
Vol.abs/2310.05149.IEEE.
[35] LeoGao,StellaBiderman,SidBlack,LaurenceGolding,TravisHoppe,CharlesFoster,JasonPhang,HoraceHe,Anish
Thite,NoaNabeshima,ShawnPresser,andConnorLeahy.2021. ThePile:An800GBDatasetofDiverseTextfor
LanguageModeling.arXivabs/2101.00027(2021).
[36] LuyuGao,XueguangMa,JimmyLin,andJamieCallan.2023.PreciseZero-ShotDenseRetrievalwithoutRelevance
Labels.InProceedingsofthe61stAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:Long
Papers).AssociationforComputationalLinguistics,1762â€“1777.
[37] TianyuGao,XingchengYao,andDanqiChen.2021.SimCSE:SimpleContrastiveLearningofSentenceEmbeddings.
InProceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguageProcessing.AssociationforComputational
Linguistics,6894â€“6910.
[38] YunfanGao,YunXiong,XinyuGao,KangxiangJia,JinliuPan,YuxiBi,YiDai,JiaweiSun,MengWang,andHaofen
Wang.2023.Retrieval-AugmentedGenerationforLargeLanguageModels:ASurvey.arXivabs/2312.10997(2023).
[39] YunfanGao,YunXiong,MengWang,andHaofenWang.2024. ModularRAG:TransformingRAGSystemsinto
LEGO-likeReconfigurableFrameworks.arXiv(2024).
[40] MichaelGlass,GaetanoRossiello,MdFaisalMahbubChowdhury,AnkitaNaik,PengshanCai,andAlfioGliozzo.2022.
Re2G:Retrieve,Rerank,Generate.InProceedingsoftheConferenceoftheNorthAmericanChapteroftheAssociation
forComputationalLinguistics:HumanLanguageTechnologies.AssociationforComputationalLinguistics,2701â€“2715.
[41] SimonGottschalkandElenaDemidova.2018. EventKG:AMultilingualEvent-CentricTemporalKnowledgeGraph.
SpringerInternationalPublishing.272â€“287pages.
[42] KelvinGuu,KentonLee,ZoraTung,PanupongPasupat,andMing-WeiChang.2020.RetrievalAugmentedLanguage
ModelPre-Training.InInternationalConferenceonMachineLearning(ICML).3929â€“3938.
[43] WilliamL.Hamilton.2020.Graphrepresentationlearning.SpringerInternationalPublishing.
[44] MichelineHancock-Beaulieu,MikeGatford,XiangjiHuang,StephenE.Robertson,SteveWalker,andP.W.Williams.

## OkapiatTREC-5.InProceedingsofTheFifthTextREtrievalConference,TREC1996,Gaithersburg,Maryland,

USA,November20-22,1996(NISTSpecialPublication,Vol.500-238),EllenM.VoorheesandDonnaK.Harman(Eds.).
NationalInstituteofStandardsandTechnology(NIST). http://trec.nist.gov/pubs/trec5/papers/city.procpaper.ps.gz
[45] DanHendrycks,CollinBurns,StevenBasart,AndyZou,MantasMazeika,DawnSong,andJacobSteinhardt.2021.
MeasuringMassiveMultitaskLanguageUnderstanding..InInternationalConferenceonLearningRepresentations

## (Iclr).

[46] EnriqueHerreraViedma,GabriellaPasi,AntonioG.LopezHerrera,andCarlosPorcel.2006.Evaluatingtheinformation
qualityofWebsites:Amethodologybasedonfuzzycomputingwithwords. JournaloftheAmericanSocietyfor
InformationScienceandTechnology57,4(2006),538â€“549.
[47] SebastianHofstÃ¤tter,JiecaoChen,KarthikRaman,andHamedZamani.2023.Fid-light:Efficientandeffectiveretrievalaugmentedtextgeneration.InProceedingsofthe46thInternationalACMSIGIRConferenceonResearchandDevelopment
inInformationRetrieval.1437â€“1447.
[48] YuchengHuandYuxingLu.2024.RAGandRAU:ASurveyonRetrieval-AugmentedLanguageModelinNatural
LanguageProcessing.arXivabs/2404.19543(2024).
[49] ZiniuHu,AhmetIscen,ChenSun,ZiruiWang,Kai-WeiChang,YizhouSun,CordeliaSchmid,DavidA.Ross,and
AlirezaFathi.2023. Reveal:Retrieval-AugmentedVisual-LanguagePre-TrainingwithMulti-SourceMultimodal
KnowledgeMemory.In2023IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR).IEEE,23369â€“
23379.
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 31 -->

TheSurveyofRetrieval-AugmentedTextGenerationinLargeLanguageModels 31
[50] JieHuang,HanyinShao,KevinChen-ChuanChang,JinjunXiong,andWen-meiHwu.2022.UnderstandingJargon:
CombiningExtractionandGenerationforDefinitionModeling.InProceedingsoftheConferenceonEmpiricalMethods
inNaturalLanguageProcessing.AssociationforComputationalLinguistics.
[51] JimmyXiangjiHuang,JunMiao,andBenHe.2013.Highperformancequeryexpansionusingadaptiveco-training.
Inf.Process.Manag.49,2(2013),441â€“453. https://doi.org/10.1016/J.IPM.2012.08.002
[52] QiushiHuang,ShuaiFu,XuboLiu,WenwuWang,TomKo,YuZhang,andLilianH.Y.Tang.2023.LearningRetrieval
AugmentationforPersonalizedDialogueGeneration..InConferenceonEmpiricalMethodsinNaturalLanguage
Processing(EMNLP).2523â€“2540.
[53] WenyuHuang,MirellaLapata,PavlosVougiouklis,NikosPapasarantopoulos,andJeffZPan.2023. Retrieval
AugmentedGenerationwithRichAnswerEncoding.Proc.ofIJCNLP-AACL2023(2023).
[54] XiangjiHuangandQinminHu.2009.Abayesianlearningapproachtopromotingdiversityinrankingforbiomedical
informationretrieval.InProceedingsofthe32ndAnnualInternationalACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval,SIGIR2009,Boston,MA,USA,July19-23,2009,JamesAllan,JavedA.Aslam,Mark
Sanderson,ChengXiangZhai,andJustinZobel(Eds.).ACM,307â€“314. https://doi.org/10.1145/1571941.1571995
[55] YizhengHuangandJimmyHuang.2024.ExploringChatGPTforNext-generationInformationRetrieval:Opportunities
andChallenges.CoRRabs/2402.11203(2024). https://doi.org/10.48550/ARXIV.2402.11203arXiv:2402.11203
[56] YizhengHuangandJimmyX.Huang.2023. DiversifiedPriorKnowledgeEnhancedGeneralLanguageModelfor
BiomedicalInformationRetrieval.InECAI2023-26thEuropeanConferenceonArtificialIntelligence,September30-
October4,2023,KrakÃ³w,Poland-Including12thConferenceonPrestigiousApplicationsofIntelligentSystems(PAIS2023)
(FrontiersinArtificialIntelligenceandApplications,Vol.372),KobiGal,AnnNowÃ©,GrzegorzJ.Nalepa,RoyFairstein,
andRoxanaRadulescu(Eds.).IOSPress,1109â€“1115. https://doi.org/10.3233/FAIA230385
[57] GautierIzacard,MathildeCaron,LucasHosseini,SebastianRiedel,PiotrBojanowski,ArmandJoulin,andEdouard
Grave.2022.UnsupervisedDenseInformationRetrievalwithContrastiveLearning.TransactionsonMachineLearning
Research(TMLR)2022(2022).
[58] GautierIzacardandEdouardGrave.2021.LeveragingPassageRetrievalwithGenerativeModelsforOpenDomain
QuestionAnswering.InProceedingsofthe16thConferenceoftheEuropeanChapteroftheAssociationforComputational
Linguistics:MainVolume.AssociationforComputationalLinguistics,874â€“880.
[59] GautierIzacard,PatrickS.H.Lewis,MariaLomeli,LucasHosseini,FabioPetroni,TimoSchick,JaneDwivedi-Yu,
ArmandJoulin,SebastianRiedel,andEdouardGrave.2023. Atlas:Few-shotLearningwithRetrievalAugmented
LanguageModels.JournalofMachineLearningResearch(JMLR)24(2023),251:1â€“251:43.
[60] IsratJahan,Md.TahmidRahmanLaskar,ChunPeng,andJimmyXiangjiHuang.2023.EvaluationofChatGPTon
BiomedicalTasks:AZero-ShotComparisonwithFine-TunedGenerativeTransformers.CoRRabs/2306.04504(2023).
https://doi.org/10.48550/ARXIV.2306.04504arXiv:2306.04504
[61] BernardJ.Jansen,DanielleL.Booth,andAmandaSpink.2009.PatternsofqueryreformulationduringWebsearching.
J.Assoc.Inf.Sci.Technol.60,7(2009),1358â€“1371. https://doi.org/10.1002/ASI.21071
[62] HJÃ©gou,MDouze,andCSchmid.2011.ProductQuantizationforNearestNeighborSearch.IEEETransactionson
PatternAnalysisandMachineIntelligence33,1(2011),117â€“128.
[63] WenqiJiang,MarcoZeller,RogerWaleffe,TorstenHoefler,andGustavoAlonso.2023.Chameleon:aHeterogeneous
andDisaggregatedAcceleratorSystemforRetrieval-AugmentedLanguageModels.arXivabs/2310.09949(2023).
[64] WenqiJiang,ShuaiZhang,BoranHan,JieWang,BernieWang,andTimKraska.2024. PipeRAG:FastRetrieval-
AugmentedGenerationviaAlgorithm-SystemCo-design.arXivabs/2403.05676(2024).
[65] ZhengbaoJiang,FrankF.Xu,LuyuGao,ZhiqingSun,QianLiu,JaneDwivedi-Yu,YimingYang,JamieCallan,and
GrahamNeubig.2023. ActiveRetrievalAugmentedGeneration.InConferenceonEmpiricalMethodsinNatural
LanguageProcessing(EMNLP).7969â€“7992.
[66] DiJin,EileenPan,NassimOufattole,Wei-HungWeng,HanyiFang,andPeterSzolovits.2021.WhatDiseaseDoes
ThisPatientHave?ALarge-ScaleOpenDomainQuestionAnsweringDatasetfromMedicalExams.AppliedSciences
11,14(2021),6421.
[67] QiaoJin,BhuwanDhingra,ZhengpingLiu,WilliamW.Cohen,andXinghuaLu.2019. PubMedQA:ADatasetfor
BiomedicalResearchQuestionAnswering..InConferenceonEmpiricalMethodsinNaturalLanguageProcessing

## (Emnlp).2567â€“2577.

[68] JeffJohnson,MatthijsDouze,andHervÃ©JÃ©gou.2021.Billion-ScaleSimilaritySearchwithGPUs.IEEETransactionson
BigData7,3(2021),535â€“547. https://doi.org/10.1109/TBDATA.2019.2921572
[69] MandarJoshi,EunsolChoi,DanielWeld,andLukeZettlemoyer.2017.TriviaQA:ALargeScaleDistantlySupervised
ChallengeDatasetforReadingComprehension.InProceedingsofthe55thAnnualMeetingoftheAssociationfor
ComputationalLinguistics(Volume1:LongPapers).AssociationforComputationalLinguistics,1601â€“1611.
[70] MinkiKang,JinMyungKwak,JinheonBaek,andSungJuHwang.2023.KnowledgeGraph-AugmentedLanguage
ModelsforKnowledge-GroundedDialogueGeneration.arXivabs/2305.18846(2023).
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 32 -->

32 Huangetal.
[71] VladimirKarpukhin,BarlasOguz,SewonMin,PatrickS.H.Lewis,LedellWu,SergeyEdunov,DanqiChen,and
Wen-tauYih.2020. DensePassageRetrievalforOpen-DomainQuestionAnswering..InConferenceonEmpirical
MethodsinNaturalLanguageProcessing(EMNLP).6769â€“6781.
[72] UrvashiKhandelwal,OmerLevy,DanJurafsky,LukeZettlemoyer,andMikeLewis.2020.Generalizationthrough
Memorization:NearestNeighborLanguageModels.InInternationalConferenceonLearningRepresentations(ICLR).
[73] OmarKhattab,KeshavSanthanam,XiangLisaLi,DavidHall,PercyLiang,ChristopherPotts,andMateiZaharia.

### Demonstrate-Search-Predict:Composingretrievalandlanguagemodelsforknowledge-intensiveNLP.arXiv

abs/2212.14024(2022).
[74] OmarKhattabandMateiZaharia.2020.ColBERT-EfficientandEffectivePassageSearchviaContextualizedLate
InteractionoverBERT.InProceedingsofthe43rdInternationalACMSIGIRConferenceonResearchandDevelopmentin
InformationRetrieval.ACM,39â€“48.
[75] SanghoonKim,DahyunKim,ChanjunPark,WonsungLee,WonhoSong,YunsuKim,HyeonwooKim,YungiKim,
HyeonjuLee,JihooKim,ChangbaeAhn,SeonghoonYang,SukyungLee,HyunbyungPark,GyoungjinGim,MikyoungCha,HwalsukLee,andSunghunKim.2024.SOLAR10.7B:ScalingLargeLanguageModelswithSimpleyet
EffectiveDepthUp-Scaling.InProceedingsoftheConferenceoftheNorthAmericanChapteroftheAssociationfor
ComputationalLinguistics:HumanLanguageTechnologies(Volume6:IndustryTrack),Vol.abs/2312.15166.Association
forComputationalLinguistics.
[76] TomKwiatkowski,JennimariaPalomaki,OliviaRedfield,MichaelCollins,AnkurParikh,ChrisAlberti,Danielle
Epstein,IlliaPolosukhin,JacobDevlin,KentonLee,KristinaToutanova,LlionJones,MatthewKelcey,Ming-Wei
Chang,AndrewM.Dai,JakobUszkoreit,QuocLe,andSlavPetrov.2019. NaturalQuestions:ABenchmarkfor
QuestionAnsweringResearch.TransactionsoftheAssociationforComputationalLinguistics7(2019),453â€“466.
[77] Md.TahmidRahmanLaskar,M.SaifulBari,MizanurRahman,MdAmranHossenBhuiyan,ShafiqJoty,andJimmyXiangjiHuang.2023.ASystematicStudyandComprehensiveEvaluationofChatGPTonBenchmarkDatasets.CoRR
abs/2305.18486(2023). https://doi.org/10.48550/ARXIV.2305.18486arXiv:2305.18486
[78] Md.TahmidRahmanLaskar,EnamulHoque,andJimmyX.Huang.2020.QueryFocusedAbstractiveSummarization
viaIncorporatingQueryRelevanceandTransferLearningwithTransformerModels.InAdvancesinArtificial
Intelligence-33rdCanadianConferenceonArtificialIntelligence,CanadianAI2020,Ottawa,ON,Canada,May13-15,
2020,Proceedings(LectureNotesinComputerScience,Vol.12109),CyrilGoutteandXiaodanZhu(Eds.).Springer,
342â€“348. https://doi.org/10.1007/978-3-030-47358-7_35
[79] CarlosLassanceandStÃ©phaneClinchant.2022. NaverLabsEurope(SPLADE)@TRECNeuCLIR2022..InText
RetrievalConference(TREC).
[80] CarlosLassance,HervÃ©DÃ©jean,ThibaultFormal,andStÃ©phaneClinchant.2024. SPLADE-v3:Newbaselinesfor
SPLADE.arXivabs/2403.06789(2024).
[81] MyeonghwaLee,SeonhoAn,andMin-SooKim.2024.PlanRAG:APlan-then-RetrievalAugmentedGenerationfor
GenerativeLargeLanguageModelsasDecisionMakers.InProceedingsoftheConferenceoftheNorthAmericanChapter
oftheAssociationforComputationalLinguistics:HumanLanguageTechnologies(Volume1:LongPapers).Association
forComputationalLinguistics.
[82] MikeLewis,YinhanLiu,NamanGoyal,MarjanGhazvininejad,AbdelrahmanMohamed,OmerLevy,VeselinStoyanov,
andLukeZettlemoyer.2020.BART:DenoisingSequence-to-SequencePre-trainingforNaturalLanguageGeneration,
Translation,andComprehension.InProceedingsofthe58thAnnualMeetingoftheAssociationforComputational
Linguistics.AssociationforComputationalLinguistics,7871â€“7880.
[83] PatrickS.H.Lewis,EthanPerez,AleksandraPiktus,FabioPetroni,VladimirKarpukhin,NamanGoyal,Heinrich
KÃ¼ttler,MikeLewis,Wen-tauYih,TimRocktÃ¤schel,SebastianRiedel,andDouweKiela.2020.Retrieval-Augmented
GenerationforKnowledge-IntensiveNLPTasks..InConferenceonNeuralInformationProcessingSystems(NeurIPS).
[84] CanjiaLi,AndrewYates,SeanMacAvaney,BenHe,andYingfeiSun.2024. PARADE:PassageRepresentation
AggregationforDocumentReranking.ACMTransactionsonInformationSystems42,2(2024),1â€“26.
[85] HuayangLi,YixuanSu,DengCai,YanWang,andLemaoLiu.2022.ASurveyonRetrieval-AugmentedTextGeneration.
arXivabs/2202.01110(2022).
[86] XingxuanLi,RuochenZhao,YewKenChia,BoshengDing,ShafiqR.Joty,SoujanyaPoria,andLidongBing.2024.
Chain-of-Knowledge:GroundingLargeLanguageModelsviaDynamicKnowledgeAdaptingoverHeterogeneous
Sources.InTheTwelfthInternationalConferenceonLearningRepresentations.
[87] Chin-YewLin.2004.ROUGE:APackageforAutomaticEvaluationofSummaries.InTextSummarizationBranches
Out.AssociationforComputationalLinguistics,Barcelona,Spain,74â€“81. https://aclanthology.org/W04-1013
[88] Sheng-ChiehLin,AkariAsai,MinghanLi,BarlasOguz,JimmyLin,YasharMehdad,Wen-tauYih,andXilunChen.

### HowtoTrainYourDragon:DiverseAugmentationTowardsGeneralizableDenseRetrieval.InFindingsofthe

AssociationforComputationalLinguistics:EMNLP2023.AssociationforComputationalLinguistics,6385â€“6400.
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 33 -->

TheSurveyofRetrieval-AugmentedTextGenerationinLargeLanguageModels 33
[89] XiVictoriaLin,XilunChen,MingdaChen,WeijiaShi,MariaLomeli,RichardJames,PedroRodriguez,JacobKahn,
GergelySzilvasy,MikeLewis,LukeZettlemoyer,andWen-tauYih.2024. RA-DIT:Retrieval-AugmentedDual
InstructionTuning.InTheTwelfthInternationalConferenceonLearningRepresentations,Vol.abs/2310.01352.
[90] XiVictoriaLin,TodorMihaylov,MikelArtetxe,TianluWang,ShuohuiChen,DanielSimig,MyleOtt,NamanGoyal,
ShrutiBhosale,JingfeiDu,RamakanthPasunuru,SamShleifer,PunitSinghKoura,VishravChaudhary,BrianOâ€™Horo,
JeffWang,LukeZettlemoyer,ZornitsaKozareva,MonaDiab,VeselinStoyanov,andXianLi.2022.Few-shotLearning
withMultilingualGenerativeLanguageModels.InProceedingsoftheConferenceonEmpiricalMethodsinNatural
LanguageProcessing.AssociationforComputationalLinguistics.
[91] YiLiu,LianzheHuang,ShichengLi,SishuoChen,HaoZhou,FandongMeng,JieZhou,andXuSun.2023.RECALL:A
BenchmarkforLLMsRobustnessagainstExternalCounterfactualKnowledge.arXivabs/2311.08147(2023).
[92] ZiyangLuo,CanXu,PuZhao,XiuboGeng,ChongyangTao,JingMa,QingweiLin,andDaxinJiang.2023.Augmented
LargeLanguageModelswithParametricKnowledgeGuiding.arXivabs/2305.04757(2023).
[93] HengzhaoMa,JianzhongLi,andYongZhang.2024.ReconsideringTreebasedMethodsfork-MaximumInner-Product
Search:TheLRUS-CoverTree.In2024IEEE40thInternationalConferenceonDataEngineering(ICDE).IEEE.
[94] XinbeiMa,YeyunGong,PengchengHe,HaiZhao,andNanDuan.2023.QueryRewritinginRetrieval-Augmented
LargeLanguageModels.InProceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguageProcessing.
AssociationforComputationalLinguistics,5303â€“5315.
[95] XueguangMa,LiangWang,NanYang,FuruWei,andJimmyLin.2024.Fine-TuningLLaMAforMulti-StageText
Retrieval.InProceedingsofthe47thInternationalACMSIGIRConferenceonResearchandDevelopmentinInformation
Retrieval,Vol.1.ACM,2421â€“2425.
[96] XueguangMa,LiangWang,NanYang,FuruWei,andJimmyLin.2024.Fine-TuningLLaMAforMulti-StageText
Retrieval.InProceedingsofthe47thInternationalACMSIGIRConferenceonResearchandDevelopmentinInformation
Retrieval,SIGIR2024,WashingtonDC,USA,July14-18,2024,GraceHuiYang,HongningWang,SamHan,Claudia
Hauff,GuidoZuccon,andYiZhang(Eds.).ACM,2421â€“2425. https://doi.org/10.1145/3626772.3657951
[97] YuA.MalkovandD.A.Yashunin.2020.EfficientandRobustApproximateNearestNeighborSearchUsingHierarchical
NavigableSmallWorldGraphs.IEEETransactionsonPatternAnalysisandMachineIntelligence42,4(2020),824â€“836.
https://doi.org/10.1109/TPAMI.2018.2889473
[98] ChristopherD.Manning,PrabhakarRaghavan,andHinrichSchÃ¼tze.2008. Introductiontoinformationretrieval.
CambridgeUniversityPress. https://doi.org/10.1017/CBO9780511809071
[99] NiklasMuennighoff.2022.SGPT:GPTSentenceEmbeddingsforSemanticSearch.arXivabs/2202.08904(2022).
[100] ReiichiroNakano,JacobHilton,SuchirBalaji,JeffWu,LongOuyang,ChristinaKim,ChristopherHesse,Shantanu
Jain,VineetKosaraju,WilliamSaunders,XuJiang,KarlCobbe,TynaEloundou,GretchenKrueger,KevinButton,
MatthewKnight,BenjaminChess,andJohnSchulman.2021.WebGPT:Browser-assistedquestion-answeringwith
humanfeedback.arXivabs/2112.09332(2021).
[101] JianmoNi,ChenQu,JingLu,ZhuyunDai,GustavoHernandezAbrego,JiMa,VincentZhao,YiLuan,KeithHall,Ming-
WeiChang,andYinfeiYang.2022.LargeDualEncodersAreGeneralizableRetrievers.InProceedingsoftheConference
onEmpiricalMethodsinNaturalLanguageProcessing.AssociationforComputationalLinguistics,9844â€“9855.
[102] RodrigoNogueira,ZhiyingJiang,RonakPradeep,andJimmyLin.2020. DocumentRankingwithaPretrained
Sequence-to-SequenceModel.InFindingsoftheAssociationforComputationalLinguistics:EMNLP2020.Association
forComputationalLinguistics.
[103] RodrigoNogueira,WeiYang,KyunghyunCho,andJimmyLin.2019.Multi-stagedocumentrankingwithBERT.CoRR
abs/1910.14424(2019).
[104] LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollL.Wainwright,PamelaMishkin,ChongZhang,Sandhini
Agarwal,KatarinaSlama,AlexRay,JohnSchulman,JacobHilton,FraserKelton,LukeMiller,MaddieSimens,Amanda
Askell,PeterWelinder,PaulF.Christiano,JanLeike,andRyanLowe.2022. Traininglanguagemodelstofollow
instructionswithhumanfeedback.InConferenceonNeuralInformationProcessingSystems(NeurIPS).
[105] AnkitPal,LogeshKumarUmapathi,andMalaikannanSankarasubbu.2022.MedMCQA:ALarge-scaleMulti-Subject
Multi-ChoiceDatasetforMedicaldomainQuestionAnswering..InConferenceonHealth,Inference,andLearning

## (Chil).248â€“260.

[106] YuPan,JianxinSun,andHongfengYu.2023. LM-DiskANN:LowMemoryFootprintinDisk-NativeDynamic
Graph-BasedANNIndexing.In2023IEEEInternationalConferenceonBigData(BigData).IEEE.
[107] KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu.2002. BLEU:amethodforautomaticevaluation
ofmachinetranslation.InProceedingsofthe40thAnnualMeetingonAssociationforComputationalLinguistics
(Philadelphia,Pennsylvania)(ACLâ€™02).AssociationforComputationalLinguistics,USA,311â€“318. https://doi.org/10.
3115/1073083.1073135
[108] GuilhermePenedo,QuentinMalartic,DanielHesslow,RuxandraCojocaru,HamzaAlobeidli,AlessandroCappelli,BaptistePannier,EbtesamAlmazrouei,andJulienLaunay.2023.TheRefinedWebDatasetforFalconLLM:Outperforming
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 34 -->

34 Huangetal.
CuratedCorporawithWebDataOnly..InConferenceonNeuralInformationProcessingSystems(NeurIPS).
[109] FabioPetroni,AleksandraPiktus,AngelaFan,PatrickLewis,MajidYazdani,NicolaDeCao,JamesThorne,Yacine
Jernite,VladimirKarpukhin,JeanMaillard,VassilisPlachouras,TimRocktÃ¤schel,andSebastianRiedel.2021.KILT:a
BenchmarkforKnowledgeIntensiveLanguageTasks.InProceedingsoftheConferenceoftheNorthAmericanChapter
oftheAssociationforComputationalLinguistics:HumanLanguageTechnologies.AssociationforComputational
Linguistics,2523â€“2544.
[110] FilipRadlinskiandNickCraswell.2010. Comparingthesensitivityofinformationretrievalmetrics..InAnnual
InternationalACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval(SIGIR).667â€“674.
[111] ColinRaffel,NoamM.Shazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,YanqiZhou,WeiLi,
andPeterJ.Liu.2020.ExploringtheLimitsofTransferLearningwithaUnifiedText-to-TextTransformer.Journalof
MachineLearningResearch(JMLR)21(2020),140:1â€“140:67.
[112] OriRam,YoavLevine,ItayDalmedigos,DorMuhlgay,AmnonShashua,KevinLeyton-Brown,andYoavShoham.2023.
In-ContextRetrieval-AugmentedLanguageModels.TransactionsoftheAssociationforComputationalLinguistics11
(2023),1316â€“1331.
[113] OriRam,GalShachaf,OmerLevy,JonathanBerant,andAmirGloberson.2022.LearningtoRetrievePassageswithout
Supervision.InProceedingsoftheConferenceoftheNorthAmericanChapteroftheAssociationforComputational
Linguistics:HumanLanguageTechnologies.AssociationforComputationalLinguistics.
[114] DavidRau,HervÃ©DÃ©jean,NadezhdaChirkova,ThibaultFormal,ShuaiWang,VassilinaNikoulina,andStÃ©phane
Clinchant.2024. BERGEN:ABenchmarkingLibraryforRetrieval-AugmentedGeneration. arXivabs/2407.01102
(2024).
[115] NilsReimersandIrynaGurevych.2019.Sentence-BERT:SentenceEmbeddingsusingSiameseBERT-Networks.In
ProceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJoint
ConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP).AssociationforComputationalLinguistics,3980â€“3990.
[116] StephenRobertsonandHugoZaragoza.2009.TheProbabilisticRelevanceFramework:BM25andBeyond.Foundations
andTrendsÂ®inInformationRetrieval3,4(2009),333â€“389.
[117] JonSaad-Falcon,O.Khattab,ChristopherPotts,andMateiZaharia.2023.ARES:AnAutomatedEvaluationFramework
forRetrieval-AugmentedGenerationSystems.InNorthAmericanChapteroftheAssociationforComputational
Linguistics,Vol.abs/2311.09476.
[118] AlirezaSalemi,SuryaKallumadi,andHamedZamani.2024.OptimizationMethodsforPersonalizingLargeLanguage
ModelsthroughRetrievalAugmentation.InProceedingsofthe47thInternationalACMSIGIRConferenceonResearch
andDevelopmentinInformationRetrieval.ACM,752â€“762.
[119] AlirezaSalemiandHamedZamani.2024. EvaluatingRetrievalQualityinRetrieval-AugmentedGeneration.In
Proceedingsofthe47thInternationalACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval,
Vol.21.ACM,2395â€“2400.
[120] SaraSarto,MarcellaCornia,LorenzoBaraldi,andRitaCucchiara.2022.Retrieval-AugmentedTransformerforImage
Captioning.InInternationalConferenceonContent-basedMultimediaIndexing.ACM.
[121] ZhihongShao,YeyunGong,YelongShen,MinlieHuang,NanDuan,andWeizhuChen.2023.EnhancingRetrieval-
AugmentedLargeLanguageModelswithIterativeRetrieval-GenerationSynergy.InFindingsoftheAssociationfor
ComputationalLinguistics:EMNLP2023.AssociationforComputationalLinguistics,9248â€“9274.
[122] WeijiaShi,SewonMin,MichihiroYasunaga,MinjoonSeo,RichJames,M.Lewis,LukeZettlemoyer,andWen-tauYih.

### REPLUG:Retrieval-AugmentedBlack-BoxLanguageModels.InNorthAmericanChapteroftheAssociationfor

ComputationalLinguistics,Vol.abs/2301.12652.
[123] YunxiaoShi,XingZi,ZijingShi,HaiminZhang,QiangWu,andMinXu.2024. ERAGent:EnhancingRetrieval-
AugmentedLanguageModelswithImprovedAccuracy,Efficiency,andPersonalization.arXivabs/2405.06683(2024).
[124] WeihangSu,YichenTang,QingyaoAi,ZhijingWu,andYiqunLiu.2024.DRAGIN:DynamicRetrievalAugmented
GenerationbasedontheReal-timeInformationNeedsofLargeLanguageModels.arXivabs/2403.10081(2024).
[125] ZhiqingSun,XuezhiWang,YiTay,YimingYang,andDennyZhou.2023.Recitation-AugmentedLanguageModels.
InInternationalConferenceonLearningRepresentations(ICLR).
[126] KentoTatsuno,DaisukeMiyashita,TaigaIkeda,KiyoshiIshiyama,KazunariSumiyoshi,andJunDeguchi.2024.
AiSAQ:All-in-StorageANNSwithProductQuantizationforDRAM-freeInformationRetrieval.arXivabs/2404.06004
(2024).
[127] YiTay,MostafaDehghani,VinhQ.Tran,XavierGarcia,JasonWei,XuezhiWang,HyungWonChung,DaraBahri,
TalSchuster,HuaixiuStevenZheng,DennyZhou,NeilHoulsby,andDonaldMetzler.2023.UL2:UnifyingLanguage
LearningParadigms.InInternationalConferenceonLearningRepresentations(ICLR).
[128] YiTay,VinhTran,MostafaDehghani,JianmoNi,DaraBahri,HarshMehta,ZhenQin,KaiHui,ZheZhao,JaiPrakash
Gupta,TalSchuster,WilliamW.Cohen,andDonaldMetzler.2022.TransformerMemoryasaDifferentiableSearch
Index.InConferenceonNeuralInformationProcessingSystems(NeurIPS).
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 35 -->

TheSurveyofRetrieval-AugmentedTextGenerationinLargeLanguageModels 35
[129] JamesThorne,AndreasVlachos,ChristosChristodoulopoulos,andArpitMittal.2018.FEVER:aLarge-scaleDataset
forFactExtractionandVERification.InProceedingsoftheConferenceoftheNorthAmericanChapteroftheAssociation
forComputationalLinguistics:HumanLanguageTechnologies,Volume1(LongPapers).AssociationforComputational
Linguistics,809â€“819.
[130] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,NikolayBashlykov,
SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,DanBikel,LukasBlecher,CristianCantonFerrer,MoyaChen,
GuillemCucurull,DavidEsiobu,JudeFernandes,JeremyFu,WenyinFu,BrianFuller,CynthiaGao,VedanujGoswami,
NamanGoyal,AnthonyHartshorn,SagharHosseini,RuiHou,HakanInan,MarcinKardas,ViktorKerkez,Madian
Khabsa,IsabelKloumann,ArtemKorenev,PunitSinghKoura,Marie-AnneLachaux,ThibautLavril,JenyaLee,Diana
Liskovich,YinghaiLu,YuningMao,XavierMartinet,TodorMihaylov,PushkarMishra,IgorMolybog,YixinNie,
AndrewPoulton,JeremyReizenstein,RashiRungta,KalyanSaladi,AlanSchelten,RuanSilva,EricMichaelSmith,
RanjanSubramanian,XiaoqingEllenTan,BinhTang,RossTaylor,AdinaWilliams,JianXiangKuan,PuxinXu,
ZhengYan,IliyanZarov,YuchenZhang,AngelaFan,MelanieKambadur,SharanNarang,AurelienRodriguez,Robert
Stojnic,SergeyEdunov,andThomasScialom.2023.Llama2:OpenFoundationandFine-TunedChatModels.arXiv
abs/2307.09288(2023).
[131] HarshTrivedi,NiranjanBalasubramanian,TusharKhot,andAshishSabharwal.2023.InterleavingRetrievalwith
Chain-of-ThoughtReasoningforKnowledge-IntensiveMulti-StepQuestions.InProceedingsofthe61stAnnualMeeting
oftheAssociationforComputationalLinguistics(Volume1:LongPapers).AssociationforComputationalLinguistics,
10014â€“10037.
[132] GeorgeTsatsaronis,GeorgiosBalikas,ProdromosMalakasiotis,IoannisPartalas,MatthiasZschunke,MichaelR
Alvers,DirkWeissenborn,AnastasiaKrithara,SergiosPetridis,DimitrisPolychronopoulos,YannisAlmirantis,John
Pavlopoulos,NicolasBaskiotis,PatrickGallinari,ThierryArtiÃ©res,Axel-CyrilleNgongaNgomo,NormanHeino,Eric
Gaussier,LilianaBarrio-Alvers,MichaelSchroeder,IonAndroutsopoulos,andGeorgiosPaliouras.2015.Anoverview
oftheBIOASQlarge-scalebiomedicalsemanticindexingandquestionansweringcompetition.BMCBioinformatics
16,1(2015).
[133] AshishVaswani,NoamM.Shazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN.Gomez,LukaszKaiser,and
IlliaPolosukhin.2017.AttentionisAllyouNeed.InNeuralInformationProcessingSystems.5998â€“6008.
[134] AlexWang,YadaPruksachatkun,NikitaNangia,AmanpreetSingh,JulianMichael,FelixHill,OmerLevy,and
SamuelR.Bowman.2019.SuperGLUE:AStickierBenchmarkforGeneral-PurposeLanguageUnderstandingSystems.
InConferenceonNeuralInformationProcessingSystems(NeurIPS).3261â€“3275.
[135] HaoyuWang,TuoZhao,andJingGao.2024.BlendFilter:AdvancingRetrieval-AugmentedLargeLanguageModels
viaQueryGenerationBlendingandKnowledgeFiltering.arXivabs/2402.11129(2024).
[136] LiangWang,NanYang,XiaolongHuang,BinxingJiao,LinjunYang,DaxinJiang,RanganMajumder,andFuruWei.

#### TextEmbeddingsbyWeakly-SupervisedContrastivePre-training.arXivabs/2212.03533(2022).

[137] LiangWang,NanYang,andFuruWei.2023.Query2doc:QueryExpansionwithLargeLanguageModels.InProceedings
oftheConferenceonEmpiricalMethodsinNaturalLanguageProcessing.AssociationforComputationalLinguistics,
9414â€“9423.
[138] QifanWang,YiFang,AnirudhRavula,FuliFeng,XiaojunQuan,andDongfangLiu.2022.WebFormer:TheWeb-page
TransformerforStructureInformationExtraction.InProceedingsoftheACMWebConference2022.ACM.
[139] XiaohuaWang,ZhenghuaWang,XuanGao,FeiranZhang,YixinWu,ZhiboXu,TianyuanShi,ZhengyuanWang,
ShizhengLi,QiQian,RuichengYin,ChangzeLv,XiaoqingZheng,andXuanjingHuang.2024.SearchingforBest
PracticesinRetrieval-AugmentedGeneration.arXivabs/2407.01219(2024).
[140] XintaoWang,QianwenYang,YongtingQiu,JiaqingLiang,QianyuHe,ZhouhongGu,YanghuaXiao,andWeiWang.

## KnowledGPT:EnhancingLargeLanguageModelswithRetrievalandStorageAccessonKnowledgeBases.

arXivabs/2308.11761(2023).
[141] ZhiruoWang,JunAraki,ZhengbaoJiang,Md.RizwanParvez,andGrahamNeubig.2023.LearningtoFilterContext
forRetrieval-AugmentedGeneration.arXivabs/2311.08377(2023).
[142] BigScienceWorkshop,:,TevenLeScao,AngelaFan,ChristopherAkiki,andetc.2022.BLOOM:A176B-Parameter
Open-AccessMultilingualLanguageModel.arXivabs/2211.05100(2022).
[143] ShitaoXiao,ZhengLiu,YingxiaShao,andZhaoCao.2022.RetroMAE:Pre-TrainingRetrieval-orientedLanguage
ModelsViaMaskedAuto-Encoder..InConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP).
538â€“548.
[144] GuangzhiXiong,QiaoJin,ZhiyongLu,andAidongZhang.2024.BenchmarkingRetrieval-AugmentedGeneration
forMedicine.arXivabs/2402.13178(2024).
[145] JieXiong,LiYu,XiNiu,andYoufangLeng.2023.XRR:Extrememulti-labeltextclassificationwithcandidateretrieving
anddeepranking.InformationSciences622(2023),115â€“132.
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 36 -->

36 Huangetal.
[146] LeeXiong,ChenyanXiong,YeLi,Kwok-FungTang,JialinLiu,PaulN.Bennett,JunaidAhmed,andArnoldOverwijk.

## ApproximateNearestNeighborNegativeContrastiveLearningforDenseTextRetrieval.InInternational

ConferenceonLearningRepresentations(ICLR).
[147] FangyuanXu,WeijiaShi,andEunsolChoi.2024. RECOMP:ImprovingRetrieval-AugmentedLMswithContext
CompressionandSelectiveAugmentation.InTheTwelfthInternationalConferenceonLearningRepresentations,
Vol.abs/2310.04408.
[148] ShichengXu,LiangPang,JunXu,HuaweiShen,andXueqiCheng.2024. List-awareReranking-TruncationJoint
ModelforSearchandRetrieval-augmentedGeneration.InProceedingsoftheACMonWebConference2024,Vol.21.

## Acm,1330â€“1340.

[149] Shi-QiYan,Jia-ChenGu,YunZhu,andZhen-HuaLing.2024.CorrectiveRetrievalAugmentedGeneration.arXiv
abs/2401.15884(2024).
[150] DijiYang,JinmengRao,KezhenChen,XiaoyuanGuo,YawenZhang,JieYang,andYiZhang.2024.IM-RAG:Multi-
RoundRetrieval-AugmentedGenerationThroughLearningInnerMonologues.InProceedingsofthe47thInternational
ACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval,Vol.33.ACM,730â€“740.
[151] HaoyanYang,ZhitaoLi,YongZhang,JianzongWang,NingCheng,MingLi,andJingXiao.2023. PRCA:Fitting
Black-BoxLargeLanguageModelsforRetrievalQuestionAnsweringviaPluggableReward-DrivenContextual
Adapter.InProceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguageProcessing.Associationfor
ComputationalLinguistics,5364â€“5375.
[152] ZhuolinYang,WeiPing,ZihanLiu,VijayKorthikanti,WeiliNie,De-AnHuang,LinxiFan,ZhidingYu,ShiyiLan,Bo
Li,MohammadShoeybi,Ming-YuLiu,YukeZhu,BryanCatanzaro,ChaoweiXiao,andAnimaAnandkumar.2023.
Re-ViLM:Retrieval-AugmentedVisualLanguageModelforZeroandFew-ShotImageCaptioning.InFindingsofthe
AssociationforComputationalLinguistics:EMNLP2023.AssociationforComputationalLinguistics,11844â€“11857.
[153] ZhilinYang,PengQi,SaizhengZhang,YoshuaBengio,WilliamCohen,RuslanSalakhutdinov,andChristopherD.
Manning.2018. HotpotQA:ADatasetforDiverse,ExplainableMulti-hopQuestionAnswering.InProceedingsof
theConferenceonEmpiricalMethodsinNaturalLanguageProcessing.AssociationforComputationalLinguistics,
2369â€“2380.
[154] HaoYu,AoranGan,KaiZhang,ShiweiTong,QiLiu,andZhaofengLiu.2024.EvaluationofRetrieval-Augmented
Generation:ASurvey.arXivabs/2405.07437(2024).
[155] ShiYu,JiahuaLiu,JingqinYang,ChenyanXiong,PaulN.Bennett,JianfengGao,andZhiyuanLiu.2020. Few-
ShotGenerativeConversationalQueryRewriting.InProceedingsofthe43rdInternationalACMSIGIRconference
onresearchanddevelopmentinInformationRetrieval,SIGIR2020,VirtualEvent,China,July25-30,2020,JimmyX.
Huang,YiChang,XueqiCheng,JaapKamps,VanessaMurdock,Ji-RongWen,andYiqunLiu(Eds.).ACM,1933â€“1936.
https://doi.org/10.1145/3397271.3401323
[156] WenhaoYu,DanIter,ShuohangWang,YichongXu,MingxuanJu,SoumyaSanyal,ChenguangZhu,MichaelZeng,
andMengJiang.2023. GenerateratherthanRetrieve:LargeLanguageModelsareStrongContextGenerators.In
InternationalConferenceonLearningRepresentations(ICLR).
[157] ZichunYu,ChenyanXiong,ShiYu,andZhiyuanLiu.2023.Augmentation-AdaptedRetrieverImprovesGeneralization
ofLanguageModelsasGenericPlug-In.InProceedingsofthe61stAnnualMeetingoftheAssociationforComputational
Linguistics(Volume1:LongPapers).AssociationforComputationalLinguistics,2421â€“2436.
[158] YiYuan,HaoheLiu,XuboLiu,QiushiHuang,MarkD.Plumbley,andWenwuWang.2024. Retrieval-Augmented
Text-to-AudioGeneration.InICASSP-IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing
(ICASSP),Vol.abs/2309.08051.IEEE.
[159] ZhenruiYue,HuiminZeng,YimengLu,LanyuShang,YangZhang,andDongWang.2024.Evidence-DrivenRetrieval
AugmentedResponseGenerationforOnlineMisinformation.InNorthAmericanChapteroftheAssociationfor
ComputationalLinguistics,Vol.abs/2403.14952.
[160] SaberZerhoudiandMichaelGranitzer.2024.PersonaRAG:EnhancingRetrieval-AugmentedGenerationSystems
withUser-CentricAgents.arXiv(2024).
[161] SusanZhang,StephenRoller,NamanGoyal,MikelArtetxe,MoyaChen,ShuohuiChen,ChristopherDewan,MonaT.
Diab,XianLi,XiVictoriaLin,TodorMihaylov,MyleOtt,SamShleifer,KurtShuster,DanielSimig,PunitSinghKoura,
AnjaliSridhar,TianluWang,andLukeZettlemoyer.2022.OPT:OpenPre-trainedTransformerLanguageModels.
arXivabs/2205.01068(2022).
[162] PenghaoZhao,HailinZhang,QinhanYu,ZhengrenWang,YuntengGeng,FangchengFu,LingYang,WentaoZhang,Jie
Jiang,andBinCui.2024.Retrieval-AugmentedGenerationforAI-GeneratedContent:ASurvey.arXivabs/2402.19473
(2024).
[163] HuaixiuStevenZheng,SwaroopMishra,XinyunChen,Heng-TzeCheng,EdH.Chi,QuocVLe,andDennyZhou.

### TakeaStepBack:EvokingReasoningviaAbstractioninLargeLanguageModels.InTheTwelfthInternational

ConferenceonLearningRepresentations,Vol.abs/2310.06117.
,Vol.1,No.1,Article.Publicationdate:August2018.

<!-- Page 37 -->

TheSurveyofRetrieval-AugmentedTextGenerationinLargeLanguageModels 37
[164] NingZhong,YuefengLi,andSheng-TangWu.2012.EffectivePatternDiscoveryforTextMining.IEEETransactions
onKnowledgeandDataEngineering24,1(2012),30â€“44.
[165] YutaoZhu,HuayingYuan,ShutingWang,JiongnanLiu,WenhanLiu,ChenlongDeng,ZhichengDou,andJi-Rong
Wen.2023.LargeLanguageModelsforInformationRetrieval:ASurvey.arXivabs/2308.07107(2023).
[166] HongleiZhuang,ZhenQin,RolfJagerman,KaiHui,JiMa,JingLu,JianmoNi,XuanhuiWang,andMichaelBendersky.

### RankT5:Fine-TuningT5forTextRankingwithRankingLosses.InProceedingsofthe46thInternationalACM

SIGIRConferenceonResearchandDevelopmentinInformationRetrieval.ACM.
Received20February2007;revised12March2009;accepted5June2009
,Vol.1,No.1,Article.Publicationdate:August2018.

## Tables

**Table (Page 20):**

| EvaluationFramework | Aspects | Methods | Metrics | Datasets |
|---|---|---|---|---|
| RAGAS[33] | QualityofRAGSystems | ContextRelevance | ExtractedSentences/TotalSentences | WikiEval7 |
|  |  | AnswerRelevance | AverageCosineSimilarity |  |
|  |  | Faithfulness | SupportedStatements/TotalStatements |  |
| ARES[117] | ImprovingRAGAS | ContextRelevance | ConfidenceIntervals | KILT[109] SuperGLUE[134] |
|  |  | AnswerRelevance |  |  |
|  |  | AnswerFaithfulness |  |  |
| RECALL[91] | CounterfactualRobustness | ResponseQuality | Accuracy(QA) BLEU,ROUGE-L(Generation) | EventKG[41] UJ[50] |
|  |  | Robustness | MisleadingRate(QA) MistakeReappearanceRate(Generation) |  |
| RGB[14] | ImpactofRAGonLLMs | NoiseRobustness | Accuracy | Synthetic |
|  |  | NegativeRejection | RejectionRate |  |
|  |  | InformationIntegration | Accuracy |  |
|  |  | CounterfactualRobustness | ErrorDetectionRate ErrorCorrectionRate |  |
| MIRAGE[144] | RAGinMedicalQA | Zero-ShotLearning | Accuracy | MMLU-Med[45] MedQA-US[66] MedMCQA[105] PubMedQA[67] BioASQ-Y/N[132] |
|  |  | Multi-ChoiceEvaluation |  |  |
|  |  | Retrieval-AugmentedGeneration |  |  |
|  |  | Question-OnlyRetrieval |  |  |
| eRAG[119] | RetrievalQualityinRAG | DownstreamTask | Accuracy,ROUGE | KILT |
|  |  | Set-based | Precision,Recall,HitRate |  |
|  |  | Ranking | MAP,MRR,NDCG |  |
| BERGEN[114] | StandardizingRAGExperiments | Surface-Based | EM,F1,Precision,Recall | QADatasets[69,76] |
|  |  | Semantic | BEM[11],LLMeval[114] |  |


**Table (Page 22):**

| Research | Year | RetrievalSource |  | Multi-hop | Training | Pre-Retrieval |  |  | Retrieval | Post-Retrieval |  | Generation |  |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|  |  | Internal | External |  |  | Indexing | QueryManipulation | DataModification | Search&Ranking | Re-Ranking | Filtering | Enhancing | Customization |
| REALM[42] | 2020 |  | (cid:33) |  | (cid:33) | (cid:33) |  |  | (cid:33) |  |  |  |  |
| kNN-LMs[72] | 2020 | (cid:33) | (cid:33) |  |  | (cid:33) |  |  | (cid:33) |  |  | (cid:33) |  |
| RAG[83] | 2020 |  | (cid:33) |  | (cid:33) | (cid:33) |  |  | (cid:33) |  |  |  |  |
| FiD[58] | 2021 |  | (cid:33) |  |  |  |  |  | (cid:33) |  |  | (cid:33) |  |
| Webgpt[100] | 2021 |  | (cid:33) | (cid:33) | (cid:33) | (cid:33) | (cid:33) |  | (cid:33) |  | (cid:33) | (cid:33) |  |
| Re2G[40] | 2022 | (cid:33) |  | (cid:33) | (cid:33) |  |  |  |  | (cid:33) |  |  |  |
| RETRO[9] | 2022 |  | (cid:33) | (cid:33) | (cid:33) | (cid:33) |  |  | (cid:33) |  |  |  |  |
| DSP[73] | 2022 |  | (cid:33) | (cid:33) |  |  | (cid:33) |  |  | (cid:33) |  | (cid:33) |  |
| CoK[86] | 2023 |  | (cid:33) | (cid:33) |  |  | (cid:33) |  |  | (cid:33) |  |  |  |
| IRCOT[131] | 2023 |  | (cid:33) | (cid:33) |  |  | (cid:33) |  |  |  |  | (cid:33) |  |
| ITRG[34] | 2023 | (cid:33) | (cid:33) | (cid:33) |  |  |  |  | (cid:33) |  |  | (cid:33) |  |
| PKG[92] | 2023 | (cid:33) |  |  |  |  |  |  |  |  |  |  |  |
| RA-DIT[89] | 2023 |  | (cid:33) | (cid:33) | (cid:33) |  |  | (cid:33) | (cid:33) |  |  | (cid:33) |  |
| Self-RAG[4] | 2023 |  | (cid:33) |  | (cid:33) |  |  |  |  |  | (cid:33) |  |  |
| SURGE[70] | 2023 | (cid:33) |  |  |  |  |  |  | (cid:33) |  |  |  |  |
| FiD-TF[5] | 2023 |  | (cid:33) |  |  |  |  |  |  | (cid:33) | (cid:33) |  |  |
| PRCA[151] | 2023 |  | (cid:33) |  | (cid:33) |  |  |  | (cid:33) |  |  | (cid:33) |  |
| REPLUG[122] | 2023 |  | (cid:33) |  | (cid:33) |  |  |  |  |  |  | (cid:33) |  |
| AAR[157] | 2023 |  | (cid:33) |  | (cid:33) |  |  |  | (cid:33) |  |  |  |  |
| Query2doc[137] | 2023 | (cid:33) |  |  |  |  | (cid:33) |  |  |  |  |  |  |
| Step-Back[163] | 2023 |  | (cid:33) | (cid:33) |  |  | (cid:33) |  |  |  |  |  |  |
| ITER-RETGEN[121] | 2023 |  | (cid:33) | (cid:33) |  |  |  |  | (cid:33) | (cid:33) |  |  |  |
| RECITE[125] | 2023 | (cid:33) |  | (cid:33) | (cid:33) |  |  | (cid:33) |  |  |  | (cid:33) |  |
| PROMPTAGATOR[27] | 2023 | (cid:33) |  | (cid:33) |  |  | (cid:33) |  |  | (cid:33) | (cid:33) |  |  |
| UPRISE[20] | 2023 | (cid:33) |  | (cid:33) | (cid:33) |  |  | (cid:33) | (cid:33) |  |  | (cid:33) |  |
| GENREAD[156] | 2023 | (cid:33) |  |  |  |  |  | (cid:33) |  |  |  | (cid:33) |  |
| LAPDOG[52] | 2023 |  | (cid:33) |  | (cid:33) |  | (cid:33) |  | (cid:33) | (cid:33) |  | (cid:33) | (cid:33) |
| KnowledGPT[140] | 2023 |  | (cid:33) | (cid:33) |  |  | (cid:33) | (cid:33) |  |  |  |  |  |
| Selfmem[21] | 2023 |  | (cid:33) | (cid:33) | (cid:33) |  |  |  |  | (cid:33) |  | (cid:33) |  |
| MEMWALKER[13] | 2023 |  | (cid:33) |  |  | (cid:33) |  |  | (cid:33) |  |  | (cid:33) |  |
| RECOMP[147] | 2023 |  | (cid:33) |  | (cid:33) |  |  |  |  |  | (cid:33) |  |  |
| Rewrite-Retrieve-Read[94] | 2023 |  | (cid:33) |  | (cid:33) |  | (cid:33) |  |  |  |  |  |  |
| Atlas[94] | 2023 |  | (cid:33) | (cid:33) | (cid:33) | (cid:33) |  |  | (cid:33) | (cid:33) |  |  |  |
| DKS-RAC[53] | 2023 |  | (cid:33) | (cid:33) | (cid:33) |  |  |  |  | (cid:33) | (cid:33) |  |  |
| In-ContextRALM[112] | 2023 |  | (cid:33) |  |  |  |  |  |  | (cid:33) |  |  |  |
| Fid-light[47] | 2023 |  | (cid:33) | (cid:33) |  |  |  |  |  | (cid:33) |  |  |  |
| FLARE[65] | 2023 |  | (cid:33) |  |  |  | (cid:33) |  | (cid:33) |  |  |  |  |
| Chameleon[63] | 2023 |  | (cid:33) |  | (cid:33) | (cid:33) |  |  | (cid:33) |  |  |  |  |
| ERAGent[123] | 2024 | (cid:33) | (cid:33) |  | (cid:33) |  | (cid:33) |  | (cid:33) |  | (cid:33) | (cid:33) | (cid:33) |
| PipeRAG[64] | 2024 |  | (cid:33) | (cid:33) | (cid:33) | (cid:33) |  |  |  |  |  | (cid:33) |  |
| GenRT[148] | 2024 | (cid:33) |  |  | (cid:33) |  |  |  |  | (cid:33) | (cid:33) |  |  |
| PersonaRAG[160] | 2024 | (cid:33) | (cid:33) | (cid:33) | (cid:33) |  | (cid:33) |  | (cid:33) | (cid:33) |  | (cid:33) | (cid:33) |
| CRAG[149] | 2024 | (cid:33) | (cid:33) |  | (cid:33) |  |  | (cid:33) |  | (cid:33) | (cid:33) | (cid:33) |  |
| IMRAG[150] | 2024 |  | (cid:33) | (cid:33) | (cid:33) |  | (cid:33) |  |  | (cid:33) |  | (cid:33) |  |
| AiSAQ[126] | 2024 | (cid:33) |  |  |  | (cid:33) |  |  | (cid:33) | (cid:33) |  |  |  |
| ROPG[118] | 2024 | (cid:33) |  |  | (cid:33) |  |  |  | (cid:33) |  |  | (cid:33) | (cid:33) |
| RQ-RAG[12] | 2024 |  | (cid:33) | (cid:33) | (cid:33) |  | (cid:33) |  |  | (cid:33) |  |  |  |
| PlanRAG[81] | 2024 |  | (cid:33) | (cid:33) |  |  | (cid:33) |  |  | (cid:33) |  | (cid:33) |  |
| RARG[159] | 2024 |  | (cid:33) |  | (cid:33) |  |  | (cid:33) | (cid:33) |  |  | (cid:33) |  |
| DRAGIN[124] | 2024 |  | (cid:33) | (cid:33) |  |  | (cid:33) |  | (cid:33) |  |  | (cid:33) |  |
| LRUS-CoverTree[93] | 2024 | (cid:33) |  |  | (cid:33) | (cid:33) |  |  | (cid:33) | (cid:33) |  |  |  |


**Table (Page 23):**

| Research | Year | Retriever | Generator |
|---|---|---|---|
| REALM[42] | 2020 | BERT[29] | Transformers[133] |
| kNN-LMs[72] | 2020 | FAISS[68] | Transformers |
| RAG[83] | 2020 | DPR[71] | BART-Large[82] |
| FiD[58] | 2021 | BM25[116],DPR | T5[111] |
| Webgpt[100] | 2021 | Bing | GPT-3[10] |
| Re2G[40] | 2022 | BM25,DPR | BART |
| RETRO[9] | 2022 | BERT | Transformer |
| DSP[73] | 2022 | ColBERTv2[74] | GPT-3.5(text-davinci-002) |
| CoK[86] | 2023 | LLaMA2-7B[130],ChatGPT(gpt-3.5-turbo-0613) | ChatGPT(gpt-3.5-turbo-0613) |
| IRCOT[131] | 2023 | BM25 | GPT-3(code-davinci-002),Flan-T5[24] |
| ITRG[34] | 2023 | Atlas[94] | LLaMA-33B |
| PKG[92] | 2023 | LLaMA-7B | InstructGPT-3.5(text-davinic-002)[104] |
| RA-DIT[89] | 2023 | DRAGON+[88] | LLaMA |
| Self-RAG[4] | 2023 | Contriever[57] | LLaMA2(7Band13B),GPT-4[1] |
| SURGE[70] | 2023 | GraphNeuralNetworks(GNN)[43] | Transformers |
| FiD-TF[5] | 2023 | BM25,SBERT[115] | T5 |
| PRCA[151] | 2023 | BM25,DPR,Contriver,SimCSE[37],SBERT | T5,Phoenix-7B[19],Vicuna-7B[22],ChatGLM[31],GPT-3.5 |
| REPLUG[122] | 2023 | Contriever | GPT-3 |
| AAR[157] | 2023 | ANCE[146],Contriever | Flan-T5,InstructGPT |
| Query2doc[137] | 2023 | BM25,DPR | GPT-3(text-davinci-003) |
| Step-Back[163] | 2023 | PaLM-2L[23] | PaLM-2L,GPT-4 |
| ITER-RETGEN[121] | 2023 | Contriever | InstructGPT(text-davinci-003),LLaMA2 |
| RECITE[125] | 2023 |  | PaLM,UL2[127],OPT[161],Codex[16] |
| PROMPTAGATOR[27] | 2023 | T5 | FLAN |
| UPRISE[20] | 2023 | GPT-Neo-2.7B[8] | BLOOM-7.1B[142],OPT-66B,GPT-3-175B |
| GENREAD[156] | 2023 |  | InstructGPT |
| LAPDOG[52] | 2023 | Contriever | T5 |
| KnowledGPT[140] | 2023 |  | GPT-4 |
| Selfmem[21] | 2023 | BM25 | XGLM[90],XLM-Rbase[25] |
| MEMWALKER[13] | 2023 | LLaMA2 | LLaMA2 |
| RECOMP[147] | 2023 | BM25 | T5-Large |
| Rewrite-Retrieve-Read[94] | 2023 | Bing | T5-Large,ChatGPT(gpt-3.5-turbo),Vicuna-13B |
| Atlas[94] | 2023 | Contriever | T5 |
| DKS-RAC[53] | 2023 | DPR | BART |
| In-ContextRALM[112] | 2023 | BM25,BERT-base,Contriever,Spider[113] | GPT-2,GPT-Neo,GPT-J[35],OPT,andLLaMA |
| Fid-light[47] | 2023 | GTR-Base[101] | T5 |
| FLARE[65] | 2023 | BM25,Bing | GPT-3.5(text-davinci-003) |
| Chameleon[63] | 2023 | ChamVS[63] | ChamLM[63] |
| ERAGent[123] | 2024 | Bing | GPT-3.5,Falcon1B[108] |
| PipeRAG[64] | 2024 | SBERT | RETRO[9] |
| GenRT[148] | 2024 | LambdaMart[2] |  |
| PersonaRAG[160] | 2024 | BM25 | GPT-3.5 |
| CRAG[149] | 2024 | Contriever | LLaMA2 |
| IMRAG[150] | 2024 | DPR | Vicuna-7B |
| AiSAQ[126] | 2024 | DiskANN[106] |  |
| ROPG[118] | 2024 | BM25,Contriever | FlanT5-XXL |
| RQ-RAG[12] | 2024 | DuckDuckGo8 | LLaMA2-7B |
| PlanRAG[81] | 2024 | GPT-4 | GPT-4 |
| RARG[159] | 2024 | BM25,E5[136] | LLaMA2-7B |
| DRAGIN[124] | 2024 | BM25,SGPT[99] | LLaMA2(7Band13B),Vicuna-13B |
| LRUS-CoverTree[93] | 2024 | k-MIPS |  |


**Table (Page 24):**

| 72.27Â±1.36 | 63.71Â±1.35 | 55.49Â±0.77 | 66.20Â±2.12 | 88.51Â±1.28 | 69.23 |
|---|---|---|---|---|---|
| 71.72Â±1.36 | 63.94Â±1.35 | 54.29Â±0.77 | 65.60Â±2.12 | 85.44Â±1.42 | 68.20 |
| 73.19Â±1.34 | 65.20Â±1.34 | 53.12Â±0.77 | 54.80Â±2.23 | 75.73Â±1.72 | 64.41 |
| 73.09Â±1.34 | 66.69Â±1.32 | 54.94Â±0.77 | 66.40Â±2.11 | 85.76Â±1.41 | 69.38 |
| 75.57Â±1.30 | 64.34Â±1.34 | 55.34Â±0.77 | 69.00Â±2.07 | 87.06Â±1.35 | 70.26 |
| 73.37Â±1.34 | 64.73Â±1.34 | 54.75Â±0.77 | 67.20Â±2.10 | 88.51Â±1.28 | 69.71 |
| 73.37Â±1.34 | 63.47Â±1.35 | 54.10Â±0.77 | 26.40Â±1.97 | 71.36Â±1.82 | 57.74 |
| 74.10Â±1.33 | 65.99Â±1.33 | 54.03Â±0.77 | 26.40Â±1.97 | 69.90Â±1.85 | 58.08 |
| 72.18Â±1.36 | 63.63Â±1.35 | 52.71Â±0.77 | 22.20Â±1.86 | 66.83Â±1.89 | 55.51 |
| 71.99Â±1.36 | 65.12Â±1.34 | 55.15Â±0.77 | 29.00Â±2.03 | 73.46Â±1.78 | 58.95 |
| 74.20Â±1.33 | 64.57Â±1.34 | 54.72Â±0.77 | 31.00Â±2.07 | 76.21Â±1.71 | 60.14 |
| 73.19Â±1.34 | 64.96Â±1.34 | 54.53Â±0.77 | 31.00Â±2.07 | 72.01Â±1.81 | 59.14 |


**Table (Page 25):**

|  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  | +0.03 |  | +0.06 +0.15 +0.17 |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  | +0.15 |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  | +0.09 |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  | Retrie Close Oracl | ver d Boo e | k |


**Table (Page 25):**

|  |  |  |  | O | ra |
|---|---|---|---|---|---|
|  |  |  | SPLADE- RetroMAE | v3+RR +RR |  |
|  |  | S RetroM | PLADE-v3 AE |  |  |
|  |  | BM25+RR |  |  |  |
|  |  |  |  |  |  |
|  |  |  |  |  |  |
